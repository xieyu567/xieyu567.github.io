{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1558844936745},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1558844936745},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1558844936745},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1558844936749},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1558844936749},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1558844936749},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1558844936749},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1558844936749},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1558844936749},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1558844936749},{"_id":"themes/next/README.cn.md","hash":"263b74f1ac7c0f6f9424b8cced4b0b320ae61efc","modified":1558844936749},{"_id":"themes/next/README.md","hash":"287c7e6b7a6ddf75d815dda0df8bd228e3f285c5","modified":1558844936749},{"_id":"themes/next/_config.yml","hash":"c80068e5f9bdd07efadf1f9fd10d473df95fc7a7","modified":1561618904041},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1558844936749},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1558844936749},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1558844936752},{"_id":"source/categories/index.md","hash":"baf93ea83a9242dcac52921cb09b555fe2f75a27","modified":1558845268802},{"_id":"source/_posts/AI-Chapter1.md","hash":"c039d733c0c666deb7d74b56b8dd8216987b8e2d","modified":1560155462714},{"_id":"source/_posts/Basic-Concept.md","hash":"0fa929ae7cf485ce7fe4d831781dbfe75d8f6afa","modified":1560152528381},{"_id":"source/_posts/Decision-Tree.md","hash":"ade6f0f722b0437b8b2918152e952f5a0493d508","modified":1560152530928},{"_id":"source/_posts/Deep-Learning.md","hash":"2ef8de3915651b49eec2b3d93f4c1c373739a7fe","modified":1560137200834},{"_id":"source/_posts/Exercise-1.md","hash":"ebc3423ff9d439bc8f408681709a4879179ac1ee","modified":1560152520758},{"_id":"source/_posts/KNN.md","hash":"e8e1f68e14b0699baffb58d82e2c479173aa0ba9","modified":1560152544964},{"_id":"source/_posts/Exercise-2.md","hash":"a45a42b2aa7d10812ad69c23821078b5b209e9e7","modified":1560152522448},{"_id":"source/_posts/AI-chapter2.md","hash":"1b707735e395dd039c7e9688931dd5c6700139ca","modified":1560222010474},{"_id":"source/_posts/Logistic-regression.md","hash":"1c349aed41b5b9911a61503a137b65ce87d85c72","modified":1560152551514},{"_id":"source/_posts/Kafka.md","hash":"a8fdd03c5228753966e5728a6ffc0a58b2bf6174","modified":1560152595140},{"_id":"source/_posts/Naive-Bayesian.md","hash":"6cd0b81e604a491a42e7fbca51c600ec8df7280c","modified":1560152567104},{"_id":"source/_posts/Maximum-entropy-model.md","hash":"3b0a4afbec158b2a1dff3699b4a73f0791eb8ef9","modified":1560152559644},{"_id":"source/_posts/Network-chapter4.md","hash":"129f32430a1a9e8c47914c793b0ef9a6bbdb5b66","modified":1567214354607},{"_id":"source/_posts/Network-chapter2.md","hash":"1d0872934a3d86deea7f904aa928775cf06db9b9","modified":1566620466433},{"_id":"source/_posts/Network-chapter1.md","hash":"d453beb7c68d66dbc16ae2a55ec3de67d1a9f1f5","modified":1563932146752},{"_id":"source/_posts/Network-chapter3.md","hash":"02276e7ab82f2db41567ac68d2ab9ffba7ad6efa","modified":1567133398460},{"_id":"source/_posts/Network-chapter5.md","hash":"a8c666f9fa5f8a06e365391d8aa615faa5f58025","modified":1567579429191},{"_id":"source/_posts/Perceptron.md","hash":"b303906e56b565c37efe67c9090de2b4c33848f7","modified":1560152576313},{"_id":"source/_posts/SVM.md","hash":"7d4a0f327215edf127f239cf6ced1e05c2565a2e","modified":1560155508389},{"_id":"source/_posts/pacman-update-problem.md","hash":"753c38737a7d2cd442b6ecfc549fb95d6faee4a5","modified":1567309783339},{"_id":"source/_posts/亚述.md","hash":"166f9252d1b6377551d6f0e08d700bf2dc01491b","modified":1560932057089},{"_id":"source/_posts/亚非语系.md","hash":"8a3372e43d5d5200cf9c1842f240eb8514c911ea","modified":1560932763170},{"_id":"source/_posts/腓尼基.md","hash":"5347eda528ef2a1b44e825599f1542551cc02743","modified":1561086665735},{"_id":"source/_posts/苏美尔.md","hash":"31f159da1e64a20d24b50c789a32fafa2f7bf10b","modified":1561441683255},{"_id":"source/tags/index.md","hash":"7ff14db8de819c2279dc067a6916b0cd0fa4c244","modified":1558845314077},{"_id":"source/_posts/闪米特.md","hash":"dbfc7e845b5e30cf1a12559b6d6db1155c82482f","modified":1560930515220},{"_id":"source/_posts/Transformer.md","hash":"86fe4a6d41d58125451a6f4e1ac3f7c2083f2961","modified":1561640103274},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1558844936749},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"50d48c47162817a3810a9d9ad51104e83947419a","modified":1558844936749},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1558844936749},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1558844936749},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1558844936749},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1558844936749},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1559570063784},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1558844936749},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1558844936749},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1558844936749},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1558844936749},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1558844936749},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1558844936749},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1558844936749},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1558844936749},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1558844936749},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1558844936749},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1558844936749},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1558844936749},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1558844936749},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1558844936749},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1558844936752},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1558844936752},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1558844936752},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1558844936752},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1558844936752},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1558844936752},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1558844936752},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1558844936752},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1558844936752},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1558844936779},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1558844936779},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1558844936779},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1558844936759},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1558844936749},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1558844936749},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1558844936749},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1558844936749},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1559570042080},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1558844936749},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"6a54c3c85ff6b19d275827a327abbf4bd99b2ebf","modified":1558844936749},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1558844936749},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1558844936752},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1558844936752},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1558844936752},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1558844936752},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1558844936752},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1558844936752},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1558844936752},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1558844936752},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1558844936752},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1558844936752},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1558844936749},{"_id":"themes/next/layout/_partials/footer.swig","hash":"710aef5772ba42b00d618085f3413c5520c3b75c","modified":1558847641419},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1558844936749},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1558844936749},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1558844936749},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1558844936749},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1558844936749},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1558844936759},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1558844936752},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1558844936752},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1558844936752},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1558844936752},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1558844936752},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1558844936752},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1558844936752},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1558844936752},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1558844936755},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1558844936759},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1558844936759},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1558844936759},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1558844936759},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1558844936759},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1558844936759},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1558844936759},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1558844936759},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1558844936759},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1558844936759},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1558844936759},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1558844936759},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1558844936759},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1558844936759},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1558844936762},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1558844936762},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1558844936762},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1558844936762},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1558844936752},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1558844936752},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1558844936759},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1558844936759},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1558844936759},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1558844936759},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1558844936759},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1558844936752},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1558844936752},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1558844936752},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1558844936752},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1558844936752},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1558844936752},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1558844936752},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1558844936752},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1558844936752},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1558844936752},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1558844936752},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1558844936752},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1558844936752},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1558844936752},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1558844936752},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1558844936749},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1558844936752},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1558844936749},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1558844936752},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1558844936749},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1558844936749},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1558844936749},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1558844936749},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1558844936752},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1558844936752},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1558844936762},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1558844936752},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1558844936762},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1558844936762},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1558844936762},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1558844936762},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1558844936762},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1558844936762},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1558844936762},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1558844936762},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1558844936762},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1558844936762},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1558844936759},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1558844936759},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1558844936759},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1558844936759},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1558844936759},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1558844936759},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1558844936759},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1558844936759},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1558844936765},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1558844936765},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1558844936762},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1558844936765},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1558844936765},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1558844936765},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1558844936765},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1558844936765},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1558844936765},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1558844936765},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1558844936765},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1558844936765},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1558844936765},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1558844936775},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1558844936775},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1558844936775},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1558844936775},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1558844936775},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1558844936775},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1558844936775},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1558844936775},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1558844936775},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1558844936775},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1558844936775},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1558844936775},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1558844936775},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1558844936775},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1558844936779},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1558844936779},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1558844936779},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1558844936779},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1558844936779},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1558844936775},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1558844936752},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1558844936752},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1558844936762},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1558844936755},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1558844936759},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1558844936759},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1558844936759},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1558844936759},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1558844936759},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1558844936759},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4aac01962520d60b03b23022ab601ad4bd19c08c","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1558844936759},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1558844936762},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1558844936762},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1558844936762},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1558844936765},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1558844936765},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1558844936765},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1558844936765},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1558844936765},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1558844936769},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1558844936779},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1558844936779},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1558844936762},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1558844936772},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1558844936772},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1558844936779},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1558844936755},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1558844936759},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1558844936759},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1558844936759},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1558844936759},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1558844936762},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1558844936762},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1558844936762},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1558844936762},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1558844936765},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1558844936765},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1558844936762},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1558844936769},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1558844936769},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1558844936772},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1558844936765},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1558844936772},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1558844936779},{"_id":"public/categories/index.html","hash":"d514da26ce5a47d4a11d4a5914d0762fda4f1d5b","modified":1567584386433},{"_id":"public/tags/index.html","hash":"35316e93ee3417c3ec6c8de560fcfd58e916707d","modified":1567584386434},{"_id":"public/2019/09/01/pacman-update-problem/index.html","hash":"b7f1203d4c44c20e5518c58bb4c4534def47c70c","modified":1567584386434},{"_id":"public/2019/06/21/苏美尔/index.html","hash":"8f7a8d25537fd514c593a892b1fb728f2bc89965","modified":1567584386434},{"_id":"public/archives/page/3/index.html","hash":"cbe443df15b6f8b28e2f1592aad756ade38b1d40","modified":1567584386434},{"_id":"public/archives/2019/page/3/index.html","hash":"6bc7899818b6c07f3b505a2b24bba514538cc4a7","modified":1567584386434},{"_id":"public/archives/2019/05/index.html","hash":"b6538ffd00ab1b5d06a2c3d11fe22a349778a196","modified":1567584386434},{"_id":"public/archives/2019/06/page/2/index.html","hash":"3fef2c56b483407ba522c9c932d3fb49f1cc24df","modified":1567584386434},{"_id":"public/archives/2019/07/index.html","hash":"0fab7b8e1494f0c3e95b7dd43d1223e70b13f538","modified":1567584386434},{"_id":"public/archives/2019/08/index.html","hash":"108a2df046a78e581615867fcaa761c47c69c730","modified":1567584386434},{"_id":"public/archives/2019/09/index.html","hash":"a02ebd8eefc5d23faf961ad4fac49ab06b439548","modified":1567584386434},{"_id":"public/categories/统计学习方法/index.html","hash":"c9838441d0fa81b930a0fd362a452c237c230290","modified":1567584386434},{"_id":"public/categories/AI-an-answer-of-the-modern-method/index.html","hash":"e18e8eae52e39d876a27925a60a680c7280d0f2e","modified":1567584386434},{"_id":"public/categories/papers/index.html","hash":"f40448fa5e58bc0a816f184dff7a08f03d564926","modified":1567584386434},{"_id":"public/categories/big-data/index.html","hash":"fbdfa71e5cdcaa55f51433322f701920ed53072e","modified":1567584386434},{"_id":"public/categories/Computer-Network-A-Systems-Approach/index.html","hash":"c564be6ebb065094682477f1bfe9ae3482ba0ffa","modified":1567584386434},{"_id":"public/categories/solved-problem/index.html","hash":"314348682c9dcf7350933f077e310c6623024424","modified":1567584386434},{"_id":"public/categories/语言学-亚非语系/index.html","hash":"e2c0ac80fbcc67d74750cc8585ff6d6798f51349","modified":1567584386434},{"_id":"public/categories/语言学/index.html","hash":"cdae27390f966fa069ba3e61c55a1b8ebf0ec427","modified":1567584386434},{"_id":"public/categories/NLP/index.html","hash":"e015ce8e1b9cdc6d47b090c029012af0116e4730","modified":1567584386435},{"_id":"public/tags/AI/index.html","hash":"a1e0d2046e10e1ca2365254dc585c0efecc02a27","modified":1567584386435},{"_id":"public/tags/big-data/index.html","hash":"c4c0a09cb7692aa7b89f663865b0bf81fc82bc11","modified":1567584386435},{"_id":"public/tags/Network/index.html","hash":"85cfd80ebef64d0d6600fac50980d31f19f3393c","modified":1567584386435},{"_id":"public/tags/Manjaro/index.html","hash":"dfb49fd6190d587a50a0c5b78a3879c724795ad3","modified":1567584386435},{"_id":"public/tags/杂谈/index.html","hash":"d580050358d6c68d3c6d14f1e2d34dec995005b4","modified":1567584386435},{"_id":"public/2019/08/31/Network-chapter5/index.html","hash":"11f51aa20a51ec68c56555ca0e53398888f96133","modified":1567584386435},{"_id":"public/2019/08/30/Network-chapter4/index.html","hash":"fd62f3f3acb65e56474ccc68f59b129f9a89ccd3","modified":1567584386435},{"_id":"public/2019/08/24/Network-chapter3/index.html","hash":"ed159a40069b77944443ec5a4907726efafb62c3","modified":1567584386435},{"_id":"public/2019/07/24/Network-chapter2/index.html","hash":"684498cbcffb86a872669c3f7746a96361ddc968","modified":1567584386435},{"_id":"public/2019/07/23/Network-chapter1/index.html","hash":"02f674a0198f2693743a27a4344459a4a94b0981","modified":1567584386435},{"_id":"public/2019/06/27/Transformer/index.html","hash":"886ec77264f02bce0bbedb8c6df92f47b447da44","modified":1567584386435},{"_id":"public/2019/06/21/腓尼基/index.html","hash":"ad732e136a9bbe13439e0907ebc7425377d4934a","modified":1567584386435},{"_id":"public/2019/06/19/亚非语系/index.html","hash":"2ea15615042244daa309931b85d905c9e4ca6182","modified":1567584386435},{"_id":"public/2019/06/19/闪米特/index.html","hash":"ed4c05131f930a207ed2ee67867641c0e2f28270","modified":1567584386435},{"_id":"public/2019/06/19/亚述/index.html","hash":"26002e10f955a8294c109bd3ae823c6b757b74c9","modified":1567584386435},{"_id":"public/2019/06/11/AI-chapter2/index.html","hash":"88b94f1e011178595850b201ed6488c4ff744da0","modified":1567584386435},{"_id":"public/2019/06/10/AI-Chapter1/index.html","hash":"331b0b6abe868a701ad11b10989834fb3da8316a","modified":1567584386435},{"_id":"public/2019/06/05/Deep-Learning/index.html","hash":"def58e5233717df39cfc41366a270516614ec957","modified":1567584386435},{"_id":"public/2019/06/05/Kafka/index.html","hash":"c8ee76b512f93428c7e259109855d3eb549e71ca","modified":1567584386435},{"_id":"public/2019/06/03/Exercise-2/index.html","hash":"30acfc0dc0150b2b68cef6c8985b34f3620487b1","modified":1567584386435},{"_id":"public/2019/06/03/Maximum-entropy-model/index.html","hash":"82d43b7efb61e9c0eaf1e0678c2d95467953454c","modified":1567584386436},{"_id":"public/2019/06/03/Logistic-regression/index.html","hash":"7a99ef312ea87131837c5ff51a90e09e7e061de1","modified":1567584386436},{"_id":"public/2019/05/30/Exercise-1/index.html","hash":"ce295e3fad643afcaa4363ec97432487684fff89","modified":1567584386436},{"_id":"public/2019/05/30/Basic-Concept/index.html","hash":"8426b6e82eccb3c6315b4cdd8314f3f88192d991","modified":1567584386436},{"_id":"public/2019/05/29/Decision-Tree/index.html","hash":"856d28249599d13db7b84725271da48170597312","modified":1567584386436},{"_id":"public/2019/05/29/Perceptron/index.html","hash":"aee9ee84be23e5e86de16af9538bfba5a6ffaf7d","modified":1567584386436},{"_id":"public/2019/05/29/KNN/index.html","hash":"e54fcfcd1a895e9e1aae4cbc009b8f31badea7d7","modified":1567584386436},{"_id":"public/2019/05/27/Naive-Bayesian/index.html","hash":"1765e74aa5d3cb5bed402dcf241ce267f8096750","modified":1567584386436},{"_id":"public/archives/index.html","hash":"65ad226c6facd0129982b567e4bbddeca5f50aac","modified":1567584386436},{"_id":"public/archives/page/2/index.html","hash":"5701664d3837d67e71deee7146a3d5c86a9235b4","modified":1567584386436},{"_id":"public/archives/2019/index.html","hash":"c413173614a5c8afbe0e2479deb9da7c7b56eaf9","modified":1567584386436},{"_id":"public/archives/2019/page/2/index.html","hash":"cf2ad36935b5eca387c208b3f30663492c9741e0","modified":1567584386436},{"_id":"public/archives/2019/06/index.html","hash":"7341f368f69f1d2e0d7a99f206590841d8170112","modified":1567584386436},{"_id":"public/index.html","hash":"3398242507c82b5650c52ceebd69fd5f533f0f42","modified":1567584386436},{"_id":"public/page/2/index.html","hash":"4254ed5e18ece951a66ab692c6658d11022c3a6c","modified":1567584386436},{"_id":"public/page/3/index.html","hash":"069f4fbef827eb68f7aa20a513acde95d8587e3c","modified":1567584386436},{"_id":"public/tags/machine-learning/index.html","hash":"af4fff912f7bfce210bfdf5a551b391b7e9094d4","modified":1567584386436},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1567584386442},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1567584386442},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1567584386442},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1567584386442},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1567584386442},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1567584386442},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1567584386442},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1567584386442},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1567584386442},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1567584386442},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1567584386442},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1567584386442},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1567584386442},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1567584386442},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1567584386443},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1567584386443},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1567584386443},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1567584386443},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1567584386443},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1567584386443},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1567584386443},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1567584386443},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1567584386443},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1567584386443},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1567584386443},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1567584386443},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1567584386443},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1567584386443},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1567584386443},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1567584386443},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1567584386443},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1567584386443},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1567584386443},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1567584386792},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1567584386798},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1567584386808},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1567584386808},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1567584386809},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1567584386809},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1567584386809},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1567584386809},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1567584386809},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1567584386809},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1567584386809},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1567584386809},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1567584386809},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1567584386810},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1567584386810},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1567584386810},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1567584386810},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1567584386810},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1567584386810},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1567584386810},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1567584386810},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1567584386810},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1567584386810},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1567584386810},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1567584386810},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1567584386810},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1567584386810},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1567584386811},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1567584386811},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1567584386811},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1567584386811},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1567584386811},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1567584386811},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1567584386811},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1567584386811},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1567584386811},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1567584386812},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1567584386812},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1567584386812},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1567584386812},{"_id":"public/lib/fastclick/README.html","hash":"d6e90449a2c09f3033f7e43d68b0cc8208e22e09","modified":1567584386812},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"06811ca2f722dead021493457f27cdc264ef928d","modified":1567584386812},{"_id":"public/lib/jquery_lazyload/README.html","hash":"a08fccd381c8fdb70ba8974b208254c5ba23a95f","modified":1567584386812},{"_id":"public/css/main.css","hash":"228c1cf65681ea0812c43e6576ce49861ef86126","modified":1567584386812},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1567584386812},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1567584386812},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1567584386824},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1567584386824},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1567584386824},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1567584386824},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1567584386824},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1567584386824},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1567584386824},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1567584386825},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1567584386828},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1567584386828},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1567584386832},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1567584386833},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1567584386841},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1567584386841},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1567584386842},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1567584386842},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1567584386842},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1567584386842},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1567584386842},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1567584386842},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1567584386856},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1567584386857},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1567584386857},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1567584386861},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1567584386863},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1567584386869},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1567584386871},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1567584386872},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1567584386877},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1567584386879}],"Category":[{"name":"统计学习方法","_id":"ck04z9nsf0004z4o0lh69hyjp"},{"name":"AI-an answer of the modern method","_id":"ck04z9nsk0009z4o0tbxjjdiy"},{"name":"papers","_id":"ck04z9nss000oz4o0jt8egsk9"},{"name":"big data","_id":"ck04z9nsv000wz4o05ykeajw1"},{"name":"Computer Network A Systems Approach","_id":"ck04z9nsy0016z4o0ef59plmh"},{"name":"solved problem","_id":"ck04z9nt6001vz4o0orpp30e1"},{"name":"语言学-亚非语系","_id":"ck04z9nt80024z4o0dexrexzy"},{"name":"语言学","_id":"ck04z9ntc002lz4o0fl12n48c"},{"name":"NLP","_id":"ck04z9ntd002qz4o0uj697nd7"}],"Data":[],"Page":[{"title":"categories","date":"2019-05-26T04:33:46.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2019-05-26 12:33:46\ntype: \"categories\"\nlayout: \"categories\"\n---\n","updated":"2019-05-26T04:34:28.802Z","path":"categories/index.html","comments":1,"_id":"ck04z9ns70000z4o0vrgyent1","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2019-05-26T04:34:45.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2019-05-26 12:34:45\ntype: \"tags\"\nlayout: \"tags\"\n---\n","updated":"2019-05-26T04:35:14.077Z","path":"tags/index.html","comments":1,"_id":"ck04z9nsc0002z4o0sdm7245s","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Basic Concept","date":"2019-05-30T00:38:07.000Z","mathjax":true,"_content":"#### 机器学习中的一些基本概念\n##### 贝叶斯学习\n&emsp;&emsp;在概率模型的学习中, 利用贝叶斯定理, 计算后验概率,并进行模型估计, 对数据进行预测.将模型,未观测要素及其参数用变量表示.基本公式如下:\n$$P(\\theta |D)=\\frac{P(\\theta)P(D|\\theta)}{P(D)}$$\n&emsp;&emsp;其中$D$是数据, $\\theta$是模型参数, $P(\\theta)$是先验概率, $P(D|\\theta)$是似然函数, $P(\\theta |D)$是后验概率. 通常是取后验概率最大的模型.\n\n----\n##### 经验风险最小化和结构风险最小化\n&emsp;&emsp;在数据量足够大的时候,经验风险趋于期望风险,这时经验风险最小化就是求解模型最优化等价问题.\n$$min_{f\\in \\mathcal{F}}\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))$$\n&emsp;&emsp;但是当样本量较小时,会存在过拟合现象.这时需要引入结构风险来防止过拟合.\n$$min_{f\\in \\mathcal{F}}\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))+\\lambda J(f)$$\n&emsp;&emsp;其中$J(f)$是模型复杂度,经典的方法是使用一个正则化项(如一范数,二范数),$\\lambda$是用来权衡经验风险和模型复杂度的系数\n\n&emsp;&emsp;极大似然估计是经验风险最小化的例子.也就是损失函数是对数损失函数时,极大似然估计等价于经验风险最小化；贝叶斯估计中的最大后验概率估计是结构风险最小化的例子.也就是损失函数是对数损失函数,模型复杂度由模型先验概率表示时,最大后验概率估计等价于结构风险最小化.\n\n-----\n##### 模型泛化\n&emsp;&emsp;泛化误差实际上就是学习到模型的期望风险.泛化能力分析通常是求泛化误差上界.# Todo(公式推导)\n\n-----\n##### 生成模型和判别模型\n&emsp;&emsp;生成模型是由数据学习联合概率分布$P(X,Y)$,然后求出条件概率分布$P(Y|X)$作为预测的模型.典型的生成模型有朴素贝叶斯和隐马尔可夫模型.\n$$P(Y|X)=\\frac{P(X,Y)}{P(X)}$$\n&emsp;&emsp;判别模型是由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型.\n\n&emsp;&emsp;显然生成模型关注输入$X$产生输出$Y$的生成关系,而判别模型关心给定输入$X$,应该预测什么样的输出$Y$；当样本容量增加时,生成模型的收敛速度更快；存在隐变量的时候,就只能使用生成模型；判别模型能够简化学习问题,对数据进行一定程度的抽象.\n\n----\n##### 评价指标\n样本\\预测 | 0 | 1\n:---: | :---: | :---:\n0 | TN | FP\n1 | FN | TP\n\n精确率(precision): $P=\\frac{TP}{TP+FP}$\n\n准确率(accuracy): $A=\\frac{TP+TN}{TP+FP+FN+TN}$\n\n召回率(recall): $R=\\frac{TP}{TP+FN}$\n\nF1值: $F_1=\\frac{2PR}{P+R}=\\frac{2TP}{2TP+FP+FN}$ ","source":"_posts/Basic-Concept.md","raw":"---\ntitle: Basic Concept\ndate: 2019-05-30 08:38:07\ncategories: 统计学习方法\ntags: machine learning\nmathjax: true\n---\n#### 机器学习中的一些基本概念\n##### 贝叶斯学习\n&emsp;&emsp;在概率模型的学习中, 利用贝叶斯定理, 计算后验概率,并进行模型估计, 对数据进行预测.将模型,未观测要素及其参数用变量表示.基本公式如下:\n$$P(\\theta |D)=\\frac{P(\\theta)P(D|\\theta)}{P(D)}$$\n&emsp;&emsp;其中$D$是数据, $\\theta$是模型参数, $P(\\theta)$是先验概率, $P(D|\\theta)$是似然函数, $P(\\theta |D)$是后验概率. 通常是取后验概率最大的模型.\n\n----\n##### 经验风险最小化和结构风险最小化\n&emsp;&emsp;在数据量足够大的时候,经验风险趋于期望风险,这时经验风险最小化就是求解模型最优化等价问题.\n$$min_{f\\in \\mathcal{F}}\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))$$\n&emsp;&emsp;但是当样本量较小时,会存在过拟合现象.这时需要引入结构风险来防止过拟合.\n$$min_{f\\in \\mathcal{F}}\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))+\\lambda J(f)$$\n&emsp;&emsp;其中$J(f)$是模型复杂度,经典的方法是使用一个正则化项(如一范数,二范数),$\\lambda$是用来权衡经验风险和模型复杂度的系数\n\n&emsp;&emsp;极大似然估计是经验风险最小化的例子.也就是损失函数是对数损失函数时,极大似然估计等价于经验风险最小化；贝叶斯估计中的最大后验概率估计是结构风险最小化的例子.也就是损失函数是对数损失函数,模型复杂度由模型先验概率表示时,最大后验概率估计等价于结构风险最小化.\n\n-----\n##### 模型泛化\n&emsp;&emsp;泛化误差实际上就是学习到模型的期望风险.泛化能力分析通常是求泛化误差上界.# Todo(公式推导)\n\n-----\n##### 生成模型和判别模型\n&emsp;&emsp;生成模型是由数据学习联合概率分布$P(X,Y)$,然后求出条件概率分布$P(Y|X)$作为预测的模型.典型的生成模型有朴素贝叶斯和隐马尔可夫模型.\n$$P(Y|X)=\\frac{P(X,Y)}{P(X)}$$\n&emsp;&emsp;判别模型是由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型.\n\n&emsp;&emsp;显然生成模型关注输入$X$产生输出$Y$的生成关系,而判别模型关心给定输入$X$,应该预测什么样的输出$Y$；当样本容量增加时,生成模型的收敛速度更快；存在隐变量的时候,就只能使用生成模型；判别模型能够简化学习问题,对数据进行一定程度的抽象.\n\n----\n##### 评价指标\n样本\\预测 | 0 | 1\n:---: | :---: | :---:\n0 | TN | FP\n1 | FN | TP\n\n精确率(precision): $P=\\frac{TP}{TP+FP}$\n\n准确率(accuracy): $A=\\frac{TP+TN}{TP+FP+FN+TN}$\n\n召回率(recall): $R=\\frac{TP}{TP+FN}$\n\nF1值: $F_1=\\frac{2PR}{P+R}=\\frac{2TP}{2TP+FP+FN}$ ","slug":"Basic-Concept","published":1,"updated":"2019-06-10T07:42:08.381Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9ns80001z4o03x3k0nbd","content":"<h4 id=\"机器学习中的一些基本概念\"><a href=\"#机器学习中的一些基本概念\" class=\"headerlink\" title=\"机器学习中的一些基本概念\"></a>机器学习中的一些基本概念</h4><h5 id=\"贝叶斯学习\"><a href=\"#贝叶斯学习\" class=\"headerlink\" title=\"贝叶斯学习\"></a>贝叶斯学习</h5><p>&emsp;&emsp;在概率模型的学习中, 利用贝叶斯定理, 计算后验概率,并进行模型估计, 对数据进行预测.将模型,未观测要素及其参数用变量表示.基本公式如下:</p>\n<script type=\"math/tex; mode=display\">P(\\theta |D)=\\frac{P(\\theta)P(D|\\theta)}{P(D)}</script><p>&emsp;&emsp;其中$D$是数据, $\\theta$是模型参数, $P(\\theta)$是先验概率, $P(D|\\theta)$是似然函数, $P(\\theta |D)$是后验概率. 通常是取后验概率最大的模型.</p>\n<hr>\n<h5 id=\"经验风险最小化和结构风险最小化\"><a href=\"#经验风险最小化和结构风险最小化\" class=\"headerlink\" title=\"经验风险最小化和结构风险最小化\"></a>经验风险最小化和结构风险最小化</h5><p>&emsp;&emsp;在数据量足够大的时候,经验风险趋于期望风险,这时经验风险最小化就是求解模型最优化等价问题.</p>\n<script type=\"math/tex; mode=display\">min_{f\\in \\mathcal{F}}\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))</script><p>&emsp;&emsp;但是当样本量较小时,会存在过拟合现象.这时需要引入结构风险来防止过拟合.</p>\n<script type=\"math/tex; mode=display\">min_{f\\in \\mathcal{F}}\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))+\\lambda J(f)</script><p>&emsp;&emsp;其中$J(f)$是模型复杂度,经典的方法是使用一个正则化项(如一范数,二范数),$\\lambda$是用来权衡经验风险和模型复杂度的系数</p>\n<p>&emsp;&emsp;极大似然估计是经验风险最小化的例子.也就是损失函数是对数损失函数时,极大似然估计等价于经验风险最小化；贝叶斯估计中的最大后验概率估计是结构风险最小化的例子.也就是损失函数是对数损失函数,模型复杂度由模型先验概率表示时,最大后验概率估计等价于结构风险最小化.</p>\n<hr>\n<h5 id=\"模型泛化\"><a href=\"#模型泛化\" class=\"headerlink\" title=\"模型泛化\"></a>模型泛化</h5><p>&emsp;&emsp;泛化误差实际上就是学习到模型的期望风险.泛化能力分析通常是求泛化误差上界.# Todo(公式推导)</p>\n<hr>\n<h5 id=\"生成模型和判别模型\"><a href=\"#生成模型和判别模型\" class=\"headerlink\" title=\"生成模型和判别模型\"></a>生成模型和判别模型</h5><p>&emsp;&emsp;生成模型是由数据学习联合概率分布$P(X,Y)$,然后求出条件概率分布$P(Y|X)$作为预测的模型.典型的生成模型有朴素贝叶斯和隐马尔可夫模型.</p>\n<script type=\"math/tex; mode=display\">P(Y|X)=\\frac{P(X,Y)}{P(X)}</script><p>&emsp;&emsp;判别模型是由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型.</p>\n<p>&emsp;&emsp;显然生成模型关注输入$X$产生输出$Y$的生成关系,而判别模型关心给定输入$X$,应该预测什么样的输出$Y$；当样本容量增加时,生成模型的收敛速度更快；存在隐变量的时候,就只能使用生成模型；判别模型能够简化学习问题,对数据进行一定程度的抽象.</p>\n<hr>\n<h5 id=\"评价指标\"><a href=\"#评价指标\" class=\"headerlink\" title=\"评价指标\"></a>评价指标</h5><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">样本\\预测</th>\n<th style=\"text-align:center\">0</th>\n<th style=\"text-align:center\">1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">TN</td>\n<td style=\"text-align:center\">FP</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">FN</td>\n<td style=\"text-align:center\">TP</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>精确率(precision): $P=\\frac{TP}{TP+FP}$</p>\n<p>准确率(accuracy): $A=\\frac{TP+TN}{TP+FP+FN+TN}$</p>\n<p>召回率(recall): $R=\\frac{TP}{TP+FN}$</p>\n<p>F1值: $F_1=\\frac{2PR}{P+R}=\\frac{2TP}{2TP+FP+FN}$ </p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"机器学习中的一些基本概念\"><a href=\"#机器学习中的一些基本概念\" class=\"headerlink\" title=\"机器学习中的一些基本概念\"></a>机器学习中的一些基本概念</h4><h5 id=\"贝叶斯学习\"><a href=\"#贝叶斯学习\" class=\"headerlink\" title=\"贝叶斯学习\"></a>贝叶斯学习</h5><p>&emsp;&emsp;在概率模型的学习中, 利用贝叶斯定理, 计算后验概率,并进行模型估计, 对数据进行预测.将模型,未观测要素及其参数用变量表示.基本公式如下:</p>\n<script type=\"math/tex; mode=display\">P(\\theta |D)=\\frac{P(\\theta)P(D|\\theta)}{P(D)}</script><p>&emsp;&emsp;其中$D$是数据, $\\theta$是模型参数, $P(\\theta)$是先验概率, $P(D|\\theta)$是似然函数, $P(\\theta |D)$是后验概率. 通常是取后验概率最大的模型.</p>\n<hr>\n<h5 id=\"经验风险最小化和结构风险最小化\"><a href=\"#经验风险最小化和结构风险最小化\" class=\"headerlink\" title=\"经验风险最小化和结构风险最小化\"></a>经验风险最小化和结构风险最小化</h5><p>&emsp;&emsp;在数据量足够大的时候,经验风险趋于期望风险,这时经验风险最小化就是求解模型最优化等价问题.</p>\n<script type=\"math/tex; mode=display\">min_{f\\in \\mathcal{F}}\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))</script><p>&emsp;&emsp;但是当样本量较小时,会存在过拟合现象.这时需要引入结构风险来防止过拟合.</p>\n<script type=\"math/tex; mode=display\">min_{f\\in \\mathcal{F}}\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))+\\lambda J(f)</script><p>&emsp;&emsp;其中$J(f)$是模型复杂度,经典的方法是使用一个正则化项(如一范数,二范数),$\\lambda$是用来权衡经验风险和模型复杂度的系数</p>\n<p>&emsp;&emsp;极大似然估计是经验风险最小化的例子.也就是损失函数是对数损失函数时,极大似然估计等价于经验风险最小化；贝叶斯估计中的最大后验概率估计是结构风险最小化的例子.也就是损失函数是对数损失函数,模型复杂度由模型先验概率表示时,最大后验概率估计等价于结构风险最小化.</p>\n<hr>\n<h5 id=\"模型泛化\"><a href=\"#模型泛化\" class=\"headerlink\" title=\"模型泛化\"></a>模型泛化</h5><p>&emsp;&emsp;泛化误差实际上就是学习到模型的期望风险.泛化能力分析通常是求泛化误差上界.# Todo(公式推导)</p>\n<hr>\n<h5 id=\"生成模型和判别模型\"><a href=\"#生成模型和判别模型\" class=\"headerlink\" title=\"生成模型和判别模型\"></a>生成模型和判别模型</h5><p>&emsp;&emsp;生成模型是由数据学习联合概率分布$P(X,Y)$,然后求出条件概率分布$P(Y|X)$作为预测的模型.典型的生成模型有朴素贝叶斯和隐马尔可夫模型.</p>\n<script type=\"math/tex; mode=display\">P(Y|X)=\\frac{P(X,Y)}{P(X)}</script><p>&emsp;&emsp;判别模型是由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型.</p>\n<p>&emsp;&emsp;显然生成模型关注输入$X$产生输出$Y$的生成关系,而判别模型关心给定输入$X$,应该预测什么样的输出$Y$；当样本容量增加时,生成模型的收敛速度更快；存在隐变量的时候,就只能使用生成模型；判别模型能够简化学习问题,对数据进行一定程度的抽象.</p>\n<hr>\n<h5 id=\"评价指标\"><a href=\"#评价指标\" class=\"headerlink\" title=\"评价指标\"></a>评价指标</h5><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">样本\\预测</th>\n<th style=\"text-align:center\">0</th>\n<th style=\"text-align:center\">1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">TN</td>\n<td style=\"text-align:center\">FP</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">FN</td>\n<td style=\"text-align:center\">TP</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>精确率(precision): $P=\\frac{TP}{TP+FP}$</p>\n<p>准确率(accuracy): $A=\\frac{TP+TN}{TP+FP+FN+TN}$</p>\n<p>召回率(recall): $R=\\frac{TP}{TP+FN}$</p>\n<p>F1值: $F_1=\\frac{2PR}{P+R}=\\frac{2TP}{2TP+FP+FN}$ </p>\n"},{"title":"AI-Chapter1","date":"2019-06-10T07:08:54.000Z","visitors":null,"mathjax":true,"_content":"第一章的习题:\n\n#### 1.1\n##### a) 智能: 能够按照人类一样思考和判断.\n##### b) 人工智能: 制造能够理解智能实体的实体.\n##### c) Agent: 能够行动的某种东西,可以是机械臂或者是机器人等.\n##### d) 理性: 思维方式和行动方式合理并且符合逻辑.\n##### e) 逻辑推理: 严密按照逻辑规则从初始条件机械地推导出结论.\n#### 1.3\n&emsp;&emsp;反射行动是理性的,因为理性是行为和思考合理.但是不是智能的,因为反射行动不包含逻辑思考,而判断是否智能的关键是能否像人类一样的思考和行动,反射行动即使是无脊椎动物也具有.\n#### 1.4\n&emsp;&emsp;我认为不是的,智商测试主要测试的是人的逻辑思维,推理能力等,如果ANALOGY程序在测试中比人类的得分更高,那么可以得出ANALOGY在逻辑思维,推理能力上比人类的表现更优秀,但是像人类一样思考和判断并不代表人类的思维方式是机械的完全理性的思维方式.因此我认为这只能表示这个程序在逻辑思维能力上比人类表现更好,但是并不代表它是一个比人更智能的程序.\n#### 1.7\n&emsp;&emsp;超市条码扫描器和网络搜索引擎不是人工智能,语音激活和对网络状态动态相应的网络路由算法是人工智能.\n#### 1.8\n&emsp;&emsp;我认为现有的模型都是用一个复杂的映射关系逼近人类的认知过程,这些复杂的数学工具只是实现的手段.现在流行的神经网络方法在可解释性上还比较差,而人类的认知过程也没有完全解析,因此还是一个黑盒.\n#### 1.9\n&emsp;&emsp;生物的进化是随着环境的改变而改变的,对人来说,既包括自然环境也包括社会环境.进化的意义在于更好地适应环境生存下去.系统一般是趋向于熵增的方向.如果生物不进化,那么只有可能全部死亡或是全部生存.进化让生物也向熵增的方向发展,一部分不适应环境的基因会消失,而适应环境的基因会存续.\n#### 1.10\n&emsp;&emsp;AI是一门科学,也是一门工程.首先AI需要用机器来重现人类的认知和行为方式,需要用到心理学,生物学等不同学科的领域知识来研究人类认知过程.同时也需要用到计算机工程,机器人等工程学领域知识将AI进行实现.","source":"_posts/AI-Chapter1.md","raw":"---\ntitle: AI-Chapter1\ndate: 2019-06-10 15:08:54\ncategories: AI-an answer of the modern method\nvisitors: \nmathjax: true\ntags: AI\n---\n第一章的习题:\n\n#### 1.1\n##### a) 智能: 能够按照人类一样思考和判断.\n##### b) 人工智能: 制造能够理解智能实体的实体.\n##### c) Agent: 能够行动的某种东西,可以是机械臂或者是机器人等.\n##### d) 理性: 思维方式和行动方式合理并且符合逻辑.\n##### e) 逻辑推理: 严密按照逻辑规则从初始条件机械地推导出结论.\n#### 1.3\n&emsp;&emsp;反射行动是理性的,因为理性是行为和思考合理.但是不是智能的,因为反射行动不包含逻辑思考,而判断是否智能的关键是能否像人类一样的思考和行动,反射行动即使是无脊椎动物也具有.\n#### 1.4\n&emsp;&emsp;我认为不是的,智商测试主要测试的是人的逻辑思维,推理能力等,如果ANALOGY程序在测试中比人类的得分更高,那么可以得出ANALOGY在逻辑思维,推理能力上比人类的表现更优秀,但是像人类一样思考和判断并不代表人类的思维方式是机械的完全理性的思维方式.因此我认为这只能表示这个程序在逻辑思维能力上比人类表现更好,但是并不代表它是一个比人更智能的程序.\n#### 1.7\n&emsp;&emsp;超市条码扫描器和网络搜索引擎不是人工智能,语音激活和对网络状态动态相应的网络路由算法是人工智能.\n#### 1.8\n&emsp;&emsp;我认为现有的模型都是用一个复杂的映射关系逼近人类的认知过程,这些复杂的数学工具只是实现的手段.现在流行的神经网络方法在可解释性上还比较差,而人类的认知过程也没有完全解析,因此还是一个黑盒.\n#### 1.9\n&emsp;&emsp;生物的进化是随着环境的改变而改变的,对人来说,既包括自然环境也包括社会环境.进化的意义在于更好地适应环境生存下去.系统一般是趋向于熵增的方向.如果生物不进化,那么只有可能全部死亡或是全部生存.进化让生物也向熵增的方向发展,一部分不适应环境的基因会消失,而适应环境的基因会存续.\n#### 1.10\n&emsp;&emsp;AI是一门科学,也是一门工程.首先AI需要用机器来重现人类的认知和行为方式,需要用到心理学,生物学等不同学科的领域知识来研究人类认知过程.同时也需要用到计算机工程,机器人等工程学领域知识将AI进行实现.","slug":"AI-Chapter1","published":1,"updated":"2019-06-10T08:31:02.714Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsc0003z4o09tb4rjbz","content":"<p>第一章的习题:</p>\n<h4 id=\"1-1\"><a href=\"#1-1\" class=\"headerlink\" title=\"1.1\"></a>1.1</h4><h5 id=\"a-智能-能够按照人类一样思考和判断\"><a href=\"#a-智能-能够按照人类一样思考和判断\" class=\"headerlink\" title=\"a) 智能: 能够按照人类一样思考和判断.\"></a>a) 智能: 能够按照人类一样思考和判断.</h5><h5 id=\"b-人工智能-制造能够理解智能实体的实体\"><a href=\"#b-人工智能-制造能够理解智能实体的实体\" class=\"headerlink\" title=\"b) 人工智能: 制造能够理解智能实体的实体.\"></a>b) 人工智能: 制造能够理解智能实体的实体.</h5><h5 id=\"c-Agent-能够行动的某种东西-可以是机械臂或者是机器人等\"><a href=\"#c-Agent-能够行动的某种东西-可以是机械臂或者是机器人等\" class=\"headerlink\" title=\"c) Agent: 能够行动的某种东西,可以是机械臂或者是机器人等.\"></a>c) Agent: 能够行动的某种东西,可以是机械臂或者是机器人等.</h5><h5 id=\"d-理性-思维方式和行动方式合理并且符合逻辑\"><a href=\"#d-理性-思维方式和行动方式合理并且符合逻辑\" class=\"headerlink\" title=\"d) 理性: 思维方式和行动方式合理并且符合逻辑.\"></a>d) 理性: 思维方式和行动方式合理并且符合逻辑.</h5><h5 id=\"e-逻辑推理-严密按照逻辑规则从初始条件机械地推导出结论\"><a href=\"#e-逻辑推理-严密按照逻辑规则从初始条件机械地推导出结论\" class=\"headerlink\" title=\"e) 逻辑推理: 严密按照逻辑规则从初始条件机械地推导出结论.\"></a>e) 逻辑推理: 严密按照逻辑规则从初始条件机械地推导出结论.</h5><h4 id=\"1-3\"><a href=\"#1-3\" class=\"headerlink\" title=\"1.3\"></a>1.3</h4><p>&emsp;&emsp;反射行动是理性的,因为理性是行为和思考合理.但是不是智能的,因为反射行动不包含逻辑思考,而判断是否智能的关键是能否像人类一样的思考和行动,反射行动即使是无脊椎动物也具有.</p>\n<h4 id=\"1-4\"><a href=\"#1-4\" class=\"headerlink\" title=\"1.4\"></a>1.4</h4><p>&emsp;&emsp;我认为不是的,智商测试主要测试的是人的逻辑思维,推理能力等,如果ANALOGY程序在测试中比人类的得分更高,那么可以得出ANALOGY在逻辑思维,推理能力上比人类的表现更优秀,但是像人类一样思考和判断并不代表人类的思维方式是机械的完全理性的思维方式.因此我认为这只能表示这个程序在逻辑思维能力上比人类表现更好,但是并不代表它是一个比人更智能的程序.</p>\n<h4 id=\"1-7\"><a href=\"#1-7\" class=\"headerlink\" title=\"1.7\"></a>1.7</h4><p>&emsp;&emsp;超市条码扫描器和网络搜索引擎不是人工智能,语音激活和对网络状态动态相应的网络路由算法是人工智能.</p>\n<h4 id=\"1-8\"><a href=\"#1-8\" class=\"headerlink\" title=\"1.8\"></a>1.8</h4><p>&emsp;&emsp;我认为现有的模型都是用一个复杂的映射关系逼近人类的认知过程,这些复杂的数学工具只是实现的手段.现在流行的神经网络方法在可解释性上还比较差,而人类的认知过程也没有完全解析,因此还是一个黑盒.</p>\n<h4 id=\"1-9\"><a href=\"#1-9\" class=\"headerlink\" title=\"1.9\"></a>1.9</h4><p>&emsp;&emsp;生物的进化是随着环境的改变而改变的,对人来说,既包括自然环境也包括社会环境.进化的意义在于更好地适应环境生存下去.系统一般是趋向于熵增的方向.如果生物不进化,那么只有可能全部死亡或是全部生存.进化让生物也向熵增的方向发展,一部分不适应环境的基因会消失,而适应环境的基因会存续.</p>\n<h4 id=\"1-10\"><a href=\"#1-10\" class=\"headerlink\" title=\"1.10\"></a>1.10</h4><p>&emsp;&emsp;AI是一门科学,也是一门工程.首先AI需要用机器来重现人类的认知和行为方式,需要用到心理学,生物学等不同学科的领域知识来研究人类认知过程.同时也需要用到计算机工程,机器人等工程学领域知识将AI进行实现.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>第一章的习题:</p>\n<h4 id=\"1-1\"><a href=\"#1-1\" class=\"headerlink\" title=\"1.1\"></a>1.1</h4><h5 id=\"a-智能-能够按照人类一样思考和判断\"><a href=\"#a-智能-能够按照人类一样思考和判断\" class=\"headerlink\" title=\"a) 智能: 能够按照人类一样思考和判断.\"></a>a) 智能: 能够按照人类一样思考和判断.</h5><h5 id=\"b-人工智能-制造能够理解智能实体的实体\"><a href=\"#b-人工智能-制造能够理解智能实体的实体\" class=\"headerlink\" title=\"b) 人工智能: 制造能够理解智能实体的实体.\"></a>b) 人工智能: 制造能够理解智能实体的实体.</h5><h5 id=\"c-Agent-能够行动的某种东西-可以是机械臂或者是机器人等\"><a href=\"#c-Agent-能够行动的某种东西-可以是机械臂或者是机器人等\" class=\"headerlink\" title=\"c) Agent: 能够行动的某种东西,可以是机械臂或者是机器人等.\"></a>c) Agent: 能够行动的某种东西,可以是机械臂或者是机器人等.</h5><h5 id=\"d-理性-思维方式和行动方式合理并且符合逻辑\"><a href=\"#d-理性-思维方式和行动方式合理并且符合逻辑\" class=\"headerlink\" title=\"d) 理性: 思维方式和行动方式合理并且符合逻辑.\"></a>d) 理性: 思维方式和行动方式合理并且符合逻辑.</h5><h5 id=\"e-逻辑推理-严密按照逻辑规则从初始条件机械地推导出结论\"><a href=\"#e-逻辑推理-严密按照逻辑规则从初始条件机械地推导出结论\" class=\"headerlink\" title=\"e) 逻辑推理: 严密按照逻辑规则从初始条件机械地推导出结论.\"></a>e) 逻辑推理: 严密按照逻辑规则从初始条件机械地推导出结论.</h5><h4 id=\"1-3\"><a href=\"#1-3\" class=\"headerlink\" title=\"1.3\"></a>1.3</h4><p>&emsp;&emsp;反射行动是理性的,因为理性是行为和思考合理.但是不是智能的,因为反射行动不包含逻辑思考,而判断是否智能的关键是能否像人类一样的思考和行动,反射行动即使是无脊椎动物也具有.</p>\n<h4 id=\"1-4\"><a href=\"#1-4\" class=\"headerlink\" title=\"1.4\"></a>1.4</h4><p>&emsp;&emsp;我认为不是的,智商测试主要测试的是人的逻辑思维,推理能力等,如果ANALOGY程序在测试中比人类的得分更高,那么可以得出ANALOGY在逻辑思维,推理能力上比人类的表现更优秀,但是像人类一样思考和判断并不代表人类的思维方式是机械的完全理性的思维方式.因此我认为这只能表示这个程序在逻辑思维能力上比人类表现更好,但是并不代表它是一个比人更智能的程序.</p>\n<h4 id=\"1-7\"><a href=\"#1-7\" class=\"headerlink\" title=\"1.7\"></a>1.7</h4><p>&emsp;&emsp;超市条码扫描器和网络搜索引擎不是人工智能,语音激活和对网络状态动态相应的网络路由算法是人工智能.</p>\n<h4 id=\"1-8\"><a href=\"#1-8\" class=\"headerlink\" title=\"1.8\"></a>1.8</h4><p>&emsp;&emsp;我认为现有的模型都是用一个复杂的映射关系逼近人类的认知过程,这些复杂的数学工具只是实现的手段.现在流行的神经网络方法在可解释性上还比较差,而人类的认知过程也没有完全解析,因此还是一个黑盒.</p>\n<h4 id=\"1-9\"><a href=\"#1-9\" class=\"headerlink\" title=\"1.9\"></a>1.9</h4><p>&emsp;&emsp;生物的进化是随着环境的改变而改变的,对人来说,既包括自然环境也包括社会环境.进化的意义在于更好地适应环境生存下去.系统一般是趋向于熵增的方向.如果生物不进化,那么只有可能全部死亡或是全部生存.进化让生物也向熵增的方向发展,一部分不适应环境的基因会消失,而适应环境的基因会存续.</p>\n<h4 id=\"1-10\"><a href=\"#1-10\" class=\"headerlink\" title=\"1.10\"></a>1.10</h4><p>&emsp;&emsp;AI是一门科学,也是一门工程.首先AI需要用机器来重现人类的认知和行为方式,需要用到心理学,生物学等不同学科的领域知识来研究人类认知过程.同时也需要用到计算机工程,机器人等工程学领域知识将AI进行实现.</p>\n"},{"title":"Exercise 1","date":"2019-05-30T03:20:30.000Z","mathjax":true,"_content":"#### 章节1的习题解\n##### 习题1\n&emsp;&emsp;似然估计用于在已知一些参数的情况下,预测接下来在观测上所得到的结果.因为似然函数只与采样数据有关,因此也被称为采样分布.\n\n&emsp;&emsp;对于一个概率分布$D$,已知概率密度函数为$f_D$,一个参数$\\theta$,抽出有n个值的采样,则似然函数为\n$$L(x_1,...,x_n|\\theta)=f_\\theta(x_1,...,x_n)$$\n&emsp;&emsp;因此求极大似然估计就是将要估计的参数代入概率密度函数后求导.\n\n&emsp;&emsp;对于伯努利模型,将结果为1的概率$\\theta$代入概率密度函数:\n$$L(\\theta)=\\prod^n_{i=1}P(x_i)=\\theta^k(1-\\theta)^{n-k}$$\n&emsp;&emsp;对公式求导:\n$$\n\\begin{aligned}\n    \\frac{\\partial L(\\theta)}{\\partial \\theta}&=k\\theta^{k-1}(1-\\theta)^{n-k-1}-n\\theta^k(1-\\theta)^{n-k-1} \\\\\n    &=(k-n\\theta)\\theta^{k-1}(1-\\theta)^{n-k-1}\n\\end{aligned}\n$$\n&emsp;&emsp;令上式为0,得到$\\theta=\\frac{k}{n}$\n\n---\n&emsp;&emsp;贝叶斯估计的公式为:\n$$P(\\theta|x)=\\frac{P(x|\\theta)P(\\theta)}{P(x)}$$\n&emsp;&emsp;因为$P(x)$与$\\theta$无关:\n$$P(\\theta|x)\\propto P(x|\\theta)P(\\theta)$$\n&emsp;&emsp;可以看到贝叶斯估计相当于似然函数乘上一个先验分布.$t$和$r$是先验信息.将参数代入到上式:\n$$\n    \\begin{aligned}\n        argmax_\\theta P(\\theta|x)&=\\theta^k(1-\\theta)^{n-k}\\theta^t(1-\\theta)^r \\\\\n        &=\\theta^{k+t}(1-\\theta)^{n-k+r} \\\\\n    \\end{aligned}\n$$\n$$\n    \\begin{aligned}\n        \\frac{\\partial P(\\theta|x)}{\\partial\\theta}&=(k+t)\\theta^{k+t-1}(1-\\theta)^{n-k+r}-(n-k+r)\\theta^{k+t}(1-\\theta)^{n+r-k-1} \\\\\n        &=[k+t-(t+n+r)\\theta]\\theta^{k+t=1}(1-\\theta)^{n-k+r-1}\n    \\end{aligned}\n$$\n&emsp;&emsp;令上式为0,得到$\\theta=\\frac{k+t}{n+t+r}$\n\n---\n##### 习题2\n&emsp;&emsp;经验风险最小化:\n$$min_{f\\in F}=\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))$$\n&emsp;&emsp;根据条件有,损失函数为对数损失函数,即:\n$$L(y_i,f(x_i))=-logP(y_i|x_i)$$\n&emsp;&emsp;将损失函数代入,得到:\n$$min_{P}\\frac{1}{N}\\sum^N_{i=1}(-logP(y_i|x_i))=\\frac{1}{N}max_plog(\\prod^N_{i=1}P(y_i|x_i))$$\n&emsp;&emsp;去掉不相关项得到$max_p\\prod^N_{i=1}P(y_i|x_i)$. \n显然这是极大似然估计,因此在模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化等价于极大似然估计.","source":"_posts/Exercise-1.md","raw":"---\ntitle: Exercise 1\ndate: 2019-05-30 11:20:30\ncategories: 统计学习方法\ntags: machine learning\nmathjax: true\n---\n#### 章节1的习题解\n##### 习题1\n&emsp;&emsp;似然估计用于在已知一些参数的情况下,预测接下来在观测上所得到的结果.因为似然函数只与采样数据有关,因此也被称为采样分布.\n\n&emsp;&emsp;对于一个概率分布$D$,已知概率密度函数为$f_D$,一个参数$\\theta$,抽出有n个值的采样,则似然函数为\n$$L(x_1,...,x_n|\\theta)=f_\\theta(x_1,...,x_n)$$\n&emsp;&emsp;因此求极大似然估计就是将要估计的参数代入概率密度函数后求导.\n\n&emsp;&emsp;对于伯努利模型,将结果为1的概率$\\theta$代入概率密度函数:\n$$L(\\theta)=\\prod^n_{i=1}P(x_i)=\\theta^k(1-\\theta)^{n-k}$$\n&emsp;&emsp;对公式求导:\n$$\n\\begin{aligned}\n    \\frac{\\partial L(\\theta)}{\\partial \\theta}&=k\\theta^{k-1}(1-\\theta)^{n-k-1}-n\\theta^k(1-\\theta)^{n-k-1} \\\\\n    &=(k-n\\theta)\\theta^{k-1}(1-\\theta)^{n-k-1}\n\\end{aligned}\n$$\n&emsp;&emsp;令上式为0,得到$\\theta=\\frac{k}{n}$\n\n---\n&emsp;&emsp;贝叶斯估计的公式为:\n$$P(\\theta|x)=\\frac{P(x|\\theta)P(\\theta)}{P(x)}$$\n&emsp;&emsp;因为$P(x)$与$\\theta$无关:\n$$P(\\theta|x)\\propto P(x|\\theta)P(\\theta)$$\n&emsp;&emsp;可以看到贝叶斯估计相当于似然函数乘上一个先验分布.$t$和$r$是先验信息.将参数代入到上式:\n$$\n    \\begin{aligned}\n        argmax_\\theta P(\\theta|x)&=\\theta^k(1-\\theta)^{n-k}\\theta^t(1-\\theta)^r \\\\\n        &=\\theta^{k+t}(1-\\theta)^{n-k+r} \\\\\n    \\end{aligned}\n$$\n$$\n    \\begin{aligned}\n        \\frac{\\partial P(\\theta|x)}{\\partial\\theta}&=(k+t)\\theta^{k+t-1}(1-\\theta)^{n-k+r}-(n-k+r)\\theta^{k+t}(1-\\theta)^{n+r-k-1} \\\\\n        &=[k+t-(t+n+r)\\theta]\\theta^{k+t=1}(1-\\theta)^{n-k+r-1}\n    \\end{aligned}\n$$\n&emsp;&emsp;令上式为0,得到$\\theta=\\frac{k+t}{n+t+r}$\n\n---\n##### 习题2\n&emsp;&emsp;经验风险最小化:\n$$min_{f\\in F}=\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))$$\n&emsp;&emsp;根据条件有,损失函数为对数损失函数,即:\n$$L(y_i,f(x_i))=-logP(y_i|x_i)$$\n&emsp;&emsp;将损失函数代入,得到:\n$$min_{P}\\frac{1}{N}\\sum^N_{i=1}(-logP(y_i|x_i))=\\frac{1}{N}max_plog(\\prod^N_{i=1}P(y_i|x_i))$$\n&emsp;&emsp;去掉不相关项得到$max_p\\prod^N_{i=1}P(y_i|x_i)$. \n显然这是极大似然估计,因此在模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化等价于极大似然估计.","slug":"Exercise-1","published":1,"updated":"2019-06-10T07:42:00.758Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsh0006z4o00kkj22dk","content":"<h4 id=\"章节1的习题解\"><a href=\"#章节1的习题解\" class=\"headerlink\" title=\"章节1的习题解\"></a>章节1的习题解</h4><h5 id=\"习题1\"><a href=\"#习题1\" class=\"headerlink\" title=\"习题1\"></a>习题1</h5><p>&emsp;&emsp;似然估计用于在已知一些参数的情况下,预测接下来在观测上所得到的结果.因为似然函数只与采样数据有关,因此也被称为采样分布.</p>\n<p>&emsp;&emsp;对于一个概率分布$D$,已知概率密度函数为$f_D$,一个参数$\\theta$,抽出有n个值的采样,则似然函数为</p>\n<script type=\"math/tex; mode=display\">L(x_1,...,x_n|\\theta)=f_\\theta(x_1,...,x_n)</script><p>&emsp;&emsp;因此求极大似然估计就是将要估计的参数代入概率密度函数后求导.</p>\n<p>&emsp;&emsp;对于伯努利模型,将结果为1的概率$\\theta$代入概率密度函数:</p>\n<script type=\"math/tex; mode=display\">L(\\theta)=\\prod^n_{i=1}P(x_i)=\\theta^k(1-\\theta)^{n-k}</script><p>&emsp;&emsp;对公式求导:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    \\frac{\\partial L(\\theta)}{\\partial \\theta}&=k\\theta^{k-1}(1-\\theta)^{n-k-1}-n\\theta^k(1-\\theta)^{n-k-1} \\\\\n    &=(k-n\\theta)\\theta^{k-1}(1-\\theta)^{n-k-1}\n\\end{aligned}</script><p>&emsp;&emsp;令上式为0,得到$\\theta=\\frac{k}{n}$</p>\n<hr>\n<p>&emsp;&emsp;贝叶斯估计的公式为:</p>\n<script type=\"math/tex; mode=display\">P(\\theta|x)=\\frac{P(x|\\theta)P(\\theta)}{P(x)}</script><p>&emsp;&emsp;因为$P(x)$与$\\theta$无关:</p>\n<script type=\"math/tex; mode=display\">P(\\theta|x)\\propto P(x|\\theta)P(\\theta)</script><p>&emsp;&emsp;可以看到贝叶斯估计相当于似然函数乘上一个先验分布.$t$和$r$是先验信息.将参数代入到上式:</p>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        argmax_\\theta P(\\theta|x)&=\\theta^k(1-\\theta)^{n-k}\\theta^t(1-\\theta)^r \\\\\n        &=\\theta^{k+t}(1-\\theta)^{n-k+r} \\\\\n    \\end{aligned}</script><script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        \\frac{\\partial P(\\theta|x)}{\\partial\\theta}&=(k+t)\\theta^{k+t-1}(1-\\theta)^{n-k+r}-(n-k+r)\\theta^{k+t}(1-\\theta)^{n+r-k-1} \\\\\n        &=[k+t-(t+n+r)\\theta]\\theta^{k+t=1}(1-\\theta)^{n-k+r-1}\n    \\end{aligned}</script><p>&emsp;&emsp;令上式为0,得到$\\theta=\\frac{k+t}{n+t+r}$</p>\n<hr>\n<h5 id=\"习题2\"><a href=\"#习题2\" class=\"headerlink\" title=\"习题2\"></a>习题2</h5><p>&emsp;&emsp;经验风险最小化:</p>\n<script type=\"math/tex; mode=display\">min_{f\\in F}=\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))</script><p>&emsp;&emsp;根据条件有,损失函数为对数损失函数,即:</p>\n<script type=\"math/tex; mode=display\">L(y_i,f(x_i))=-logP(y_i|x_i)</script><p>&emsp;&emsp;将损失函数代入,得到:</p>\n<script type=\"math/tex; mode=display\">min_{P}\\frac{1}{N}\\sum^N_{i=1}(-logP(y_i|x_i))=\\frac{1}{N}max_plog(\\prod^N_{i=1}P(y_i|x_i))</script><p>&emsp;&emsp;去掉不相关项得到$max_p\\prod^N_{i=1}P(y_i|x_i)$.<br>显然这是极大似然估计,因此在模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化等价于极大似然估计.</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"章节1的习题解\"><a href=\"#章节1的习题解\" class=\"headerlink\" title=\"章节1的习题解\"></a>章节1的习题解</h4><h5 id=\"习题1\"><a href=\"#习题1\" class=\"headerlink\" title=\"习题1\"></a>习题1</h5><p>&emsp;&emsp;似然估计用于在已知一些参数的情况下,预测接下来在观测上所得到的结果.因为似然函数只与采样数据有关,因此也被称为采样分布.</p>\n<p>&emsp;&emsp;对于一个概率分布$D$,已知概率密度函数为$f_D$,一个参数$\\theta$,抽出有n个值的采样,则似然函数为</p>\n<script type=\"math/tex; mode=display\">L(x_1,...,x_n|\\theta)=f_\\theta(x_1,...,x_n)</script><p>&emsp;&emsp;因此求极大似然估计就是将要估计的参数代入概率密度函数后求导.</p>\n<p>&emsp;&emsp;对于伯努利模型,将结果为1的概率$\\theta$代入概率密度函数:</p>\n<script type=\"math/tex; mode=display\">L(\\theta)=\\prod^n_{i=1}P(x_i)=\\theta^k(1-\\theta)^{n-k}</script><p>&emsp;&emsp;对公式求导:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    \\frac{\\partial L(\\theta)}{\\partial \\theta}&=k\\theta^{k-1}(1-\\theta)^{n-k-1}-n\\theta^k(1-\\theta)^{n-k-1} \\\\\n    &=(k-n\\theta)\\theta^{k-1}(1-\\theta)^{n-k-1}\n\\end{aligned}</script><p>&emsp;&emsp;令上式为0,得到$\\theta=\\frac{k}{n}$</p>\n<hr>\n<p>&emsp;&emsp;贝叶斯估计的公式为:</p>\n<script type=\"math/tex; mode=display\">P(\\theta|x)=\\frac{P(x|\\theta)P(\\theta)}{P(x)}</script><p>&emsp;&emsp;因为$P(x)$与$\\theta$无关:</p>\n<script type=\"math/tex; mode=display\">P(\\theta|x)\\propto P(x|\\theta)P(\\theta)</script><p>&emsp;&emsp;可以看到贝叶斯估计相当于似然函数乘上一个先验分布.$t$和$r$是先验信息.将参数代入到上式:</p>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        argmax_\\theta P(\\theta|x)&=\\theta^k(1-\\theta)^{n-k}\\theta^t(1-\\theta)^r \\\\\n        &=\\theta^{k+t}(1-\\theta)^{n-k+r} \\\\\n    \\end{aligned}</script><script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        \\frac{\\partial P(\\theta|x)}{\\partial\\theta}&=(k+t)\\theta^{k+t-1}(1-\\theta)^{n-k+r}-(n-k+r)\\theta^{k+t}(1-\\theta)^{n+r-k-1} \\\\\n        &=[k+t-(t+n+r)\\theta]\\theta^{k+t=1}(1-\\theta)^{n-k+r-1}\n    \\end{aligned}</script><p>&emsp;&emsp;令上式为0,得到$\\theta=\\frac{k+t}{n+t+r}$</p>\n<hr>\n<h5 id=\"习题2\"><a href=\"#习题2\" class=\"headerlink\" title=\"习题2\"></a>习题2</h5><p>&emsp;&emsp;经验风险最小化:</p>\n<script type=\"math/tex; mode=display\">min_{f\\in F}=\\frac{1}{N}\\sum^N_{i=1}L(y_i,f(x_i))</script><p>&emsp;&emsp;根据条件有,损失函数为对数损失函数,即:</p>\n<script type=\"math/tex; mode=display\">L(y_i,f(x_i))=-logP(y_i|x_i)</script><p>&emsp;&emsp;将损失函数代入,得到:</p>\n<script type=\"math/tex; mode=display\">min_{P}\\frac{1}{N}\\sum^N_{i=1}(-logP(y_i|x_i))=\\frac{1}{N}max_plog(\\prod^N_{i=1}P(y_i|x_i))</script><p>&emsp;&emsp;去掉不相关项得到$max_p\\prod^N_{i=1}P(y_i|x_i)$.<br>显然这是极大似然估计,因此在模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化等价于极大似然估计.</p>\n"},{"title":"Deep Learning","date":"2019-06-05T02:00:57.000Z","updated":"2019-06-10T03:26:40.834Z","visitors":null,"mathjax":true,"_content":"&emsp;&emsp;这篇论文是2015年深度学习三巨头:Yann LeCun(Facebook AI研究院,纽约大学), Yoshua Bengio(蒙特利尔大学), Geoffrey Hinton(Google,多伦多大学)在Nature发表的综述文章.\n\n&emsp;&emsp;文中先阐述了深度学习是一种拥有多层级的特征学习方法.通过非线性单元将单层特征转换成多层级特征,这样能够得到高维,抽象程度高的复杂函数来表示特征.深度学习擅长的领域在于发掘多维数据中的复杂结构.\n\n&emsp;&emsp;之后的两章指出为什么,怎样. 因为线性分类器提取特征效果不佳$\\Rightarrow$使用深度学习来提取特征. 深度学习大多使用反向传播神经网络架构. \n\n&emsp;&emsp;接下来文中介绍了深度卷积网络在CV中的应用. 在NLP领域,文中指出了深度学习理论比起传统方法的两个优势. 一是学习到的分布式表示能够生成新的组合, 二是隐层带来的一些潜在的特征. \n\n&emsp;&emsp;这篇文章主要还是在监督学习领域讨论深度学习的发展,并没有谈及太多无监督学习和强化学习.在NLU领域希望RNN能够选择性地局部进行决策,之后的发展也沿着这个方向在做,包括之前流行的attention机制.但在最近的发展是向BERT这样重视预处理的方向.","source":"_posts/Deep-Learning.md","raw":"---\ntitle: Deep Learning\ndate: 2019-06-05 10:00:57\nupdated: \ncategories: papers\ntags:\nvisitors: \nmathjax: true\n---\n&emsp;&emsp;这篇论文是2015年深度学习三巨头:Yann LeCun(Facebook AI研究院,纽约大学), Yoshua Bengio(蒙特利尔大学), Geoffrey Hinton(Google,多伦多大学)在Nature发表的综述文章.\n\n&emsp;&emsp;文中先阐述了深度学习是一种拥有多层级的特征学习方法.通过非线性单元将单层特征转换成多层级特征,这样能够得到高维,抽象程度高的复杂函数来表示特征.深度学习擅长的领域在于发掘多维数据中的复杂结构.\n\n&emsp;&emsp;之后的两章指出为什么,怎样. 因为线性分类器提取特征效果不佳$\\Rightarrow$使用深度学习来提取特征. 深度学习大多使用反向传播神经网络架构. \n\n&emsp;&emsp;接下来文中介绍了深度卷积网络在CV中的应用. 在NLP领域,文中指出了深度学习理论比起传统方法的两个优势. 一是学习到的分布式表示能够生成新的组合, 二是隐层带来的一些潜在的特征. \n\n&emsp;&emsp;这篇文章主要还是在监督学习领域讨论深度学习的发展,并没有谈及太多无监督学习和强化学习.在NLU领域希望RNN能够选择性地局部进行决策,之后的发展也沿着这个方向在做,包括之前流行的attention机制.但在最近的发展是向BERT这样重视预处理的方向.","slug":"Deep-Learning","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsi0007z4o0meljdopx","content":"<p>&emsp;&emsp;这篇论文是2015年深度学习三巨头:Yann LeCun(Facebook AI研究院,纽约大学), Yoshua Bengio(蒙特利尔大学), Geoffrey Hinton(Google,多伦多大学)在Nature发表的综述文章.</p>\n<p>&emsp;&emsp;文中先阐述了深度学习是一种拥有多层级的特征学习方法.通过非线性单元将单层特征转换成多层级特征,这样能够得到高维,抽象程度高的复杂函数来表示特征.深度学习擅长的领域在于发掘多维数据中的复杂结构.</p>\n<p>&emsp;&emsp;之后的两章指出为什么,怎样. 因为线性分类器提取特征效果不佳$\\Rightarrow$使用深度学习来提取特征. 深度学习大多使用反向传播神经网络架构. </p>\n<p>&emsp;&emsp;接下来文中介绍了深度卷积网络在CV中的应用. 在NLP领域,文中指出了深度学习理论比起传统方法的两个优势. 一是学习到的分布式表示能够生成新的组合, 二是隐层带来的一些潜在的特征. </p>\n<p>&emsp;&emsp;这篇文章主要还是在监督学习领域讨论深度学习的发展,并没有谈及太多无监督学习和强化学习.在NLU领域希望RNN能够选择性地局部进行决策,之后的发展也沿着这个方向在做,包括之前流行的attention机制.但在最近的发展是向BERT这样重视预处理的方向.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>&emsp;&emsp;这篇论文是2015年深度学习三巨头:Yann LeCun(Facebook AI研究院,纽约大学), Yoshua Bengio(蒙特利尔大学), Geoffrey Hinton(Google,多伦多大学)在Nature发表的综述文章.</p>\n<p>&emsp;&emsp;文中先阐述了深度学习是一种拥有多层级的特征学习方法.通过非线性单元将单层特征转换成多层级特征,这样能够得到高维,抽象程度高的复杂函数来表示特征.深度学习擅长的领域在于发掘多维数据中的复杂结构.</p>\n<p>&emsp;&emsp;之后的两章指出为什么,怎样. 因为线性分类器提取特征效果不佳$\\Rightarrow$使用深度学习来提取特征. 深度学习大多使用反向传播神经网络架构. </p>\n<p>&emsp;&emsp;接下来文中介绍了深度卷积网络在CV中的应用. 在NLP领域,文中指出了深度学习理论比起传统方法的两个优势. 一是学习到的分布式表示能够生成新的组合, 二是隐层带来的一些潜在的特征. </p>\n<p>&emsp;&emsp;这篇文章主要还是在监督学习领域讨论深度学习的发展,并没有谈及太多无监督学习和强化学习.在NLU领域希望RNN能够选择性地局部进行决策,之后的发展也沿着这个方向在做,包括之前流行的attention机制.但在最近的发展是向BERT这样重视预处理的方向.</p>\n"},{"title":"Decision Tree","date":"2019-05-29T02:52:20.000Z","mathjax":true,"_content":"#### 决策树算法\n  \n决策树算法是一种简单的机器学习算法.\n\n##### 算法步骤:\n1. 如果$D$中所有实例属于同一类$C_k$,则$T$为单节点树,并将类$C_k$作为该结点的类标记.返回$T$.\n2. 如果特征集$A=\\varnothing$,则$T$为单节点树,并将$D$中实例数最大的类$C_k$作为该节点的类标记.返回$T$.\n3. 否则,计算$A$中各特征对D的信息增益或是信息增益比,选择信息增益(信息增益比)最大的特征$A_g$.\n4. 如果$A_g$的信息增益(信息增益比)小于阈值$\\epsilon$,则置$T$为单节点树,并将$D$中实例数最大的类$C_k$作为该节点的类标记,返回$T$.\n5. 否则,对$A_g$的每一可能值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记,构建子结点,由节点及其子节点构成数$T$,返回$T$.\n6. 对第$i$个子节点,以$D_i$为训练集,以$A-{A_g}$为特征集,递归地调用之前的算法,得到子树$T_i$,返回$T_i$.\n\n---\n##### 信息增益和信息增益比\n信息增益(也被称为互信息)的算法是:\n1. 计算数据集$D$的经验熵$H(D)$\n$$H(D)=-\\sum^K_{k=1}\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$$\n2. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$\n$$H(D|A)=\\sum^n_{i=1}\\frac{|D_i|}{|D|}H(D_i)=-\\sum^n_{i=1}\\frac{|D_i|}{|D|}\\sum^K_{k=1}\\frac{|D_{ik}|}{|D_i|}log_2\\frac{|D_{ik}|}{|D_i|}$$\n3. 计算信息增益\n   $$g(D,A)=H(D)-H(D|A)$$\n\n信息增益比:\n$$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$$","source":"_posts/Decision-Tree.md","raw":"---\ntitle: Decision Tree\ndate: 2019-05-29 10:52:20\ncategories: 统计学习方法\ntags: machine learning\nmathjax: true\n---\n#### 决策树算法\n  \n决策树算法是一种简单的机器学习算法.\n\n##### 算法步骤:\n1. 如果$D$中所有实例属于同一类$C_k$,则$T$为单节点树,并将类$C_k$作为该结点的类标记.返回$T$.\n2. 如果特征集$A=\\varnothing$,则$T$为单节点树,并将$D$中实例数最大的类$C_k$作为该节点的类标记.返回$T$.\n3. 否则,计算$A$中各特征对D的信息增益或是信息增益比,选择信息增益(信息增益比)最大的特征$A_g$.\n4. 如果$A_g$的信息增益(信息增益比)小于阈值$\\epsilon$,则置$T$为单节点树,并将$D$中实例数最大的类$C_k$作为该节点的类标记,返回$T$.\n5. 否则,对$A_g$的每一可能值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记,构建子结点,由节点及其子节点构成数$T$,返回$T$.\n6. 对第$i$个子节点,以$D_i$为训练集,以$A-{A_g}$为特征集,递归地调用之前的算法,得到子树$T_i$,返回$T_i$.\n\n---\n##### 信息增益和信息增益比\n信息增益(也被称为互信息)的算法是:\n1. 计算数据集$D$的经验熵$H(D)$\n$$H(D)=-\\sum^K_{k=1}\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$$\n2. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$\n$$H(D|A)=\\sum^n_{i=1}\\frac{|D_i|}{|D|}H(D_i)=-\\sum^n_{i=1}\\frac{|D_i|}{|D|}\\sum^K_{k=1}\\frac{|D_{ik}|}{|D_i|}log_2\\frac{|D_{ik}|}{|D_i|}$$\n3. 计算信息增益\n   $$g(D,A)=H(D)-H(D|A)$$\n\n信息增益比:\n$$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$$","slug":"Decision-Tree","published":1,"updated":"2019-06-10T07:42:10.928Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsj0008z4o0k46tn3pq","content":"<h4 id=\"决策树算法\"><a href=\"#决策树算法\" class=\"headerlink\" title=\"决策树算法\"></a>决策树算法</h4><p>决策树算法是一种简单的机器学习算法.</p>\n<h5 id=\"算法步骤\"><a href=\"#算法步骤\" class=\"headerlink\" title=\"算法步骤:\"></a>算法步骤:</h5><ol>\n<li>如果$D$中所有实例属于同一类$C_k$,则$T$为单节点树,并将类$C_k$作为该结点的类标记.返回$T$.</li>\n<li>如果特征集$A=\\varnothing$,则$T$为单节点树,并将$D$中实例数最大的类$C_k$作为该节点的类标记.返回$T$.</li>\n<li>否则,计算$A$中各特征对D的信息增益或是信息增益比,选择信息增益(信息增益比)最大的特征$A_g$.</li>\n<li>如果$A_g$的信息增益(信息增益比)小于阈值$\\epsilon$,则置$T$为单节点树,并将$D$中实例数最大的类$C_k$作为该节点的类标记,返回$T$.</li>\n<li>否则,对$A_g$的每一可能值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记,构建子结点,由节点及其子节点构成数$T$,返回$T$.</li>\n<li>对第$i$个子节点,以$D_i$为训练集,以$A-{A_g}$为特征集,递归地调用之前的算法,得到子树$T_i$,返回$T_i$.</li>\n</ol>\n<hr>\n<h5 id=\"信息增益和信息增益比\"><a href=\"#信息增益和信息增益比\" class=\"headerlink\" title=\"信息增益和信息增益比\"></a>信息增益和信息增益比</h5><p>信息增益(也被称为互信息)的算法是:</p>\n<ol>\n<li>计算数据集$D$的经验熵$H(D)$<script type=\"math/tex; mode=display\">H(D)=-\\sum^K_{k=1}\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}</script></li>\n<li>计算特征$A$对数据集$D$的经验条件熵$H(D|A)$<script type=\"math/tex; mode=display\">H(D|A)=\\sum^n_{i=1}\\frac{|D_i|}{|D|}H(D_i)=-\\sum^n_{i=1}\\frac{|D_i|}{|D|}\\sum^K_{k=1}\\frac{|D_{ik}|}{|D_i|}log_2\\frac{|D_{ik}|}{|D_i|}</script></li>\n<li>计算信息增益<script type=\"math/tex; mode=display\">g(D,A)=H(D)-H(D|A)</script></li>\n</ol>\n<p>信息增益比:</p>\n<script type=\"math/tex; mode=display\">g_R(D,A)=\\frac{g(D,A)}{H_A(D)}</script>","site":{"data":{}},"excerpt":"","more":"<h4 id=\"决策树算法\"><a href=\"#决策树算法\" class=\"headerlink\" title=\"决策树算法\"></a>决策树算法</h4><p>决策树算法是一种简单的机器学习算法.</p>\n<h5 id=\"算法步骤\"><a href=\"#算法步骤\" class=\"headerlink\" title=\"算法步骤:\"></a>算法步骤:</h5><ol>\n<li>如果$D$中所有实例属于同一类$C_k$,则$T$为单节点树,并将类$C_k$作为该结点的类标记.返回$T$.</li>\n<li>如果特征集$A=\\varnothing$,则$T$为单节点树,并将$D$中实例数最大的类$C_k$作为该节点的类标记.返回$T$.</li>\n<li>否则,计算$A$中各特征对D的信息增益或是信息增益比,选择信息增益(信息增益比)最大的特征$A_g$.</li>\n<li>如果$A_g$的信息增益(信息增益比)小于阈值$\\epsilon$,则置$T$为单节点树,并将$D$中实例数最大的类$C_k$作为该节点的类标记,返回$T$.</li>\n<li>否则,对$A_g$的每一可能值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记,构建子结点,由节点及其子节点构成数$T$,返回$T$.</li>\n<li>对第$i$个子节点,以$D_i$为训练集,以$A-{A_g}$为特征集,递归地调用之前的算法,得到子树$T_i$,返回$T_i$.</li>\n</ol>\n<hr>\n<h5 id=\"信息增益和信息增益比\"><a href=\"#信息增益和信息增益比\" class=\"headerlink\" title=\"信息增益和信息增益比\"></a>信息增益和信息增益比</h5><p>信息增益(也被称为互信息)的算法是:</p>\n<ol>\n<li>计算数据集$D$的经验熵$H(D)$<script type=\"math/tex; mode=display\">H(D)=-\\sum^K_{k=1}\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}</script></li>\n<li>计算特征$A$对数据集$D$的经验条件熵$H(D|A)$<script type=\"math/tex; mode=display\">H(D|A)=\\sum^n_{i=1}\\frac{|D_i|}{|D|}H(D_i)=-\\sum^n_{i=1}\\frac{|D_i|}{|D|}\\sum^K_{k=1}\\frac{|D_{ik}|}{|D_i|}log_2\\frac{|D_{ik}|}{|D_i|}</script></li>\n<li>计算信息增益<script type=\"math/tex; mode=display\">g(D,A)=H(D)-H(D|A)</script></li>\n</ol>\n<p>信息增益比:</p>\n<script type=\"math/tex; mode=display\">g_R(D,A)=\\frac{g(D,A)}{H_A(D)}</script>"},{"title":"KNN","date":"2019-05-29T01:41:47.000Z","mathjax":true,"_content":"#### k近邻算法\n  \nk近邻算法是一种简单的机器学习算法.\n\n##### 算法步骤:\n1. 对于给定的距离度量,在训练集中找到与实例$x$最近的$k$个点.\n2. 根据分类决策规则决定$x$的类别$y$.\n\n&emsp;&emsp;距离度量通常用欧式距离,常用的分类据测规则是多数表决.k值通常使用交叉验证选择最优值.\n\n&emsp;&emsp;kd树实际上是二分法在k维空间的扩展,对于训练集中个数较少的情况,kd树的实际表现并不如暴力扫描.\n\n---\n&emsp;&emsp;KNN算法作为一个简单直观的算法存在以下优点:\n1. 可用于非线性分类.\n2. 对类别的判断只与局部的数据点有关,能够较好地避免样本数量的不平衡问题.\n3. 直接使用样本之间关系,减少类别特征选择不当造成的不利影响,体现分类规则独立性的优势.\n\n&emsp;&emsp;同时,也存在一些不足:\n1. 对样本的依赖性较高,对噪声敏感.\n2. 距离度量的计算开销大,时间空间复杂度高.\n3. k值难以确定.","source":"_posts/KNN.md","raw":"---\ntitle: KNN\ndate: 2019-05-29 09:41:47\ncategories: 统计学习方法\ntags: machine learning\nmathjax: true\n---\n#### k近邻算法\n  \nk近邻算法是一种简单的机器学习算法.\n\n##### 算法步骤:\n1. 对于给定的距离度量,在训练集中找到与实例$x$最近的$k$个点.\n2. 根据分类决策规则决定$x$的类别$y$.\n\n&emsp;&emsp;距离度量通常用欧式距离,常用的分类据测规则是多数表决.k值通常使用交叉验证选择最优值.\n\n&emsp;&emsp;kd树实际上是二分法在k维空间的扩展,对于训练集中个数较少的情况,kd树的实际表现并不如暴力扫描.\n\n---\n&emsp;&emsp;KNN算法作为一个简单直观的算法存在以下优点:\n1. 可用于非线性分类.\n2. 对类别的判断只与局部的数据点有关,能够较好地避免样本数量的不平衡问题.\n3. 直接使用样本之间关系,减少类别特征选择不当造成的不利影响,体现分类规则独立性的优势.\n\n&emsp;&emsp;同时,也存在一些不足:\n1. 对样本的依赖性较高,对噪声敏感.\n2. 距离度量的计算开销大,时间空间复杂度高.\n3. k值难以确定.","slug":"KNN","published":1,"updated":"2019-06-10T07:42:24.964Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsl000cz4o0rnjq0i1v","content":"<h4 id=\"k近邻算法\"><a href=\"#k近邻算法\" class=\"headerlink\" title=\"k近邻算法\"></a>k近邻算法</h4><p>k近邻算法是一种简单的机器学习算法.</p>\n<h5 id=\"算法步骤\"><a href=\"#算法步骤\" class=\"headerlink\" title=\"算法步骤:\"></a>算法步骤:</h5><ol>\n<li>对于给定的距离度量,在训练集中找到与实例$x$最近的$k$个点.</li>\n<li>根据分类决策规则决定$x$的类别$y$.</li>\n</ol>\n<p>&emsp;&emsp;距离度量通常用欧式距离,常用的分类据测规则是多数表决.k值通常使用交叉验证选择最优值.</p>\n<p>&emsp;&emsp;kd树实际上是二分法在k维空间的扩展,对于训练集中个数较少的情况,kd树的实际表现并不如暴力扫描.</p>\n<hr>\n<p>&emsp;&emsp;KNN算法作为一个简单直观的算法存在以下优点:</p>\n<ol>\n<li>可用于非线性分类.</li>\n<li>对类别的判断只与局部的数据点有关,能够较好地避免样本数量的不平衡问题.</li>\n<li>直接使用样本之间关系,减少类别特征选择不当造成的不利影响,体现分类规则独立性的优势.</li>\n</ol>\n<p>&emsp;&emsp;同时,也存在一些不足:</p>\n<ol>\n<li>对样本的依赖性较高,对噪声敏感.</li>\n<li>距离度量的计算开销大,时间空间复杂度高.</li>\n<li>k值难以确定.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"k近邻算法\"><a href=\"#k近邻算法\" class=\"headerlink\" title=\"k近邻算法\"></a>k近邻算法</h4><p>k近邻算法是一种简单的机器学习算法.</p>\n<h5 id=\"算法步骤\"><a href=\"#算法步骤\" class=\"headerlink\" title=\"算法步骤:\"></a>算法步骤:</h5><ol>\n<li>对于给定的距离度量,在训练集中找到与实例$x$最近的$k$个点.</li>\n<li>根据分类决策规则决定$x$的类别$y$.</li>\n</ol>\n<p>&emsp;&emsp;距离度量通常用欧式距离,常用的分类据测规则是多数表决.k值通常使用交叉验证选择最优值.</p>\n<p>&emsp;&emsp;kd树实际上是二分法在k维空间的扩展,对于训练集中个数较少的情况,kd树的实际表现并不如暴力扫描.</p>\n<hr>\n<p>&emsp;&emsp;KNN算法作为一个简单直观的算法存在以下优点:</p>\n<ol>\n<li>可用于非线性分类.</li>\n<li>对类别的判断只与局部的数据点有关,能够较好地避免样本数量的不平衡问题.</li>\n<li>直接使用样本之间关系,减少类别特征选择不当造成的不利影响,体现分类规则独立性的优势.</li>\n</ol>\n<p>&emsp;&emsp;同时,也存在一些不足:</p>\n<ol>\n<li>对样本的依赖性较高,对噪声敏感.</li>\n<li>距离度量的计算开销大,时间空间复杂度高.</li>\n<li>k值难以确定.</li>\n</ol>\n"},{"title":"Exercise-2","date":"2019-06-03T12:40:17.000Z","updated":"2019-06-10T07:42:02.448Z","mathjax":true,"_content":"#### 章节2的习题解\n##### 习题1\n&emsp;&emsp;因为异或本身不是线性可分的,不光是感知机模型不能表示异或,只要是非线性模型都无法表示异或.\n\n![](https://ws3.sinaimg.cn/large/005BYqpggy1g3oahx3no8j30hs0dct90.jpg)\n\n---\n##### 习题3\n&emsp;&emsp;先证明必要性.即由线性可分$\\Rightarrow$凸壳互不相交.\n\n&emsp;&emsp;样本集线性可分,即存在超平面$wx+b=0$分割样本集,对于每个正实例点都有$wx_i+b>0$, 每个负实例点都有$wx_i+b<0$\n\n&emsp;&emsp;利用反证法,假如正负实例点集是相交的,那么一定存在点既属于正实例点集又属于负实例点集.与上述相矛盾.因此必要性得证.\n\n&emsp;&emsp;然后证明充分性,即由凸壳互不相交$\\Rightarrow$线性可分.\n\n&emsp;&emsp;对于正实例点集和负实例点集必然存在距离最近的不同类的两点距离$||x_1,x_2||_2>0$,其他各点之间的距离都大于$||x_1,x_2||_2$\n\n&emsp;&emsp;那么设一个超平面$wx+b=0$,其中:\n$$w=x_1-x_2$$\n$$b=\\frac{x_1\\cdot x_1-x_2\\cdot x_2}{2}$$\n&emsp;&emsp;因此\n$$wx+b=\\frac{||x_2-x||_2^2-||x_1-x||^2_2}{2}$$\n对于和$x_2$同一类的点,可得到$||x_2-x||_2^2\\leq ||x_1-x||^2_2$,因此$wx+b<0$,对于和$x_1$同一类的点,则可以得到相反的结果$wx+b>0$.因此得证线性可分.","source":"_posts/Exercise-2.md","raw":"---\ntitle: Exercise-2\ndate: 2019-06-03 20:40:17\nupdated: \ncategories: 统计学习方法\ntags: machine learning\nmathjax: true\n---\n#### 章节2的习题解\n##### 习题1\n&emsp;&emsp;因为异或本身不是线性可分的,不光是感知机模型不能表示异或,只要是非线性模型都无法表示异或.\n\n![](https://ws3.sinaimg.cn/large/005BYqpggy1g3oahx3no8j30hs0dct90.jpg)\n\n---\n##### 习题3\n&emsp;&emsp;先证明必要性.即由线性可分$\\Rightarrow$凸壳互不相交.\n\n&emsp;&emsp;样本集线性可分,即存在超平面$wx+b=0$分割样本集,对于每个正实例点都有$wx_i+b>0$, 每个负实例点都有$wx_i+b<0$\n\n&emsp;&emsp;利用反证法,假如正负实例点集是相交的,那么一定存在点既属于正实例点集又属于负实例点集.与上述相矛盾.因此必要性得证.\n\n&emsp;&emsp;然后证明充分性,即由凸壳互不相交$\\Rightarrow$线性可分.\n\n&emsp;&emsp;对于正实例点集和负实例点集必然存在距离最近的不同类的两点距离$||x_1,x_2||_2>0$,其他各点之间的距离都大于$||x_1,x_2||_2$\n\n&emsp;&emsp;那么设一个超平面$wx+b=0$,其中:\n$$w=x_1-x_2$$\n$$b=\\frac{x_1\\cdot x_1-x_2\\cdot x_2}{2}$$\n&emsp;&emsp;因此\n$$wx+b=\\frac{||x_2-x||_2^2-||x_1-x||^2_2}{2}$$\n对于和$x_2$同一类的点,可得到$||x_2-x||_2^2\\leq ||x_1-x||^2_2$,因此$wx+b<0$,对于和$x_1$同一类的点,则可以得到相反的结果$wx+b>0$.因此得证线性可分.","slug":"Exercise-2","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsm000ez4o0atu6wzkm","content":"<h4 id=\"章节2的习题解\"><a href=\"#章节2的习题解\" class=\"headerlink\" title=\"章节2的习题解\"></a>章节2的习题解</h4><h5 id=\"习题1\"><a href=\"#习题1\" class=\"headerlink\" title=\"习题1\"></a>习题1</h5><p>&emsp;&emsp;因为异或本身不是线性可分的,不光是感知机模型不能表示异或,只要是非线性模型都无法表示异或.</p>\n<p><img src=\"https://ws3.sinaimg.cn/large/005BYqpggy1g3oahx3no8j30hs0dct90.jpg\" alt></p>\n<hr>\n<h5 id=\"习题3\"><a href=\"#习题3\" class=\"headerlink\" title=\"习题3\"></a>习题3</h5><p>&emsp;&emsp;先证明必要性.即由线性可分$\\Rightarrow$凸壳互不相交.</p>\n<p>&emsp;&emsp;样本集线性可分,即存在超平面$wx+b=0$分割样本集,对于每个正实例点都有$wx_i+b&gt;0$, 每个负实例点都有$wx_i+b&lt;0$</p>\n<p>&emsp;&emsp;利用反证法,假如正负实例点集是相交的,那么一定存在点既属于正实例点集又属于负实例点集.与上述相矛盾.因此必要性得证.</p>\n<p>&emsp;&emsp;然后证明充分性,即由凸壳互不相交$\\Rightarrow$线性可分.</p>\n<p>&emsp;&emsp;对于正实例点集和负实例点集必然存在距离最近的不同类的两点距离$||x_1,x_2||_2&gt;0$,其他各点之间的距离都大于$||x_1,x_2||_2$</p>\n<p>&emsp;&emsp;那么设一个超平面$wx+b=0$,其中:</p>\n<script type=\"math/tex; mode=display\">w=x_1-x_2</script><script type=\"math/tex; mode=display\">b=\\frac{x_1\\cdot x_1-x_2\\cdot x_2}{2}</script><p>&emsp;&emsp;因此</p>\n<script type=\"math/tex; mode=display\">wx+b=\\frac{||x_2-x||_2^2-||x_1-x||^2_2}{2}</script><p>对于和$x_2$同一类的点,可得到$||x_2-x||_2^2\\leq ||x_1-x||^2_2$,因此$wx+b<0$,对于和$x_1$同一类的点,则可以得到相反的结果$wx+b>0$.因此得证线性可分.</0$,对于和$x_1$同一类的点,则可以得到相反的结果$wx+b></p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"章节2的习题解\"><a href=\"#章节2的习题解\" class=\"headerlink\" title=\"章节2的习题解\"></a>章节2的习题解</h4><h5 id=\"习题1\"><a href=\"#习题1\" class=\"headerlink\" title=\"习题1\"></a>习题1</h5><p>&emsp;&emsp;因为异或本身不是线性可分的,不光是感知机模型不能表示异或,只要是非线性模型都无法表示异或.</p>\n<p><img src=\"https://ws3.sinaimg.cn/large/005BYqpggy1g3oahx3no8j30hs0dct90.jpg\" alt></p>\n<hr>\n<h5 id=\"习题3\"><a href=\"#习题3\" class=\"headerlink\" title=\"习题3\"></a>习题3</h5><p>&emsp;&emsp;先证明必要性.即由线性可分$\\Rightarrow$凸壳互不相交.</p>\n<p>&emsp;&emsp;样本集线性可分,即存在超平面$wx+b=0$分割样本集,对于每个正实例点都有$wx_i+b&gt;0$, 每个负实例点都有$wx_i+b&lt;0$</p>\n<p>&emsp;&emsp;利用反证法,假如正负实例点集是相交的,那么一定存在点既属于正实例点集又属于负实例点集.与上述相矛盾.因此必要性得证.</p>\n<p>&emsp;&emsp;然后证明充分性,即由凸壳互不相交$\\Rightarrow$线性可分.</p>\n<p>&emsp;&emsp;对于正实例点集和负实例点集必然存在距离最近的不同类的两点距离$||x_1,x_2||_2&gt;0$,其他各点之间的距离都大于$||x_1,x_2||_2$</p>\n<p>&emsp;&emsp;那么设一个超平面$wx+b=0$,其中:</p>\n<script type=\"math/tex; mode=display\">w=x_1-x_2</script><script type=\"math/tex; mode=display\">b=\\frac{x_1\\cdot x_1-x_2\\cdot x_2}{2}</script><p>&emsp;&emsp;因此</p>\n<script type=\"math/tex; mode=display\">wx+b=\\frac{||x_2-x||_2^2-||x_1-x||^2_2}{2}</script><p>对于和$x_2$同一类的点,可得到$||x_2-x||_2^2\\leq ||x_1-x||^2_2$,因此$wx+b<0$,对于和$x_1$同一类的点,则可以得到相反的结果$wx+b>0$.因此得证线性可分.</0$,对于和$x_1$同一类的点,则可以得到相反的结果$wx+b></p>\n"},{"title":"AI-chapter2","date":"2019-06-10T23:44:46.000Z","visitors":null,"mathjax":true,"_content":"第二章的习题\n\n#### 2.1\n&emsp;&emsp;还是以扫地机为例子,如果之前的序列是扫地机一直向右走并清扫灰尘,但是当前已经到了墙角下,无法再向右走,如果扫地机只依赖于之前的时间步,那么就会一直停止.因此理性Agent的行动不仅依赖于环境状态,也要考虑它到达的时间点.\n#### 2.2\n##### 1)\n&emsp;&emsp;要证明这个扫地机是理性Agent,就是要证明对每个可能的感知序列,根据已知的感知序列提供的证据和Agent具有的先验知识,扫地机会选择能使其性能度量最大化的行动.先验知识是环境,也就是地板的分布.性能度量是每个时间步对每块清洁的方格奖励1分.因为只有左右两块地板,扫地机只具备向左,向右和打扫地面三个行动.按照之前的行动序列,可以认为这是能使性能度量分值的期望最高的行动.因此这个Agent是理性的.\n##### 2)\n&emsp;&emsp;如果给移动加一个代价,那么在扫地机就不能积极地进行移动.可以设计这样的一个函数:\n\n&emsp;&emsp;当A地板是干净的,下一步移动到B地板,如果B地板也是干净的,那么就停一步,也就是在一个时间步之后再移动到A地板,如果B地板是脏的,那么就没有时间延迟.同理,如果从B地板移动到A地板后,根据环境,在之前的时间延迟之上再加一个时间步的时间延迟.\n\n&emsp;&emsp;这样的Agent仍然是基于规则-行动的,没有复杂的内部状态,只是基于当前的环境对执行时间进行推迟,以减少移动带来的惩罚.\n##### 3)\n&emsp;&emsp;对于地理环境不明的情况,最好是使用随机的方式,也就是让机器自己学习到地理信息.\n#### 2.3\n##### a)\n&emsp;&emsp;并非不可能是完全理性的,在限制条件下采取的理性也可能是完美理性的,局部最优解可能就是全局最优解.\n##### b)\n&emsp;&emsp;存在这样的情况,比如扫地机在需要给地面铺上沙子的任务,原来的规则就和现有的任务相悖,因此行动也会是不理性的.\n##### c)\n&emsp;&emsp;存在.自动驾驶任务下,每个自动驾驶的汽车都会选择当前最佳的行动.\n##### d)\n&emsp;&emsp;不是的,Agent程序以当前感知为输入,Agent函数以整个历史作为输入.\n##### e)\n&emsp;&emsp;是的,Agent函数都可以用程序/机器组合实现.\n##### f)\n&emsp;&emsp;不存在,完全是随机的话,就不存在最优解,那也就无法判断Agent是否是理性的.\n##### g)\n&emsp;&emsp;可能,任务环境不同但是相近的才能使这句话成立,因为性能度量要一致,给定的Agent才能进行任务迁移.\n##### h)\n&emsp;&emsp;在无法观察环境下,Agent也可能是理性的.\n#### 2.5\nAgent: AI的最小单元 \\\nAgent函数: AI行动的逻辑单元 \\\nAgent程序: Agent函数的实现 \\\n反射Agent: 基于条件-行动准则的Agent,只关注当前输入. \\\n基于模型的Agent: 根据过去历史维持内部状态,通过先验知识(对现实建模)和内部状态决定行动.比如自动驾驶的汽车观察到上一帧前面的车没有点亮车灯,而当前时间后车灯同时亮了,根据先验知识,可以知道前车在刹车,同样根据先验知识,得到自己也需要刹车.由此由模型和内部状态得到了减速行为. \\\n基于目标的Agent: 在前面的基础上加上目标信息.还是前车刹车的例子.基于目标的Agent是通过推理的形式得到,因为前车刹车了,为了不和前车相撞,我也需要刹车.也就是不同与前面是基于规则,基于目标的Agent能够推理进行决策. \\\n基于效用的Agent: 在上面的基础上再引入效用函数.用来评价行动和决策的分数.评价行为是否是经济,高效的. \\\n学习Agent: 能够实现自学习的Agent,通过Critic, Learning element, Performance element, Problem generator四个组件实现.","source":"_posts/AI-chapter2.md","raw":"---\ntitle: AI-chapter2\ndate: 2019-06-11 07:44:46\ncategories: AI-an answer of the modern method\nvisitors: \nmathjax: true\ntags: AI\n---\n第二章的习题\n\n#### 2.1\n&emsp;&emsp;还是以扫地机为例子,如果之前的序列是扫地机一直向右走并清扫灰尘,但是当前已经到了墙角下,无法再向右走,如果扫地机只依赖于之前的时间步,那么就会一直停止.因此理性Agent的行动不仅依赖于环境状态,也要考虑它到达的时间点.\n#### 2.2\n##### 1)\n&emsp;&emsp;要证明这个扫地机是理性Agent,就是要证明对每个可能的感知序列,根据已知的感知序列提供的证据和Agent具有的先验知识,扫地机会选择能使其性能度量最大化的行动.先验知识是环境,也就是地板的分布.性能度量是每个时间步对每块清洁的方格奖励1分.因为只有左右两块地板,扫地机只具备向左,向右和打扫地面三个行动.按照之前的行动序列,可以认为这是能使性能度量分值的期望最高的行动.因此这个Agent是理性的.\n##### 2)\n&emsp;&emsp;如果给移动加一个代价,那么在扫地机就不能积极地进行移动.可以设计这样的一个函数:\n\n&emsp;&emsp;当A地板是干净的,下一步移动到B地板,如果B地板也是干净的,那么就停一步,也就是在一个时间步之后再移动到A地板,如果B地板是脏的,那么就没有时间延迟.同理,如果从B地板移动到A地板后,根据环境,在之前的时间延迟之上再加一个时间步的时间延迟.\n\n&emsp;&emsp;这样的Agent仍然是基于规则-行动的,没有复杂的内部状态,只是基于当前的环境对执行时间进行推迟,以减少移动带来的惩罚.\n##### 3)\n&emsp;&emsp;对于地理环境不明的情况,最好是使用随机的方式,也就是让机器自己学习到地理信息.\n#### 2.3\n##### a)\n&emsp;&emsp;并非不可能是完全理性的,在限制条件下采取的理性也可能是完美理性的,局部最优解可能就是全局最优解.\n##### b)\n&emsp;&emsp;存在这样的情况,比如扫地机在需要给地面铺上沙子的任务,原来的规则就和现有的任务相悖,因此行动也会是不理性的.\n##### c)\n&emsp;&emsp;存在.自动驾驶任务下,每个自动驾驶的汽车都会选择当前最佳的行动.\n##### d)\n&emsp;&emsp;不是的,Agent程序以当前感知为输入,Agent函数以整个历史作为输入.\n##### e)\n&emsp;&emsp;是的,Agent函数都可以用程序/机器组合实现.\n##### f)\n&emsp;&emsp;不存在,完全是随机的话,就不存在最优解,那也就无法判断Agent是否是理性的.\n##### g)\n&emsp;&emsp;可能,任务环境不同但是相近的才能使这句话成立,因为性能度量要一致,给定的Agent才能进行任务迁移.\n##### h)\n&emsp;&emsp;在无法观察环境下,Agent也可能是理性的.\n#### 2.5\nAgent: AI的最小单元 \\\nAgent函数: AI行动的逻辑单元 \\\nAgent程序: Agent函数的实现 \\\n反射Agent: 基于条件-行动准则的Agent,只关注当前输入. \\\n基于模型的Agent: 根据过去历史维持内部状态,通过先验知识(对现实建模)和内部状态决定行动.比如自动驾驶的汽车观察到上一帧前面的车没有点亮车灯,而当前时间后车灯同时亮了,根据先验知识,可以知道前车在刹车,同样根据先验知识,得到自己也需要刹车.由此由模型和内部状态得到了减速行为. \\\n基于目标的Agent: 在前面的基础上加上目标信息.还是前车刹车的例子.基于目标的Agent是通过推理的形式得到,因为前车刹车了,为了不和前车相撞,我也需要刹车.也就是不同与前面是基于规则,基于目标的Agent能够推理进行决策. \\\n基于效用的Agent: 在上面的基础上再引入效用函数.用来评价行动和决策的分数.评价行为是否是经济,高效的. \\\n学习Agent: 能够实现自学习的Agent,通过Critic, Learning element, Performance element, Problem generator四个组件实现.","slug":"AI-chapter2","published":1,"updated":"2019-06-11T03:00:10.474Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsq000jz4o09gpzbpxj","content":"<p>第二章的习题</p>\n<h4 id=\"2-1\"><a href=\"#2-1\" class=\"headerlink\" title=\"2.1\"></a>2.1</h4><p>&emsp;&emsp;还是以扫地机为例子,如果之前的序列是扫地机一直向右走并清扫灰尘,但是当前已经到了墙角下,无法再向右走,如果扫地机只依赖于之前的时间步,那么就会一直停止.因此理性Agent的行动不仅依赖于环境状态,也要考虑它到达的时间点.</p>\n<h4 id=\"2-2\"><a href=\"#2-2\" class=\"headerlink\" title=\"2.2\"></a>2.2</h4><h5 id=\"1\"><a href=\"#1\" class=\"headerlink\" title=\"1)\"></a>1)</h5><p>&emsp;&emsp;要证明这个扫地机是理性Agent,就是要证明对每个可能的感知序列,根据已知的感知序列提供的证据和Agent具有的先验知识,扫地机会选择能使其性能度量最大化的行动.先验知识是环境,也就是地板的分布.性能度量是每个时间步对每块清洁的方格奖励1分.因为只有左右两块地板,扫地机只具备向左,向右和打扫地面三个行动.按照之前的行动序列,可以认为这是能使性能度量分值的期望最高的行动.因此这个Agent是理性的.</p>\n<h5 id=\"2\"><a href=\"#2\" class=\"headerlink\" title=\"2)\"></a>2)</h5><p>&emsp;&emsp;如果给移动加一个代价,那么在扫地机就不能积极地进行移动.可以设计这样的一个函数:</p>\n<p>&emsp;&emsp;当A地板是干净的,下一步移动到B地板,如果B地板也是干净的,那么就停一步,也就是在一个时间步之后再移动到A地板,如果B地板是脏的,那么就没有时间延迟.同理,如果从B地板移动到A地板后,根据环境,在之前的时间延迟之上再加一个时间步的时间延迟.</p>\n<p>&emsp;&emsp;这样的Agent仍然是基于规则-行动的,没有复杂的内部状态,只是基于当前的环境对执行时间进行推迟,以减少移动带来的惩罚.</p>\n<h5 id=\"3\"><a href=\"#3\" class=\"headerlink\" title=\"3)\"></a>3)</h5><p>&emsp;&emsp;对于地理环境不明的情况,最好是使用随机的方式,也就是让机器自己学习到地理信息.</p>\n<h4 id=\"2-3\"><a href=\"#2-3\" class=\"headerlink\" title=\"2.3\"></a>2.3</h4><h5 id=\"a\"><a href=\"#a\" class=\"headerlink\" title=\"a)\"></a>a)</h5><p>&emsp;&emsp;并非不可能是完全理性的,在限制条件下采取的理性也可能是完美理性的,局部最优解可能就是全局最优解.</p>\n<h5 id=\"b\"><a href=\"#b\" class=\"headerlink\" title=\"b)\"></a>b)</h5><p>&emsp;&emsp;存在这样的情况,比如扫地机在需要给地面铺上沙子的任务,原来的规则就和现有的任务相悖,因此行动也会是不理性的.</p>\n<h5 id=\"c\"><a href=\"#c\" class=\"headerlink\" title=\"c)\"></a>c)</h5><p>&emsp;&emsp;存在.自动驾驶任务下,每个自动驾驶的汽车都会选择当前最佳的行动.</p>\n<h5 id=\"d\"><a href=\"#d\" class=\"headerlink\" title=\"d)\"></a>d)</h5><p>&emsp;&emsp;不是的,Agent程序以当前感知为输入,Agent函数以整个历史作为输入.</p>\n<h5 id=\"e\"><a href=\"#e\" class=\"headerlink\" title=\"e)\"></a>e)</h5><p>&emsp;&emsp;是的,Agent函数都可以用程序/机器组合实现.</p>\n<h5 id=\"f\"><a href=\"#f\" class=\"headerlink\" title=\"f)\"></a>f)</h5><p>&emsp;&emsp;不存在,完全是随机的话,就不存在最优解,那也就无法判断Agent是否是理性的.</p>\n<h5 id=\"g\"><a href=\"#g\" class=\"headerlink\" title=\"g)\"></a>g)</h5><p>&emsp;&emsp;可能,任务环境不同但是相近的才能使这句话成立,因为性能度量要一致,给定的Agent才能进行任务迁移.</p>\n<h5 id=\"h\"><a href=\"#h\" class=\"headerlink\" title=\"h)\"></a>h)</h5><p>&emsp;&emsp;在无法观察环境下,Agent也可能是理性的.</p>\n<h4 id=\"2-5\"><a href=\"#2-5\" class=\"headerlink\" title=\"2.5\"></a>2.5</h4><p>Agent: AI的最小单元 \\<br>Agent函数: AI行动的逻辑单元 \\<br>Agent程序: Agent函数的实现 \\<br>反射Agent: 基于条件-行动准则的Agent,只关注当前输入. \\<br>基于模型的Agent: 根据过去历史维持内部状态,通过先验知识(对现实建模)和内部状态决定行动.比如自动驾驶的汽车观察到上一帧前面的车没有点亮车灯,而当前时间后车灯同时亮了,根据先验知识,可以知道前车在刹车,同样根据先验知识,得到自己也需要刹车.由此由模型和内部状态得到了减速行为. \\<br>基于目标的Agent: 在前面的基础上加上目标信息.还是前车刹车的例子.基于目标的Agent是通过推理的形式得到,因为前车刹车了,为了不和前车相撞,我也需要刹车.也就是不同与前面是基于规则,基于目标的Agent能够推理进行决策. \\<br>基于效用的Agent: 在上面的基础上再引入效用函数.用来评价行动和决策的分数.评价行为是否是经济,高效的. \\<br>学习Agent: 能够实现自学习的Agent,通过Critic, Learning element, Performance element, Problem generator四个组件实现.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>第二章的习题</p>\n<h4 id=\"2-1\"><a href=\"#2-1\" class=\"headerlink\" title=\"2.1\"></a>2.1</h4><p>&emsp;&emsp;还是以扫地机为例子,如果之前的序列是扫地机一直向右走并清扫灰尘,但是当前已经到了墙角下,无法再向右走,如果扫地机只依赖于之前的时间步,那么就会一直停止.因此理性Agent的行动不仅依赖于环境状态,也要考虑它到达的时间点.</p>\n<h4 id=\"2-2\"><a href=\"#2-2\" class=\"headerlink\" title=\"2.2\"></a>2.2</h4><h5 id=\"1\"><a href=\"#1\" class=\"headerlink\" title=\"1)\"></a>1)</h5><p>&emsp;&emsp;要证明这个扫地机是理性Agent,就是要证明对每个可能的感知序列,根据已知的感知序列提供的证据和Agent具有的先验知识,扫地机会选择能使其性能度量最大化的行动.先验知识是环境,也就是地板的分布.性能度量是每个时间步对每块清洁的方格奖励1分.因为只有左右两块地板,扫地机只具备向左,向右和打扫地面三个行动.按照之前的行动序列,可以认为这是能使性能度量分值的期望最高的行动.因此这个Agent是理性的.</p>\n<h5 id=\"2\"><a href=\"#2\" class=\"headerlink\" title=\"2)\"></a>2)</h5><p>&emsp;&emsp;如果给移动加一个代价,那么在扫地机就不能积极地进行移动.可以设计这样的一个函数:</p>\n<p>&emsp;&emsp;当A地板是干净的,下一步移动到B地板,如果B地板也是干净的,那么就停一步,也就是在一个时间步之后再移动到A地板,如果B地板是脏的,那么就没有时间延迟.同理,如果从B地板移动到A地板后,根据环境,在之前的时间延迟之上再加一个时间步的时间延迟.</p>\n<p>&emsp;&emsp;这样的Agent仍然是基于规则-行动的,没有复杂的内部状态,只是基于当前的环境对执行时间进行推迟,以减少移动带来的惩罚.</p>\n<h5 id=\"3\"><a href=\"#3\" class=\"headerlink\" title=\"3)\"></a>3)</h5><p>&emsp;&emsp;对于地理环境不明的情况,最好是使用随机的方式,也就是让机器自己学习到地理信息.</p>\n<h4 id=\"2-3\"><a href=\"#2-3\" class=\"headerlink\" title=\"2.3\"></a>2.3</h4><h5 id=\"a\"><a href=\"#a\" class=\"headerlink\" title=\"a)\"></a>a)</h5><p>&emsp;&emsp;并非不可能是完全理性的,在限制条件下采取的理性也可能是完美理性的,局部最优解可能就是全局最优解.</p>\n<h5 id=\"b\"><a href=\"#b\" class=\"headerlink\" title=\"b)\"></a>b)</h5><p>&emsp;&emsp;存在这样的情况,比如扫地机在需要给地面铺上沙子的任务,原来的规则就和现有的任务相悖,因此行动也会是不理性的.</p>\n<h5 id=\"c\"><a href=\"#c\" class=\"headerlink\" title=\"c)\"></a>c)</h5><p>&emsp;&emsp;存在.自动驾驶任务下,每个自动驾驶的汽车都会选择当前最佳的行动.</p>\n<h5 id=\"d\"><a href=\"#d\" class=\"headerlink\" title=\"d)\"></a>d)</h5><p>&emsp;&emsp;不是的,Agent程序以当前感知为输入,Agent函数以整个历史作为输入.</p>\n<h5 id=\"e\"><a href=\"#e\" class=\"headerlink\" title=\"e)\"></a>e)</h5><p>&emsp;&emsp;是的,Agent函数都可以用程序/机器组合实现.</p>\n<h5 id=\"f\"><a href=\"#f\" class=\"headerlink\" title=\"f)\"></a>f)</h5><p>&emsp;&emsp;不存在,完全是随机的话,就不存在最优解,那也就无法判断Agent是否是理性的.</p>\n<h5 id=\"g\"><a href=\"#g\" class=\"headerlink\" title=\"g)\"></a>g)</h5><p>&emsp;&emsp;可能,任务环境不同但是相近的才能使这句话成立,因为性能度量要一致,给定的Agent才能进行任务迁移.</p>\n<h5 id=\"h\"><a href=\"#h\" class=\"headerlink\" title=\"h)\"></a>h)</h5><p>&emsp;&emsp;在无法观察环境下,Agent也可能是理性的.</p>\n<h4 id=\"2-5\"><a href=\"#2-5\" class=\"headerlink\" title=\"2.5\"></a>2.5</h4><p>Agent: AI的最小单元 \\<br>Agent函数: AI行动的逻辑单元 \\<br>Agent程序: Agent函数的实现 \\<br>反射Agent: 基于条件-行动准则的Agent,只关注当前输入. \\<br>基于模型的Agent: 根据过去历史维持内部状态,通过先验知识(对现实建模)和内部状态决定行动.比如自动驾驶的汽车观察到上一帧前面的车没有点亮车灯,而当前时间后车灯同时亮了,根据先验知识,可以知道前车在刹车,同样根据先验知识,得到自己也需要刹车.由此由模型和内部状态得到了减速行为. \\<br>基于目标的Agent: 在前面的基础上加上目标信息.还是前车刹车的例子.基于目标的Agent是通过推理的形式得到,因为前车刹车了,为了不和前车相撞,我也需要刹车.也就是不同与前面是基于规则,基于目标的Agent能够推理进行决策. \\<br>基于效用的Agent: 在上面的基础上再引入效用函数.用来评价行动和决策的分数.评价行为是否是经济,高效的. \\<br>学习Agent: 能够实现自学习的Agent,通过Critic, Learning element, Performance element, Problem generator四个组件实现.</p>\n"},{"title":"Kafka","date":"2019-06-05T01:24:39.000Z","updated":"2019-06-10T07:43:15.140Z","mathjax":true,"_content":"为了做流处理的实验,最近又在自己的台式机上搭建了大数据处理的环境,考虑到之前直接使用CDH的体验很差,经常会出现各种奇葩的问题,这次就自己配置各种组建.\n\n因为之前hadoop和spark已经配置很多次了,就不再记录.现在使用的是hadoop3.2.0版本,spark使用的是2.4.3版本,为了进行流处理还需要配置kafka给spark stream喂数据.因为kafka在单机上自带脚本可以实现单节点zookeeper实例,就暂时先不配置zookeeper.\n\n启动zookeeper的方法是:\n\n    bin/zookeeper-server-start.sh config zookeeper.properties\n\n然后再以守护进程方式后台启动kafka服务器:\n\n    bin/kafka-server-start.sh -daemon config/server.properties\n接下来可以创建topic:\n\n    bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic topicname\n创建producer发送信息:\n\n    bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topicname\n\n还可以创建consumer从终端输出信息:\n\n    bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topicname --from-beginning","source":"_posts/Kafka.md","raw":"---\ntitle: Kafka\ndate: 2019-06-05 09:24:39\nupdated: \ncategories: big data\ntags: big data\nmathjax: true\n---\n为了做流处理的实验,最近又在自己的台式机上搭建了大数据处理的环境,考虑到之前直接使用CDH的体验很差,经常会出现各种奇葩的问题,这次就自己配置各种组建.\n\n因为之前hadoop和spark已经配置很多次了,就不再记录.现在使用的是hadoop3.2.0版本,spark使用的是2.4.3版本,为了进行流处理还需要配置kafka给spark stream喂数据.因为kafka在单机上自带脚本可以实现单节点zookeeper实例,就暂时先不配置zookeeper.\n\n启动zookeeper的方法是:\n\n    bin/zookeeper-server-start.sh config zookeeper.properties\n\n然后再以守护进程方式后台启动kafka服务器:\n\n    bin/kafka-server-start.sh -daemon config/server.properties\n接下来可以创建topic:\n\n    bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic topicname\n创建producer发送信息:\n\n    bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topicname\n\n还可以创建consumer从终端输出信息:\n\n    bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topicname --from-beginning","slug":"Kafka","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsr000mz4o0kmcg9mlu","content":"<p>为了做流处理的实验,最近又在自己的台式机上搭建了大数据处理的环境,考虑到之前直接使用CDH的体验很差,经常会出现各种奇葩的问题,这次就自己配置各种组建.</p>\n<p>因为之前hadoop和spark已经配置很多次了,就不再记录.现在使用的是hadoop3.2.0版本,spark使用的是2.4.3版本,为了进行流处理还需要配置kafka给spark stream喂数据.因为kafka在单机上自带脚本可以实现单节点zookeeper实例,就暂时先不配置zookeeper.</p>\n<p>启动zookeeper的方法是:</p>\n<pre><code>bin/zookeeper-server-start.sh config zookeeper.properties\n</code></pre><p>然后再以守护进程方式后台启动kafka服务器:</p>\n<pre><code>bin/kafka-server-start.sh -daemon config/server.properties\n</code></pre><p>接下来可以创建topic:</p>\n<pre><code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic topicname\n</code></pre><p>创建producer发送信息:</p>\n<pre><code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topicname\n</code></pre><p>还可以创建consumer从终端输出信息:</p>\n<pre><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topicname --from-beginning\n</code></pre>","site":{"data":{}},"excerpt":"","more":"<p>为了做流处理的实验,最近又在自己的台式机上搭建了大数据处理的环境,考虑到之前直接使用CDH的体验很差,经常会出现各种奇葩的问题,这次就自己配置各种组建.</p>\n<p>因为之前hadoop和spark已经配置很多次了,就不再记录.现在使用的是hadoop3.2.0版本,spark使用的是2.4.3版本,为了进行流处理还需要配置kafka给spark stream喂数据.因为kafka在单机上自带脚本可以实现单节点zookeeper实例,就暂时先不配置zookeeper.</p>\n<p>启动zookeeper的方法是:</p>\n<pre><code>bin/zookeeper-server-start.sh config zookeeper.properties\n</code></pre><p>然后再以守护进程方式后台启动kafka服务器:</p>\n<pre><code>bin/kafka-server-start.sh -daemon config/server.properties\n</code></pre><p>接下来可以创建topic:</p>\n<pre><code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic topicname\n</code></pre><p>创建producer发送信息:</p>\n<pre><code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topicname\n</code></pre><p>还可以创建consumer从终端输出信息:</p>\n<pre><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topicname --from-beginning\n</code></pre>"},{"title":"Logistic-regression","date":"2019-06-03T01:33:30.000Z","mathjax":true,"_content":"#### Logistic回归算法\n\n&emsp;&emsp;Logistic分布的分布函数和密度函数分别是:\n$$\n    \\begin{aligned}\n        F(x)&=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}} \\\\\n        f(x)&=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}\n    \\end{aligned}\n$$\n&emsp;&emsp;其中$\\mu$是位置参数,$\\gamma>0$是形状参数.\n分布函数曲线以点$(\\mu,\\frac{1}{2})$为中心对称.\n\n&emsp;&emsp;对于二项Logistic回归模型,条件概率分布为:\n$$\n    \\begin{aligned}\n        P(Y=1|x)&=\\frac{exp(\\omega\\cdot x)}{1+exp(\\omega\\cdot x)} \\\\\n        P(Y=0|x)&=\\frac{1}{1+exp(\\omega\\cdot x)}\n    \\end{aligned}\n$$\n&emsp;&emsp;其中$\\omega$是参数,可以用极大似然估计法估计参数,似然估计函数为:\n$$\\prod^N_{i=1}[P(Y=1|x_i)]^{y_i}[1-P(Y=1|x_i)]^{1-y_i}$$\n&emsp;&emsp;对数似然函数就是:\n$$\n    \\begin{aligned}\n        L(\\omega)&=\\sum^N_{i=1}[y_ilogP(Y=1|x_i)+(1-y_i)log(1-P(Y=1|x_i))] \\\\\n        &=\\sum^N_{i=1}[y_i(\\omega\\cdot x_i)-log(1+exp(\\omega\\cdot x_i))]\n    \\end{aligned}\n$$\n&emsp;&emsp;问题就等价于以对数似然函数为目标函数的最优化问题,可以用梯度下降法或牛顿法.\n\n---\n&emsp;&emsp;Logistic算法实现简单,速度快,但是当特征空间很大时,性能不是很好；容易欠拟合,一般准确率不太高；对于非线性特征需要转换","source":"_posts/Logistic-regression.md","raw":"---\ntitle: Logistic-regression\ndate: 2019-06-03 09:33:30\ncategories: 统计学习方法\ntags: machine learning\nmathjax: true\n---\n#### Logistic回归算法\n\n&emsp;&emsp;Logistic分布的分布函数和密度函数分别是:\n$$\n    \\begin{aligned}\n        F(x)&=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}} \\\\\n        f(x)&=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}\n    \\end{aligned}\n$$\n&emsp;&emsp;其中$\\mu$是位置参数,$\\gamma>0$是形状参数.\n分布函数曲线以点$(\\mu,\\frac{1}{2})$为中心对称.\n\n&emsp;&emsp;对于二项Logistic回归模型,条件概率分布为:\n$$\n    \\begin{aligned}\n        P(Y=1|x)&=\\frac{exp(\\omega\\cdot x)}{1+exp(\\omega\\cdot x)} \\\\\n        P(Y=0|x)&=\\frac{1}{1+exp(\\omega\\cdot x)}\n    \\end{aligned}\n$$\n&emsp;&emsp;其中$\\omega$是参数,可以用极大似然估计法估计参数,似然估计函数为:\n$$\\prod^N_{i=1}[P(Y=1|x_i)]^{y_i}[1-P(Y=1|x_i)]^{1-y_i}$$\n&emsp;&emsp;对数似然函数就是:\n$$\n    \\begin{aligned}\n        L(\\omega)&=\\sum^N_{i=1}[y_ilogP(Y=1|x_i)+(1-y_i)log(1-P(Y=1|x_i))] \\\\\n        &=\\sum^N_{i=1}[y_i(\\omega\\cdot x_i)-log(1+exp(\\omega\\cdot x_i))]\n    \\end{aligned}\n$$\n&emsp;&emsp;问题就等价于以对数似然函数为目标函数的最优化问题,可以用梯度下降法或牛顿法.\n\n---\n&emsp;&emsp;Logistic算法实现简单,速度快,但是当特征空间很大时,性能不是很好；容易欠拟合,一般准确率不太高；对于非线性特征需要转换","slug":"Logistic-regression","published":1,"updated":"2019-06-10T07:42:31.514Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nst000qz4o0cgmy3r50","content":"<h4 id=\"Logistic回归算法\"><a href=\"#Logistic回归算法\" class=\"headerlink\" title=\"Logistic回归算法\"></a>Logistic回归算法</h4><p>&emsp;&emsp;Logistic分布的分布函数和密度函数分别是:</p>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        F(x)&=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}} \\\\\n        f(x)&=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}\n    \\end{aligned}</script><p>&emsp;&emsp;其中$\\mu$是位置参数,$\\gamma&gt;0$是形状参数.<br>分布函数曲线以点$(\\mu,\\frac{1}{2})$为中心对称.</p>\n<p>&emsp;&emsp;对于二项Logistic回归模型,条件概率分布为:</p>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        P(Y=1|x)&=\\frac{exp(\\omega\\cdot x)}{1+exp(\\omega\\cdot x)} \\\\\n        P(Y=0|x)&=\\frac{1}{1+exp(\\omega\\cdot x)}\n    \\end{aligned}</script><p>&emsp;&emsp;其中$\\omega$是参数,可以用极大似然估计法估计参数,似然估计函数为:</p>\n<script type=\"math/tex; mode=display\">\\prod^N_{i=1}[P(Y=1|x_i)]^{y_i}[1-P(Y=1|x_i)]^{1-y_i}</script><p>&emsp;&emsp;对数似然函数就是:</p>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        L(\\omega)&=\\sum^N_{i=1}[y_ilogP(Y=1|x_i)+(1-y_i)log(1-P(Y=1|x_i))] \\\\\n        &=\\sum^N_{i=1}[y_i(\\omega\\cdot x_i)-log(1+exp(\\omega\\cdot x_i))]\n    \\end{aligned}</script><p>&emsp;&emsp;问题就等价于以对数似然函数为目标函数的最优化问题,可以用梯度下降法或牛顿法.</p>\n<hr>\n<p>&emsp;&emsp;Logistic算法实现简单,速度快,但是当特征空间很大时,性能不是很好；容易欠拟合,一般准确率不太高；对于非线性特征需要转换</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"Logistic回归算法\"><a href=\"#Logistic回归算法\" class=\"headerlink\" title=\"Logistic回归算法\"></a>Logistic回归算法</h4><p>&emsp;&emsp;Logistic分布的分布函数和密度函数分别是:</p>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        F(x)&=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}} \\\\\n        f(x)&=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}\n    \\end{aligned}</script><p>&emsp;&emsp;其中$\\mu$是位置参数,$\\gamma&gt;0$是形状参数.<br>分布函数曲线以点$(\\mu,\\frac{1}{2})$为中心对称.</p>\n<p>&emsp;&emsp;对于二项Logistic回归模型,条件概率分布为:</p>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        P(Y=1|x)&=\\frac{exp(\\omega\\cdot x)}{1+exp(\\omega\\cdot x)} \\\\\n        P(Y=0|x)&=\\frac{1}{1+exp(\\omega\\cdot x)}\n    \\end{aligned}</script><p>&emsp;&emsp;其中$\\omega$是参数,可以用极大似然估计法估计参数,似然估计函数为:</p>\n<script type=\"math/tex; mode=display\">\\prod^N_{i=1}[P(Y=1|x_i)]^{y_i}[1-P(Y=1|x_i)]^{1-y_i}</script><p>&emsp;&emsp;对数似然函数就是:</p>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        L(\\omega)&=\\sum^N_{i=1}[y_ilogP(Y=1|x_i)+(1-y_i)log(1-P(Y=1|x_i))] \\\\\n        &=\\sum^N_{i=1}[y_i(\\omega\\cdot x_i)-log(1+exp(\\omega\\cdot x_i))]\n    \\end{aligned}</script><p>&emsp;&emsp;问题就等价于以对数似然函数为目标函数的最优化问题,可以用梯度下降法或牛顿法.</p>\n<hr>\n<p>&emsp;&emsp;Logistic算法实现简单,速度快,但是当特征空间很大时,性能不是很好；容易欠拟合,一般准确率不太高；对于非线性特征需要转换</p>\n"},{"title":"Naive Bayesian","date":"2019-05-27T08:52:44.000Z","mathjax":true,"_content":"#### 朴素贝叶斯算法\n\n朴素贝叶斯算法是典型的生成模型,生成模型由训练数据学习联合概率分布$P(X,Y)$,然后求得后验概率分布$P(Y|X)$.\n$$P(X,Y)=P(Y)P(X|Y)$$\n\n---\n##### 算法步骤:\n1. 计算先验概率及条件概率\n$$P(Y=c_k)=\\frac{\\sum^N_{i=1}I(y_i=c_k)}{N}$$\n$$P(X^(i)=a_{jl}|\\frac{\\sum^N_{i=1}I(x^{(j)}_i=a_{jl},y_i=c_k)}{\\sum^N_{i=1}I(y_i=c_k)})$$\n其中概率计算方法可以是极大似然估计或贝叶斯估计.\n\n2. 对于给定的实例x计算\n$$P(Y=c_k)\\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)$$\n3. 确定实例$x$的类\n$$y=argmax_{c_k}P(Y=c_k)\\prod^n_{j=1}P(X^{(j)}=x^{(j)|Y=c_k})$$\n\n&emsp;&emsp;对于连续值,采用的是高斯模型,即假设特征符合高斯分布；对于离散值,采用多项式模型；另外在伯努利模型中,每个特征的取值是布尔型,这个模型我还不是很清楚在什么场景下使用.\n\n---\n&emsp;&emsp;朴素贝叶斯的基本假设是条件独立性,这是一个严格的条件,因此模型所需要的参数比较少.这个算法的优点主要有:\n1. 对小规模的数据表现较好,能处理多分类任务.\n2. 面对无关属性,$P(X_i|Y)$几乎是均匀分布,$X_i$类的条件概率不会对总的后验概率计算产生影响.\n\n&emsp;&emsp;缺点主要有:\n1. 需要计算先验概率.\n2. 输入变量必须是条件独立的,如果样本点之间存在概率依存关系,那么相关属性可能会降低朴素贝叶斯分类器的性能.","source":"_posts/Naive-Bayesian.md","raw":"---\ntitle: Naive Bayesian\ndate: 2019-05-27 16:52:44\ncategories: 统计学习方法\ntags: machine learning\nmathjax: true\n---\n#### 朴素贝叶斯算法\n\n朴素贝叶斯算法是典型的生成模型,生成模型由训练数据学习联合概率分布$P(X,Y)$,然后求得后验概率分布$P(Y|X)$.\n$$P(X,Y)=P(Y)P(X|Y)$$\n\n---\n##### 算法步骤:\n1. 计算先验概率及条件概率\n$$P(Y=c_k)=\\frac{\\sum^N_{i=1}I(y_i=c_k)}{N}$$\n$$P(X^(i)=a_{jl}|\\frac{\\sum^N_{i=1}I(x^{(j)}_i=a_{jl},y_i=c_k)}{\\sum^N_{i=1}I(y_i=c_k)})$$\n其中概率计算方法可以是极大似然估计或贝叶斯估计.\n\n2. 对于给定的实例x计算\n$$P(Y=c_k)\\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)$$\n3. 确定实例$x$的类\n$$y=argmax_{c_k}P(Y=c_k)\\prod^n_{j=1}P(X^{(j)}=x^{(j)|Y=c_k})$$\n\n&emsp;&emsp;对于连续值,采用的是高斯模型,即假设特征符合高斯分布；对于离散值,采用多项式模型；另外在伯努利模型中,每个特征的取值是布尔型,这个模型我还不是很清楚在什么场景下使用.\n\n---\n&emsp;&emsp;朴素贝叶斯的基本假设是条件独立性,这是一个严格的条件,因此模型所需要的参数比较少.这个算法的优点主要有:\n1. 对小规模的数据表现较好,能处理多分类任务.\n2. 面对无关属性,$P(X_i|Y)$几乎是均匀分布,$X_i$类的条件概率不会对总的后验概率计算产生影响.\n\n&emsp;&emsp;缺点主要有:\n1. 需要计算先验概率.\n2. 输入变量必须是条件独立的,如果样本点之间存在概率依存关系,那么相关属性可能会降低朴素贝叶斯分类器的性能.","slug":"Naive-Bayesian","published":1,"updated":"2019-06-10T07:42:47.104Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsu000uz4o0jvmu3k3j","content":"<h4 id=\"朴素贝叶斯算法\"><a href=\"#朴素贝叶斯算法\" class=\"headerlink\" title=\"朴素贝叶斯算法\"></a>朴素贝叶斯算法</h4><p>朴素贝叶斯算法是典型的生成模型,生成模型由训练数据学习联合概率分布$P(X,Y)$,然后求得后验概率分布$P(Y|X)$.</p>\n<script type=\"math/tex; mode=display\">P(X,Y)=P(Y)P(X|Y)</script><hr>\n<h5 id=\"算法步骤\"><a href=\"#算法步骤\" class=\"headerlink\" title=\"算法步骤:\"></a>算法步骤:</h5><ol>\n<li><p>计算先验概率及条件概率</p>\n<script type=\"math/tex; mode=display\">P(Y=c_k)=\\frac{\\sum^N_{i=1}I(y_i=c_k)}{N}</script><script type=\"math/tex; mode=display\">P(X^(i)=a_{jl}|\\frac{\\sum^N_{i=1}I(x^{(j)}_i=a_{jl},y_i=c_k)}{\\sum^N_{i=1}I(y_i=c_k)})</script><p>其中概率计算方法可以是极大似然估计或贝叶斯估计.</p>\n</li>\n<li><p>对于给定的实例x计算</p>\n<script type=\"math/tex; mode=display\">P(Y=c_k)\\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)</script></li>\n<li>确定实例$x$的类<script type=\"math/tex; mode=display\">y=argmax_{c_k}P(Y=c_k)\\prod^n_{j=1}P(X^{(j)}=x^{(j)|Y=c_k})</script></li>\n</ol>\n<p>&emsp;&emsp;对于连续值,采用的是高斯模型,即假设特征符合高斯分布；对于离散值,采用多项式模型；另外在伯努利模型中,每个特征的取值是布尔型,这个模型我还不是很清楚在什么场景下使用.</p>\n<hr>\n<p>&emsp;&emsp;朴素贝叶斯的基本假设是条件独立性,这是一个严格的条件,因此模型所需要的参数比较少.这个算法的优点主要有:</p>\n<ol>\n<li>对小规模的数据表现较好,能处理多分类任务.</li>\n<li>面对无关属性,$P(X_i|Y)$几乎是均匀分布,$X_i$类的条件概率不会对总的后验概率计算产生影响.</li>\n</ol>\n<p>&emsp;&emsp;缺点主要有:</p>\n<ol>\n<li>需要计算先验概率.</li>\n<li>输入变量必须是条件独立的,如果样本点之间存在概率依存关系,那么相关属性可能会降低朴素贝叶斯分类器的性能.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"朴素贝叶斯算法\"><a href=\"#朴素贝叶斯算法\" class=\"headerlink\" title=\"朴素贝叶斯算法\"></a>朴素贝叶斯算法</h4><p>朴素贝叶斯算法是典型的生成模型,生成模型由训练数据学习联合概率分布$P(X,Y)$,然后求得后验概率分布$P(Y|X)$.</p>\n<script type=\"math/tex; mode=display\">P(X,Y)=P(Y)P(X|Y)</script><hr>\n<h5 id=\"算法步骤\"><a href=\"#算法步骤\" class=\"headerlink\" title=\"算法步骤:\"></a>算法步骤:</h5><ol>\n<li><p>计算先验概率及条件概率</p>\n<script type=\"math/tex; mode=display\">P(Y=c_k)=\\frac{\\sum^N_{i=1}I(y_i=c_k)}{N}</script><script type=\"math/tex; mode=display\">P(X^(i)=a_{jl}|\\frac{\\sum^N_{i=1}I(x^{(j)}_i=a_{jl},y_i=c_k)}{\\sum^N_{i=1}I(y_i=c_k)})</script><p>其中概率计算方法可以是极大似然估计或贝叶斯估计.</p>\n</li>\n<li><p>对于给定的实例x计算</p>\n<script type=\"math/tex; mode=display\">P(Y=c_k)\\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)</script></li>\n<li>确定实例$x$的类<script type=\"math/tex; mode=display\">y=argmax_{c_k}P(Y=c_k)\\prod^n_{j=1}P(X^{(j)}=x^{(j)|Y=c_k})</script></li>\n</ol>\n<p>&emsp;&emsp;对于连续值,采用的是高斯模型,即假设特征符合高斯分布；对于离散值,采用多项式模型；另外在伯努利模型中,每个特征的取值是布尔型,这个模型我还不是很清楚在什么场景下使用.</p>\n<hr>\n<p>&emsp;&emsp;朴素贝叶斯的基本假设是条件独立性,这是一个严格的条件,因此模型所需要的参数比较少.这个算法的优点主要有:</p>\n<ol>\n<li>对小规模的数据表现较好,能处理多分类任务.</li>\n<li>面对无关属性,$P(X_i|Y)$几乎是均匀分布,$X_i$类的条件概率不会对总的后验概率计算产生影响.</li>\n</ol>\n<p>&emsp;&emsp;缺点主要有:</p>\n<ol>\n<li>需要计算先验概率.</li>\n<li>输入变量必须是条件独立的,如果样本点之间存在概率依存关系,那么相关属性可能会降低朴素贝叶斯分类器的性能.</li>\n</ol>\n"},{"title":"Maximum-entropy-model","date":"2019-06-03T01:53:09.000Z","mathjax":true,"_content":"#### 最大熵模型\n\n&emsp;&emsp;最大熵模型是生成模型,根据最大熵原理,在满足约束条件的模型集合中选择熵最大的模型.在模型集合C中,最大熵模型是条件概率分布$P(Y|X)$上的条件熵$H(P)$最大的模型:\n$$H(P)=-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)$$\n其中$\\widetilde{P}{x}$是经验分布.\n___\n##### 最大熵模型推导\n&emsp;&emsp;条件熵$P(Y|X)$表示已知$X$的情况下$Y$的不确定性,定义为给定$X$的条件下$Y$的条件概率分布的熵对$X$的期望.\n$$\n\\begin{aligned}\nH(Y|X)&=\\sum_x p(x)H(Y|X=x) \\\\\n&=-\\sum_x p(x)\\sum_y p(y|x)logp(y|x) \\\\\n&=-\\sum_{x,y}p(x,y)logp(y|x)  \n\\end{aligned}\n$$\n&emsp;&emsp;如果模型能够获取训练数据中的信息,那么可以假设两个期望相等:\n$$\n\\begin{aligned}\nE_P(X,Y)&=E_{\\widetilde{P}}(X,Y) \\\\\n-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logp(y|x)&=-\\sum_{x,y}\\widetilde{P}(x,y)logp(y|x) \n\\end{aligned}\n$$\n&emsp;&emsp;由此可以得到最大熵模型的公式.\n\n---\n##### 最大熵模型学习过程\n&emsp;&emsp;最大熵模型的学习问题:\n$$\n\\begin{aligned}\nmax_{P\\in C}&H(p)=-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)\\\\\n s.t.\\qquad &E_P(f_i)=E_{\\widetilde{P}(f_i)}, \\quad i=1,2,...,n \\\\\n &\\sum_yP(y|x)=1\n\\end{aligned}\n$$\n&emsp;&emsp;引入拉格朗日乘子$\\omega$,得到拉格朗日函数$L(P,\\omega)$:\n$$\n\\begin{aligned}\n    L(P,\\omega)&\\equiv-H(P)+\\omega_0(1-\\sum_y P(y|x))+\\sum^n_{i=1}\\omega_i(E_{\\widetilde{P}}(f_i)-E_P(f_i)) \\\\\n    &=\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)+\\omega_0(1-\\sum_y P(y|x))+ \\\\\n    &\\quad\\sum^n_{i=1}\\omega_i(\\sum_{x,y}\\widetilde{P}(x,y)f_i(x,y)-\\sum_{x,y}\\widetilde{P}(x)P(y|x)f_i(x,y))\n\\end{aligned}\n$$\n&emsp;&emsp;其中$f_i(x,y)$是二值函数,$x,y$同时满足条件时为1,其他情况为0\n\n&emsp;&emsp;拉格朗日函数的优化问题是:\n$$min_{P\\in C}max_\\omega L(P,\\omega)$$\n&emsp;&emsp;根据拉格朗日对偶性,可以求其对偶问题:\n$$max_\\omega min_{P\\in C}L(P,\\omega)$$\n&emsp;&emsp;首先求$L(P,\\omega)$关于$P$的极小值问题.即求$L(P,\\omega)$对$P(y|x)$的偏导数.\n$$\n\\begin{aligned}\n    \\frac{\\partial L(P,\\omega)}{\\partial P(y|x)}&=\\sum_{x,y}\\widetilde{P}(x)(logP(y|x)+1)-\\sum_y\\omega_0-\\sum_{x,y}(\\widetilde{P}(x)\\sum^n_{i=1}\\omega_i f_i(x,y)) \\\\\n    &=\\sum_{x,y}\\widetilde{P}(x)(logP(y|x)+1-\\omega_0-\\sum^n_{i=1}\\omega_i f_i(x,y))\n\\end{aligned}\n$$\n&emsp;&emsp;令上式为0,$\\widetilde{P}(x)$>0时,\n$$P(y|x)=exp(\\sum^n_{i=1}\\omega_i f_i(x,y)+\\omega_0-1)=\\frac{exp(\\sum^n_{i=1}\\omega_i f_i(x,y))}{exp(1-\\omega_0)}$$\n&emsp;&emsp;因为约束条件$\\sum_y P(y|x)=1$:\n$$\\sum_yP(y|x)=\\sum_y \\frac{exp(\\sum^n_{i=1}\\omega_i f_i(x,y))}{exp(1-\\omega_0)}=1$$\n$$exp(\\omega_0-1)=\\frac{1}{\\sum_yexp(\\sum^n_{i=1}\\omega_i f_i(x,y))}$$\n&emsp;&emsp;令$Z_\\lambda(x)=\\sum_yexp(\\sum^n_{i=1}\\omega_i f_i(x,y))$,$Z_\\omega(x)$被称为规范化因子,最终的式子为:\n$$P_\\omega(y|x)=\\frac{1}{Z_\\omega(x)}exp(\\sum^n_{i=1}\\omega_i f_i(x,y))$$\n&emsp;&emsp;然后再求极大值问题,也就是再对$\\omega$求偏导,令式子为0,求得$\\omega$的极大值:\n$$argmax_\\omega\\Psi(\\omega)$$\n&emsp;&emsp;最后这一步求极大值等价于最大熵模型的极大似然函数,似然函数的形式为条件概率分布为$P(y|x)$的对数似然函数.","source":"_posts/Maximum-entropy-model.md","raw":"---\ntitle: Maximum-entropy-model\ndate: 2019-06-03 09:53:09\ncategories: 统计学习方法\ntags: machine learning\nmathjax: true\n---\n#### 最大熵模型\n\n&emsp;&emsp;最大熵模型是生成模型,根据最大熵原理,在满足约束条件的模型集合中选择熵最大的模型.在模型集合C中,最大熵模型是条件概率分布$P(Y|X)$上的条件熵$H(P)$最大的模型:\n$$H(P)=-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)$$\n其中$\\widetilde{P}{x}$是经验分布.\n___\n##### 最大熵模型推导\n&emsp;&emsp;条件熵$P(Y|X)$表示已知$X$的情况下$Y$的不确定性,定义为给定$X$的条件下$Y$的条件概率分布的熵对$X$的期望.\n$$\n\\begin{aligned}\nH(Y|X)&=\\sum_x p(x)H(Y|X=x) \\\\\n&=-\\sum_x p(x)\\sum_y p(y|x)logp(y|x) \\\\\n&=-\\sum_{x,y}p(x,y)logp(y|x)  \n\\end{aligned}\n$$\n&emsp;&emsp;如果模型能够获取训练数据中的信息,那么可以假设两个期望相等:\n$$\n\\begin{aligned}\nE_P(X,Y)&=E_{\\widetilde{P}}(X,Y) \\\\\n-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logp(y|x)&=-\\sum_{x,y}\\widetilde{P}(x,y)logp(y|x) \n\\end{aligned}\n$$\n&emsp;&emsp;由此可以得到最大熵模型的公式.\n\n---\n##### 最大熵模型学习过程\n&emsp;&emsp;最大熵模型的学习问题:\n$$\n\\begin{aligned}\nmax_{P\\in C}&H(p)=-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)\\\\\n s.t.\\qquad &E_P(f_i)=E_{\\widetilde{P}(f_i)}, \\quad i=1,2,...,n \\\\\n &\\sum_yP(y|x)=1\n\\end{aligned}\n$$\n&emsp;&emsp;引入拉格朗日乘子$\\omega$,得到拉格朗日函数$L(P,\\omega)$:\n$$\n\\begin{aligned}\n    L(P,\\omega)&\\equiv-H(P)+\\omega_0(1-\\sum_y P(y|x))+\\sum^n_{i=1}\\omega_i(E_{\\widetilde{P}}(f_i)-E_P(f_i)) \\\\\n    &=\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)+\\omega_0(1-\\sum_y P(y|x))+ \\\\\n    &\\quad\\sum^n_{i=1}\\omega_i(\\sum_{x,y}\\widetilde{P}(x,y)f_i(x,y)-\\sum_{x,y}\\widetilde{P}(x)P(y|x)f_i(x,y))\n\\end{aligned}\n$$\n&emsp;&emsp;其中$f_i(x,y)$是二值函数,$x,y$同时满足条件时为1,其他情况为0\n\n&emsp;&emsp;拉格朗日函数的优化问题是:\n$$min_{P\\in C}max_\\omega L(P,\\omega)$$\n&emsp;&emsp;根据拉格朗日对偶性,可以求其对偶问题:\n$$max_\\omega min_{P\\in C}L(P,\\omega)$$\n&emsp;&emsp;首先求$L(P,\\omega)$关于$P$的极小值问题.即求$L(P,\\omega)$对$P(y|x)$的偏导数.\n$$\n\\begin{aligned}\n    \\frac{\\partial L(P,\\omega)}{\\partial P(y|x)}&=\\sum_{x,y}\\widetilde{P}(x)(logP(y|x)+1)-\\sum_y\\omega_0-\\sum_{x,y}(\\widetilde{P}(x)\\sum^n_{i=1}\\omega_i f_i(x,y)) \\\\\n    &=\\sum_{x,y}\\widetilde{P}(x)(logP(y|x)+1-\\omega_0-\\sum^n_{i=1}\\omega_i f_i(x,y))\n\\end{aligned}\n$$\n&emsp;&emsp;令上式为0,$\\widetilde{P}(x)$>0时,\n$$P(y|x)=exp(\\sum^n_{i=1}\\omega_i f_i(x,y)+\\omega_0-1)=\\frac{exp(\\sum^n_{i=1}\\omega_i f_i(x,y))}{exp(1-\\omega_0)}$$\n&emsp;&emsp;因为约束条件$\\sum_y P(y|x)=1$:\n$$\\sum_yP(y|x)=\\sum_y \\frac{exp(\\sum^n_{i=1}\\omega_i f_i(x,y))}{exp(1-\\omega_0)}=1$$\n$$exp(\\omega_0-1)=\\frac{1}{\\sum_yexp(\\sum^n_{i=1}\\omega_i f_i(x,y))}$$\n&emsp;&emsp;令$Z_\\lambda(x)=\\sum_yexp(\\sum^n_{i=1}\\omega_i f_i(x,y))$,$Z_\\omega(x)$被称为规范化因子,最终的式子为:\n$$P_\\omega(y|x)=\\frac{1}{Z_\\omega(x)}exp(\\sum^n_{i=1}\\omega_i f_i(x,y))$$\n&emsp;&emsp;然后再求极大值问题,也就是再对$\\omega$求偏导,令式子为0,求得$\\omega$的极大值:\n$$argmax_\\omega\\Psi(\\omega)$$\n&emsp;&emsp;最后这一步求极大值等价于最大熵模型的极大似然函数,似然函数的形式为条件概率分布为$P(y|x)$的对数似然函数.","slug":"Maximum-entropy-model","published":1,"updated":"2019-06-10T07:42:39.644Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsv000yz4o0iyemwxnc","content":"<h4 id=\"最大熵模型\"><a href=\"#最大熵模型\" class=\"headerlink\" title=\"最大熵模型\"></a>最大熵模型</h4><p>&emsp;&emsp;最大熵模型是生成模型,根据最大熵原理,在满足约束条件的模型集合中选择熵最大的模型.在模型集合C中,最大熵模型是条件概率分布$P(Y|X)$上的条件熵$H(P)$最大的模型:</p>\n<script type=\"math/tex; mode=display\">H(P)=-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)</script><p>其中$\\widetilde{P}{x}$是经验分布.</p>\n<hr>\n<h5 id=\"最大熵模型推导\"><a href=\"#最大熵模型推导\" class=\"headerlink\" title=\"最大熵模型推导\"></a>最大熵模型推导</h5><p>&emsp;&emsp;条件熵$P(Y|X)$表示已知$X$的情况下$Y$的不确定性,定义为给定$X$的条件下$Y$的条件概率分布的熵对$X$的期望.</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nH(Y|X)&=\\sum_x p(x)H(Y|X=x) \\\\\n&=-\\sum_x p(x)\\sum_y p(y|x)logp(y|x) \\\\\n&=-\\sum_{x,y}p(x,y)logp(y|x)  \n\\end{aligned}</script><p>&emsp;&emsp;如果模型能够获取训练数据中的信息,那么可以假设两个期望相等:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nE_P(X,Y)&=E_{\\widetilde{P}}(X,Y) \\\\\n-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logp(y|x)&=-\\sum_{x,y}\\widetilde{P}(x,y)logp(y|x) \n\\end{aligned}</script><p>&emsp;&emsp;由此可以得到最大熵模型的公式.</p>\n<hr>\n<h5 id=\"最大熵模型学习过程\"><a href=\"#最大熵模型学习过程\" class=\"headerlink\" title=\"最大熵模型学习过程\"></a>最大熵模型学习过程</h5><p>&emsp;&emsp;最大熵模型的学习问题:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nmax_{P\\in C}&H(p)=-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)\\\\\n s.t.\\qquad &E_P(f_i)=E_{\\widetilde{P}(f_i)}, \\quad i=1,2,...,n \\\\\n &\\sum_yP(y|x)=1\n\\end{aligned}</script><p>&emsp;&emsp;引入拉格朗日乘子$\\omega$,得到拉格朗日函数$L(P,\\omega)$:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    L(P,\\omega)&\\equiv-H(P)+\\omega_0(1-\\sum_y P(y|x))+\\sum^n_{i=1}\\omega_i(E_{\\widetilde{P}}(f_i)-E_P(f_i)) \\\\\n    &=\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)+\\omega_0(1-\\sum_y P(y|x))+ \\\\\n    &\\quad\\sum^n_{i=1}\\omega_i(\\sum_{x,y}\\widetilde{P}(x,y)f_i(x,y)-\\sum_{x,y}\\widetilde{P}(x)P(y|x)f_i(x,y))\n\\end{aligned}</script><p>&emsp;&emsp;其中$f_i(x,y)$是二值函数,$x,y$同时满足条件时为1,其他情况为0</p>\n<p>&emsp;&emsp;拉格朗日函数的优化问题是:</p>\n<script type=\"math/tex; mode=display\">min_{P\\in C}max_\\omega L(P,\\omega)</script><p>&emsp;&emsp;根据拉格朗日对偶性,可以求其对偶问题:</p>\n<script type=\"math/tex; mode=display\">max_\\omega min_{P\\in C}L(P,\\omega)</script><p>&emsp;&emsp;首先求$L(P,\\omega)$关于$P$的极小值问题.即求$L(P,\\omega)$对$P(y|x)$的偏导数.</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    \\frac{\\partial L(P,\\omega)}{\\partial P(y|x)}&=\\sum_{x,y}\\widetilde{P}(x)(logP(y|x)+1)-\\sum_y\\omega_0-\\sum_{x,y}(\\widetilde{P}(x)\\sum^n_{i=1}\\omega_i f_i(x,y)) \\\\\n    &=\\sum_{x,y}\\widetilde{P}(x)(logP(y|x)+1-\\omega_0-\\sum^n_{i=1}\\omega_i f_i(x,y))\n\\end{aligned}</script><p>&emsp;&emsp;令上式为0,$\\widetilde{P}(x)$&gt;0时,</p>\n<script type=\"math/tex; mode=display\">P(y|x)=exp(\\sum^n_{i=1}\\omega_i f_i(x,y)+\\omega_0-1)=\\frac{exp(\\sum^n_{i=1}\\omega_i f_i(x,y))}{exp(1-\\omega_0)}</script><p>&emsp;&emsp;因为约束条件$\\sum_y P(y|x)=1$:</p>\n<script type=\"math/tex; mode=display\">\\sum_yP(y|x)=\\sum_y \\frac{exp(\\sum^n_{i=1}\\omega_i f_i(x,y))}{exp(1-\\omega_0)}=1</script><script type=\"math/tex; mode=display\">exp(\\omega_0-1)=\\frac{1}{\\sum_yexp(\\sum^n_{i=1}\\omega_i f_i(x,y))}</script><p>&emsp;&emsp;令$Z_\\lambda(x)=\\sum_yexp(\\sum^n_{i=1}\\omega_i f_i(x,y))$,$Z_\\omega(x)$被称为规范化因子,最终的式子为:</p>\n<script type=\"math/tex; mode=display\">P_\\omega(y|x)=\\frac{1}{Z_\\omega(x)}exp(\\sum^n_{i=1}\\omega_i f_i(x,y))</script><p>&emsp;&emsp;然后再求极大值问题,也就是再对$\\omega$求偏导,令式子为0,求得$\\omega$的极大值:</p>\n<script type=\"math/tex; mode=display\">argmax_\\omega\\Psi(\\omega)</script><p>&emsp;&emsp;最后这一步求极大值等价于最大熵模型的极大似然函数,似然函数的形式为条件概率分布为$P(y|x)$的对数似然函数.</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"最大熵模型\"><a href=\"#最大熵模型\" class=\"headerlink\" title=\"最大熵模型\"></a>最大熵模型</h4><p>&emsp;&emsp;最大熵模型是生成模型,根据最大熵原理,在满足约束条件的模型集合中选择熵最大的模型.在模型集合C中,最大熵模型是条件概率分布$P(Y|X)$上的条件熵$H(P)$最大的模型:</p>\n<script type=\"math/tex; mode=display\">H(P)=-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)</script><p>其中$\\widetilde{P}{x}$是经验分布.</p>\n<hr>\n<h5 id=\"最大熵模型推导\"><a href=\"#最大熵模型推导\" class=\"headerlink\" title=\"最大熵模型推导\"></a>最大熵模型推导</h5><p>&emsp;&emsp;条件熵$P(Y|X)$表示已知$X$的情况下$Y$的不确定性,定义为给定$X$的条件下$Y$的条件概率分布的熵对$X$的期望.</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nH(Y|X)&=\\sum_x p(x)H(Y|X=x) \\\\\n&=-\\sum_x p(x)\\sum_y p(y|x)logp(y|x) \\\\\n&=-\\sum_{x,y}p(x,y)logp(y|x)  \n\\end{aligned}</script><p>&emsp;&emsp;如果模型能够获取训练数据中的信息,那么可以假设两个期望相等:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nE_P(X,Y)&=E_{\\widetilde{P}}(X,Y) \\\\\n-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logp(y|x)&=-\\sum_{x,y}\\widetilde{P}(x,y)logp(y|x) \n\\end{aligned}</script><p>&emsp;&emsp;由此可以得到最大熵模型的公式.</p>\n<hr>\n<h5 id=\"最大熵模型学习过程\"><a href=\"#最大熵模型学习过程\" class=\"headerlink\" title=\"最大熵模型学习过程\"></a>最大熵模型学习过程</h5><p>&emsp;&emsp;最大熵模型的学习问题:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nmax_{P\\in C}&H(p)=-\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)\\\\\n s.t.\\qquad &E_P(f_i)=E_{\\widetilde{P}(f_i)}, \\quad i=1,2,...,n \\\\\n &\\sum_yP(y|x)=1\n\\end{aligned}</script><p>&emsp;&emsp;引入拉格朗日乘子$\\omega$,得到拉格朗日函数$L(P,\\omega)$:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    L(P,\\omega)&\\equiv-H(P)+\\omega_0(1-\\sum_y P(y|x))+\\sum^n_{i=1}\\omega_i(E_{\\widetilde{P}}(f_i)-E_P(f_i)) \\\\\n    &=\\sum_{x,y}\\widetilde{P}(x)P(y|x)logP(y|x)+\\omega_0(1-\\sum_y P(y|x))+ \\\\\n    &\\quad\\sum^n_{i=1}\\omega_i(\\sum_{x,y}\\widetilde{P}(x,y)f_i(x,y)-\\sum_{x,y}\\widetilde{P}(x)P(y|x)f_i(x,y))\n\\end{aligned}</script><p>&emsp;&emsp;其中$f_i(x,y)$是二值函数,$x,y$同时满足条件时为1,其他情况为0</p>\n<p>&emsp;&emsp;拉格朗日函数的优化问题是:</p>\n<script type=\"math/tex; mode=display\">min_{P\\in C}max_\\omega L(P,\\omega)</script><p>&emsp;&emsp;根据拉格朗日对偶性,可以求其对偶问题:</p>\n<script type=\"math/tex; mode=display\">max_\\omega min_{P\\in C}L(P,\\omega)</script><p>&emsp;&emsp;首先求$L(P,\\omega)$关于$P$的极小值问题.即求$L(P,\\omega)$对$P(y|x)$的偏导数.</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    \\frac{\\partial L(P,\\omega)}{\\partial P(y|x)}&=\\sum_{x,y}\\widetilde{P}(x)(logP(y|x)+1)-\\sum_y\\omega_0-\\sum_{x,y}(\\widetilde{P}(x)\\sum^n_{i=1}\\omega_i f_i(x,y)) \\\\\n    &=\\sum_{x,y}\\widetilde{P}(x)(logP(y|x)+1-\\omega_0-\\sum^n_{i=1}\\omega_i f_i(x,y))\n\\end{aligned}</script><p>&emsp;&emsp;令上式为0,$\\widetilde{P}(x)$&gt;0时,</p>\n<script type=\"math/tex; mode=display\">P(y|x)=exp(\\sum^n_{i=1}\\omega_i f_i(x,y)+\\omega_0-1)=\\frac{exp(\\sum^n_{i=1}\\omega_i f_i(x,y))}{exp(1-\\omega_0)}</script><p>&emsp;&emsp;因为约束条件$\\sum_y P(y|x)=1$:</p>\n<script type=\"math/tex; mode=display\">\\sum_yP(y|x)=\\sum_y \\frac{exp(\\sum^n_{i=1}\\omega_i f_i(x,y))}{exp(1-\\omega_0)}=1</script><script type=\"math/tex; mode=display\">exp(\\omega_0-1)=\\frac{1}{\\sum_yexp(\\sum^n_{i=1}\\omega_i f_i(x,y))}</script><p>&emsp;&emsp;令$Z_\\lambda(x)=\\sum_yexp(\\sum^n_{i=1}\\omega_i f_i(x,y))$,$Z_\\omega(x)$被称为规范化因子,最终的式子为:</p>\n<script type=\"math/tex; mode=display\">P_\\omega(y|x)=\\frac{1}{Z_\\omega(x)}exp(\\sum^n_{i=1}\\omega_i f_i(x,y))</script><p>&emsp;&emsp;然后再求极大值问题,也就是再对$\\omega$求偏导,令式子为0,求得$\\omega$的极大值:</p>\n<script type=\"math/tex; mode=display\">argmax_\\omega\\Psi(\\omega)</script><p>&emsp;&emsp;最后这一步求极大值等价于最大熵模型的极大似然函数,似然函数的形式为条件概率分布为$P(y|x)$的对数似然函数.</p>\n"},{"title":"Newtwork-chapter4 ","date":"2019-08-30T02:50:18.000Z","visitors":null,"mathjax":true,"_content":"章节四的习题:\n\n#### 4.1\n\na)总共可以收到3条路由,由1,2,3链路.\n\nb)$A\\rightarrow B$采用链路1,$B\\rightarrow A$采用链路2.\n\nc)修改路由信息,或是阻止流量通过链路1.\n\n#### 4.5\na)\nSwitch|Address | Next-hop \n:---: | :---:| :---:\nP||\n||C1.A3.0.0/16|PA\n||C1.B0.0.0/12|PB\n||C2.0.0.0/8|Q\n||C3.0.0.0/8|R\n\nSwitch|Address | Next-hop \n:---: | :---:| :---:\nQ||\n||C2.0A.10.0/20|QA\n||C2.0B.0.0/16|QB\n||C1.0.0.0/8|P\n||C3.0.0.0/8|R\n\nSwitch|Address | Next-hop \n:---: | :---:| :---:\nR||\n||C1.0.0.0/8|P\n||C2.0.0.0/8|Q\n\nb)同上,但是P的路由表中C3.0.0.0/8 R改为C3.0.0.0/8 Q,\n\nR的路由表中C1.0.0.0/8 P改为C1.0.0.0/8 Q.\n\nc)同a,除了在去掉R的路由信息,P的路由表中添加C2.0A.10.0/20 QA,\n\nQ的路由表中添加C1.A3.0.0/16 PA\n\n#### 4.12\nIPv6中使用以11111111为前缀的地址作为多播地址. IPv4中使用D类地址作为多播地址,以1110作为前缀.","source":"_posts/Network-chapter4.md","raw":"---\ntitle: 'Newtwork-chapter4 '\ndate: 2019-08-30 10:50:18\ncategories: Computer Network A Systems Approach\nvisitors: \nmathjax: true\ntags: Network\n---\n章节四的习题:\n\n#### 4.1\n\na)总共可以收到3条路由,由1,2,3链路.\n\nb)$A\\rightarrow B$采用链路1,$B\\rightarrow A$采用链路2.\n\nc)修改路由信息,或是阻止流量通过链路1.\n\n#### 4.5\na)\nSwitch|Address | Next-hop \n:---: | :---:| :---:\nP||\n||C1.A3.0.0/16|PA\n||C1.B0.0.0/12|PB\n||C2.0.0.0/8|Q\n||C3.0.0.0/8|R\n\nSwitch|Address | Next-hop \n:---: | :---:| :---:\nQ||\n||C2.0A.10.0/20|QA\n||C2.0B.0.0/16|QB\n||C1.0.0.0/8|P\n||C3.0.0.0/8|R\n\nSwitch|Address | Next-hop \n:---: | :---:| :---:\nR||\n||C1.0.0.0/8|P\n||C2.0.0.0/8|Q\n\nb)同上,但是P的路由表中C3.0.0.0/8 R改为C3.0.0.0/8 Q,\n\nR的路由表中C1.0.0.0/8 P改为C1.0.0.0/8 Q.\n\nc)同a,除了在去掉R的路由信息,P的路由表中添加C2.0A.10.0/20 QA,\n\nQ的路由表中添加C1.A3.0.0/16 PA\n\n#### 4.12\nIPv6中使用以11111111为前缀的地址作为多播地址. IPv4中使用D类地址作为多播地址,以1110作为前缀.","slug":"Network-chapter4","published":1,"updated":"2019-08-31T01:19:14.607Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsw0011z4o081xi7d4h","content":"<p>章节四的习题:</p>\n<h4 id=\"4-1\"><a href=\"#4-1\" class=\"headerlink\" title=\"4.1\"></a>4.1</h4><p>a)总共可以收到3条路由,由1,2,3链路.</p>\n<p>b)$A\\rightarrow B$采用链路1,$B\\rightarrow A$采用链路2.</p>\n<p>c)修改路由信息,或是阻止流量通过链路1.</p>\n<h4 id=\"4-5\"><a href=\"#4-5\" class=\"headerlink\" title=\"4.5\"></a>4.5</h4><p>a)<br>Switch|Address | Next-hop<br>:—-: | :—-:| :—-:<br>P||<br>||C1.A3.0.0/16|PA<br>||C1.B0.0.0/12|PB<br>||C2.0.0.0/8|Q<br>||C3.0.0.0/8|R</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Switch</th>\n<th style=\"text-align:center\">Address</th>\n<th style=\"text-align:center\">Next-hop </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Q</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C2.0A.10.0/20</td>\n<td>QA</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C2.0B.0.0/16</td>\n<td>QB</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C1.0.0.0/8</td>\n<td>P</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C3.0.0.0/8</td>\n<td>R</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Switch</th>\n<th style=\"text-align:center\">Address</th>\n<th style=\"text-align:center\">Next-hop </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">R</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C1.0.0.0/8</td>\n<td>P</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C2.0.0.0/8</td>\n<td>Q</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>b)同上,但是P的路由表中C3.0.0.0/8 R改为C3.0.0.0/8 Q,</p>\n<p>R的路由表中C1.0.0.0/8 P改为C1.0.0.0/8 Q.</p>\n<p>c)同a,除了在去掉R的路由信息,P的路由表中添加C2.0A.10.0/20 QA,</p>\n<p>Q的路由表中添加C1.A3.0.0/16 PA</p>\n<h4 id=\"4-12\"><a href=\"#4-12\" class=\"headerlink\" title=\"4.12\"></a>4.12</h4><p>IPv6中使用以11111111为前缀的地址作为多播地址. IPv4中使用D类地址作为多播地址,以1110作为前缀.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>章节四的习题:</p>\n<h4 id=\"4-1\"><a href=\"#4-1\" class=\"headerlink\" title=\"4.1\"></a>4.1</h4><p>a)总共可以收到3条路由,由1,2,3链路.</p>\n<p>b)$A\\rightarrow B$采用链路1,$B\\rightarrow A$采用链路2.</p>\n<p>c)修改路由信息,或是阻止流量通过链路1.</p>\n<h4 id=\"4-5\"><a href=\"#4-5\" class=\"headerlink\" title=\"4.5\"></a>4.5</h4><p>a)<br>Switch|Address | Next-hop<br>:—-: | :—-:| :—-:<br>P||<br>||C1.A3.0.0/16|PA<br>||C1.B0.0.0/12|PB<br>||C2.0.0.0/8|Q<br>||C3.0.0.0/8|R</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Switch</th>\n<th style=\"text-align:center\">Address</th>\n<th style=\"text-align:center\">Next-hop </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Q</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C2.0A.10.0/20</td>\n<td>QA</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C2.0B.0.0/16</td>\n<td>QB</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C1.0.0.0/8</td>\n<td>P</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C3.0.0.0/8</td>\n<td>R</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Switch</th>\n<th style=\"text-align:center\">Address</th>\n<th style=\"text-align:center\">Next-hop </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">R</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C1.0.0.0/8</td>\n<td>P</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C2.0.0.0/8</td>\n<td>Q</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>b)同上,但是P的路由表中C3.0.0.0/8 R改为C3.0.0.0/8 Q,</p>\n<p>R的路由表中C1.0.0.0/8 P改为C1.0.0.0/8 Q.</p>\n<p>c)同a,除了在去掉R的路由信息,P的路由表中添加C2.0A.10.0/20 QA,</p>\n<p>Q的路由表中添加C1.A3.0.0/16 PA</p>\n<h4 id=\"4-12\"><a href=\"#4-12\" class=\"headerlink\" title=\"4.12\"></a>4.12</h4><p>IPv6中使用以11111111为前缀的地址作为多播地址. IPv4中使用D类地址作为多播地址,以1110作为前缀.</p>\n"},{"title":"Network-chapter1","date":"2019-07-23T01:59:02.000Z","visitors":null,"mathjax":true,"_content":"章节一的习题:\n\n#### 1.1\n\n&emsp;&emsp;匿名ftp有三种方法:\n1. 用户名为anonymous,密码为空\n2. 用户名为FTP,密码为空或FTP\n3. 用户名为USER,密码为pass\n\n&emsp;&emsp;majaro可以使用fileZilla这个ftp工具. 题中的ftp站点结构和题目描述不符, 如果需要RFC规范可以在IETF(Internet Engineering Task Force)获取. https://www.ietf.org/rfc/\n\n#### 1.2\n\n&emsp;&emsp;whois用于查询域名和组织,例如查询google的域名信息,可以使用两种方法:\n```bash\nwhois google\nwhois google.com\n```\n\n#### 1.3\n\n1. &emsp;$2\\times RTT+1000KB/1.5Mbps+RTT/2\\approx 5.458s$\n第一项是题目给出的初始握手阶段的时延,第二项是传输的时延$Size/Bandwidth=8Mb/1.5Mbps\\approx 5.333s$.第三项是单程的传播时延.\n1. 在上一题的基础上再加上999个RTT, $5.458s+RTT*999=55.408s$\n2. 一共需要49.5个RTT(最后一次传输只用0.5RTT),加上初始时延,$(49.5+2)*RTT=2.575s$\n3. 一共需要9.5个RTT,因此为$(9.5+2)*RTT=0.575s$\n\n#### 1.4\n\n1. &emsp;$2\\times RTT+1.5MB/10Mbps+RTT/2\\approx 1.4s$ \n2. &emsp;$1.4s+(1.5*10^3-1)RTT=121.32s$\n3. 一共需要1499.5个RTT,$(74.5+2)*RTT=6.12s$\n4. 一共需要10.5个RTT,因此为$(10.5+2)*RTT=1s$\n\n#### 1.5&1.6\n\n可得$50*10^3m/(2*10^8m/s)=100*8b/x$,解得带宽为3.2Mbps.在512字节的情况下带宽为16.384Mbps.\n\n#### 1.7\n\n&emsp;&emsp;在同一市下属的各个区,邮政地址中前面的若干位是相同的,变动的是后几位的数字.可变的位数是根据下属区的个数而定.电话号码类似,不同的位数有特殊的含义,并且有可变的掩码.\n\n#### 1.8\n\n&emsp;&emsp;存在地址不唯一的情况,在不同的内网,两个节点的地址是可能相等的,也就是说两个节点在各自的内网地址是同样的地址,如192.168.1.2\n\n#### 1.9\n\n&emsp;&emsp;组播能够有效减少冗余广播,常见的应用场景包括电视电话会议,在线教育等.\n\n#### 1.10\n\n&emsp;&emsp;FDM和STDM的问题是信道的利用率不够高,在传统的电视和电话网络中,没有足够的交互性,如电视的数据流是单向的,用户只接受数据流.电话类似,两个人不会同时都在说话.但是计算机网络中,很多突发性和长时间的数据流.使用FDM和STDM的话,信道效率不高.\n\n#### 1.11\n\n每比特的传输时间为$1/10^9bps=1\\mu s$, 长度为$1*10^{-9} s*2.3*10^8m/s=0.23m$\n\n#### 1.12\n\n共需要$(x*8*10^3)/(y*10^6)=8x/y*10^{-3}=8x/y\\text{ms}$\n\n#### 1.13&1.14\n\n1. 最小RTT为$2*55*10^9/3*10^{-8}=1100/3s=367s$\n2. 延迟带宽积为$RTT/2*128*10^3=2.348*10^7bit=2.93MB$\n3. 总时延为$5MB/128kbps+RTT/2=5*8*10^6/128*10^3+183.5=496s$\n\n#### 1.17\n\n1. 总时延为$2*(5*10^3b/1Gbps+10*10^{-6})=30\\mu s$\n2. 有三个交换机,即是有4个链路总时延为上题的两倍,即$60\\mu s$\n3. 对于直通式转发,交换机上只有$128b/1Gbps=128ns$的延迟,加上4个传播延迟和1个传输延迟,总延迟为$45.384\\mu s$.\n\n#### 1.27\n\n1. 带宽为$1920*1080*24*30=1.492992Gbps\\approx 1.5Gbps$\n2. 带宽为$8000*8=64Kbps$\n3. 带宽为$260*50=13kbps$\n4. 带宽为$24*88200=2.1168Mbps\\approx 2.1Mbps$\n\n#### 1.37\n\n![](https://i.loli.net/2019/07/24/5d37afba402a934744.png)\n&emsp;&emsp;这里只设置了5跳,星号可能是防火墙封掉了ICMP的返回.第一跳是我的网关,一共发送4个数据包,后面显示的是数据包返回的时间.\n\n&emsp;&emsp;traceroute是利用ICMP包和IP头的TTL位.具体是traceroute先发送一个TTL为1的数据包,当到达路径上第一个路由时,TTL减1,路由器收到TTL为0的包便会丢弃,并返回ICMP time exceeded的信息,traceroute收到返回的包,便可知道路径上的路由器信息.接着发送第二个包,TTL为2.以此累加,一直到到达目的地址.因为traceroute发送的包中port number是一个一般应用程序不会用到的端口(30000以上),因此目标主机会返回ICMP port unreachable的信息.traceroute收到信息就知道已到达了目标地址.\n\n&emsp;&emsp;很显然可以看出对于一些路由器丢弃TTL为0的包时,不返回信息,那么traceroute就没法工作.","source":"_posts/Network-chapter1.md","raw":"---\ntitle: Network-chapter1\ndate: 2019-07-23 09:59:02\ncategories: Computer Network A Systems Approach\nvisitors: \nmathjax: true\ntags: Network\n---\n章节一的习题:\n\n#### 1.1\n\n&emsp;&emsp;匿名ftp有三种方法:\n1. 用户名为anonymous,密码为空\n2. 用户名为FTP,密码为空或FTP\n3. 用户名为USER,密码为pass\n\n&emsp;&emsp;majaro可以使用fileZilla这个ftp工具. 题中的ftp站点结构和题目描述不符, 如果需要RFC规范可以在IETF(Internet Engineering Task Force)获取. https://www.ietf.org/rfc/\n\n#### 1.2\n\n&emsp;&emsp;whois用于查询域名和组织,例如查询google的域名信息,可以使用两种方法:\n```bash\nwhois google\nwhois google.com\n```\n\n#### 1.3\n\n1. &emsp;$2\\times RTT+1000KB/1.5Mbps+RTT/2\\approx 5.458s$\n第一项是题目给出的初始握手阶段的时延,第二项是传输的时延$Size/Bandwidth=8Mb/1.5Mbps\\approx 5.333s$.第三项是单程的传播时延.\n1. 在上一题的基础上再加上999个RTT, $5.458s+RTT*999=55.408s$\n2. 一共需要49.5个RTT(最后一次传输只用0.5RTT),加上初始时延,$(49.5+2)*RTT=2.575s$\n3. 一共需要9.5个RTT,因此为$(9.5+2)*RTT=0.575s$\n\n#### 1.4\n\n1. &emsp;$2\\times RTT+1.5MB/10Mbps+RTT/2\\approx 1.4s$ \n2. &emsp;$1.4s+(1.5*10^3-1)RTT=121.32s$\n3. 一共需要1499.5个RTT,$(74.5+2)*RTT=6.12s$\n4. 一共需要10.5个RTT,因此为$(10.5+2)*RTT=1s$\n\n#### 1.5&1.6\n\n可得$50*10^3m/(2*10^8m/s)=100*8b/x$,解得带宽为3.2Mbps.在512字节的情况下带宽为16.384Mbps.\n\n#### 1.7\n\n&emsp;&emsp;在同一市下属的各个区,邮政地址中前面的若干位是相同的,变动的是后几位的数字.可变的位数是根据下属区的个数而定.电话号码类似,不同的位数有特殊的含义,并且有可变的掩码.\n\n#### 1.8\n\n&emsp;&emsp;存在地址不唯一的情况,在不同的内网,两个节点的地址是可能相等的,也就是说两个节点在各自的内网地址是同样的地址,如192.168.1.2\n\n#### 1.9\n\n&emsp;&emsp;组播能够有效减少冗余广播,常见的应用场景包括电视电话会议,在线教育等.\n\n#### 1.10\n\n&emsp;&emsp;FDM和STDM的问题是信道的利用率不够高,在传统的电视和电话网络中,没有足够的交互性,如电视的数据流是单向的,用户只接受数据流.电话类似,两个人不会同时都在说话.但是计算机网络中,很多突发性和长时间的数据流.使用FDM和STDM的话,信道效率不高.\n\n#### 1.11\n\n每比特的传输时间为$1/10^9bps=1\\mu s$, 长度为$1*10^{-9} s*2.3*10^8m/s=0.23m$\n\n#### 1.12\n\n共需要$(x*8*10^3)/(y*10^6)=8x/y*10^{-3}=8x/y\\text{ms}$\n\n#### 1.13&1.14\n\n1. 最小RTT为$2*55*10^9/3*10^{-8}=1100/3s=367s$\n2. 延迟带宽积为$RTT/2*128*10^3=2.348*10^7bit=2.93MB$\n3. 总时延为$5MB/128kbps+RTT/2=5*8*10^6/128*10^3+183.5=496s$\n\n#### 1.17\n\n1. 总时延为$2*(5*10^3b/1Gbps+10*10^{-6})=30\\mu s$\n2. 有三个交换机,即是有4个链路总时延为上题的两倍,即$60\\mu s$\n3. 对于直通式转发,交换机上只有$128b/1Gbps=128ns$的延迟,加上4个传播延迟和1个传输延迟,总延迟为$45.384\\mu s$.\n\n#### 1.27\n\n1. 带宽为$1920*1080*24*30=1.492992Gbps\\approx 1.5Gbps$\n2. 带宽为$8000*8=64Kbps$\n3. 带宽为$260*50=13kbps$\n4. 带宽为$24*88200=2.1168Mbps\\approx 2.1Mbps$\n\n#### 1.37\n\n![](https://i.loli.net/2019/07/24/5d37afba402a934744.png)\n&emsp;&emsp;这里只设置了5跳,星号可能是防火墙封掉了ICMP的返回.第一跳是我的网关,一共发送4个数据包,后面显示的是数据包返回的时间.\n\n&emsp;&emsp;traceroute是利用ICMP包和IP头的TTL位.具体是traceroute先发送一个TTL为1的数据包,当到达路径上第一个路由时,TTL减1,路由器收到TTL为0的包便会丢弃,并返回ICMP time exceeded的信息,traceroute收到返回的包,便可知道路径上的路由器信息.接着发送第二个包,TTL为2.以此累加,一直到到达目的地址.因为traceroute发送的包中port number是一个一般应用程序不会用到的端口(30000以上),因此目标主机会返回ICMP port unreachable的信息.traceroute收到信息就知道已到达了目标地址.\n\n&emsp;&emsp;很显然可以看出对于一些路由器丢弃TTL为0的包时,不返回信息,那么traceroute就没法工作.","slug":"Network-chapter1","published":1,"updated":"2019-07-24T01:35:46.752Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsx0014z4o0ecjmshch","content":"<p>章节一的习题:</p>\n<h4 id=\"1-1\"><a href=\"#1-1\" class=\"headerlink\" title=\"1.1\"></a>1.1</h4><p>&emsp;&emsp;匿名ftp有三种方法:</p>\n<ol>\n<li>用户名为anonymous,密码为空</li>\n<li>用户名为FTP,密码为空或FTP</li>\n<li>用户名为USER,密码为pass</li>\n</ol>\n<p>&emsp;&emsp;majaro可以使用fileZilla这个ftp工具. 题中的ftp站点结构和题目描述不符, 如果需要RFC规范可以在IETF(Internet Engineering Task Force)获取. <a href=\"https://www.ietf.org/rfc/\" target=\"_blank\" rel=\"noopener\">https://www.ietf.org/rfc/</a></p>\n<h4 id=\"1-2\"><a href=\"#1-2\" class=\"headerlink\" title=\"1.2\"></a>1.2</h4><p>&emsp;&emsp;whois用于查询域名和组织,例如查询google的域名信息,可以使用两种方法:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">whois google</span><br><span class=\"line\">whois google.com</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-3\"><a href=\"#1-3\" class=\"headerlink\" title=\"1.3\"></a>1.3</h4><ol>\n<li>&emsp;$2\\times RTT+1000KB/1.5Mbps+RTT/2\\approx 5.458s$<br>第一项是题目给出的初始握手阶段的时延,第二项是传输的时延$Size/Bandwidth=8Mb/1.5Mbps\\approx 5.333s$.第三项是单程的传播时延.</li>\n<li>在上一题的基础上再加上999个RTT, $5.458s+RTT*999=55.408s$</li>\n<li>一共需要49.5个RTT(最后一次传输只用0.5RTT),加上初始时延,$(49.5+2)*RTT=2.575s$</li>\n<li>一共需要9.5个RTT,因此为$(9.5+2)*RTT=0.575s$</li>\n</ol>\n<h4 id=\"1-4\"><a href=\"#1-4\" class=\"headerlink\" title=\"1.4\"></a>1.4</h4><ol>\n<li>&emsp;$2\\times RTT+1.5MB/10Mbps+RTT/2\\approx 1.4s$ </li>\n<li>&emsp;$1.4s+(1.5*10^3-1)RTT=121.32s$</li>\n<li>一共需要1499.5个RTT,$(74.5+2)*RTT=6.12s$</li>\n<li>一共需要10.5个RTT,因此为$(10.5+2)*RTT=1s$</li>\n</ol>\n<h4 id=\"1-5-amp-1-6\"><a href=\"#1-5-amp-1-6\" class=\"headerlink\" title=\"1.5&amp;1.6\"></a>1.5&amp;1.6</h4><p>可得$50<em>10^3m/(2</em>10^8m/s)=100*8b/x$,解得带宽为3.2Mbps.在512字节的情况下带宽为16.384Mbps.</p>\n<h4 id=\"1-7\"><a href=\"#1-7\" class=\"headerlink\" title=\"1.7\"></a>1.7</h4><p>&emsp;&emsp;在同一市下属的各个区,邮政地址中前面的若干位是相同的,变动的是后几位的数字.可变的位数是根据下属区的个数而定.电话号码类似,不同的位数有特殊的含义,并且有可变的掩码.</p>\n<h4 id=\"1-8\"><a href=\"#1-8\" class=\"headerlink\" title=\"1.8\"></a>1.8</h4><p>&emsp;&emsp;存在地址不唯一的情况,在不同的内网,两个节点的地址是可能相等的,也就是说两个节点在各自的内网地址是同样的地址,如192.168.1.2</p>\n<h4 id=\"1-9\"><a href=\"#1-9\" class=\"headerlink\" title=\"1.9\"></a>1.9</h4><p>&emsp;&emsp;组播能够有效减少冗余广播,常见的应用场景包括电视电话会议,在线教育等.</p>\n<h4 id=\"1-10\"><a href=\"#1-10\" class=\"headerlink\" title=\"1.10\"></a>1.10</h4><p>&emsp;&emsp;FDM和STDM的问题是信道的利用率不够高,在传统的电视和电话网络中,没有足够的交互性,如电视的数据流是单向的,用户只接受数据流.电话类似,两个人不会同时都在说话.但是计算机网络中,很多突发性和长时间的数据流.使用FDM和STDM的话,信道效率不高.</p>\n<h4 id=\"1-11\"><a href=\"#1-11\" class=\"headerlink\" title=\"1.11\"></a>1.11</h4><p>每比特的传输时间为$1/10^9bps=1\\mu s$, 长度为$1<em>10^{-9} s</em>2.3*10^8m/s=0.23m$</p>\n<h4 id=\"1-12\"><a href=\"#1-12\" class=\"headerlink\" title=\"1.12\"></a>1.12</h4><p>共需要$(x<em>8</em>10^3)/(y<em>10^6)=8x/y</em>10^{-3}=8x/y\\text{ms}$</p>\n<h4 id=\"1-13-amp-1-14\"><a href=\"#1-13-amp-1-14\" class=\"headerlink\" title=\"1.13&amp;1.14\"></a>1.13&amp;1.14</h4><ol>\n<li>最小RTT为$2<em>55</em>10^9/3*10^{-8}=1100/3s=367s$</li>\n<li>延迟带宽积为$RTT/2<em>128</em>10^3=2.348*10^7bit=2.93MB$</li>\n<li>总时延为$5MB/128kbps+RTT/2=5<em>8</em>10^6/128*10^3+183.5=496s$</li>\n</ol>\n<h4 id=\"1-17\"><a href=\"#1-17\" class=\"headerlink\" title=\"1.17\"></a>1.17</h4><ol>\n<li>总时延为$2<em>(5</em>10^3b/1Gbps+10*10^{-6})=30\\mu s$</li>\n<li>有三个交换机,即是有4个链路总时延为上题的两倍,即$60\\mu s$</li>\n<li>对于直通式转发,交换机上只有$128b/1Gbps=128ns$的延迟,加上4个传播延迟和1个传输延迟,总延迟为$45.384\\mu s$.</li>\n</ol>\n<h4 id=\"1-27\"><a href=\"#1-27\" class=\"headerlink\" title=\"1.27\"></a>1.27</h4><ol>\n<li>带宽为$1920<em>1080</em>24*30=1.492992Gbps\\approx 1.5Gbps$</li>\n<li>带宽为$8000*8=64Kbps$</li>\n<li>带宽为$260*50=13kbps$</li>\n<li>带宽为$24*88200=2.1168Mbps\\approx 2.1Mbps$</li>\n</ol>\n<h4 id=\"1-37\"><a href=\"#1-37\" class=\"headerlink\" title=\"1.37\"></a>1.37</h4><p><img src=\"https://i.loli.net/2019/07/24/5d37afba402a934744.png\" alt><br>&emsp;&emsp;这里只设置了5跳,星号可能是防火墙封掉了ICMP的返回.第一跳是我的网关,一共发送4个数据包,后面显示的是数据包返回的时间.</p>\n<p>&emsp;&emsp;traceroute是利用ICMP包和IP头的TTL位.具体是traceroute先发送一个TTL为1的数据包,当到达路径上第一个路由时,TTL减1,路由器收到TTL为0的包便会丢弃,并返回ICMP time exceeded的信息,traceroute收到返回的包,便可知道路径上的路由器信息.接着发送第二个包,TTL为2.以此累加,一直到到达目的地址.因为traceroute发送的包中port number是一个一般应用程序不会用到的端口(30000以上),因此目标主机会返回ICMP port unreachable的信息.traceroute收到信息就知道已到达了目标地址.</p>\n<p>&emsp;&emsp;很显然可以看出对于一些路由器丢弃TTL为0的包时,不返回信息,那么traceroute就没法工作.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>章节一的习题:</p>\n<h4 id=\"1-1\"><a href=\"#1-1\" class=\"headerlink\" title=\"1.1\"></a>1.1</h4><p>&emsp;&emsp;匿名ftp有三种方法:</p>\n<ol>\n<li>用户名为anonymous,密码为空</li>\n<li>用户名为FTP,密码为空或FTP</li>\n<li>用户名为USER,密码为pass</li>\n</ol>\n<p>&emsp;&emsp;majaro可以使用fileZilla这个ftp工具. 题中的ftp站点结构和题目描述不符, 如果需要RFC规范可以在IETF(Internet Engineering Task Force)获取. <a href=\"https://www.ietf.org/rfc/\" target=\"_blank\" rel=\"noopener\">https://www.ietf.org/rfc/</a></p>\n<h4 id=\"1-2\"><a href=\"#1-2\" class=\"headerlink\" title=\"1.2\"></a>1.2</h4><p>&emsp;&emsp;whois用于查询域名和组织,例如查询google的域名信息,可以使用两种方法:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">whois google</span><br><span class=\"line\">whois google.com</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-3\"><a href=\"#1-3\" class=\"headerlink\" title=\"1.3\"></a>1.3</h4><ol>\n<li>&emsp;$2\\times RTT+1000KB/1.5Mbps+RTT/2\\approx 5.458s$<br>第一项是题目给出的初始握手阶段的时延,第二项是传输的时延$Size/Bandwidth=8Mb/1.5Mbps\\approx 5.333s$.第三项是单程的传播时延.</li>\n<li>在上一题的基础上再加上999个RTT, $5.458s+RTT*999=55.408s$</li>\n<li>一共需要49.5个RTT(最后一次传输只用0.5RTT),加上初始时延,$(49.5+2)*RTT=2.575s$</li>\n<li>一共需要9.5个RTT,因此为$(9.5+2)*RTT=0.575s$</li>\n</ol>\n<h4 id=\"1-4\"><a href=\"#1-4\" class=\"headerlink\" title=\"1.4\"></a>1.4</h4><ol>\n<li>&emsp;$2\\times RTT+1.5MB/10Mbps+RTT/2\\approx 1.4s$ </li>\n<li>&emsp;$1.4s+(1.5*10^3-1)RTT=121.32s$</li>\n<li>一共需要1499.5个RTT,$(74.5+2)*RTT=6.12s$</li>\n<li>一共需要10.5个RTT,因此为$(10.5+2)*RTT=1s$</li>\n</ol>\n<h4 id=\"1-5-amp-1-6\"><a href=\"#1-5-amp-1-6\" class=\"headerlink\" title=\"1.5&amp;1.6\"></a>1.5&amp;1.6</h4><p>可得$50<em>10^3m/(2</em>10^8m/s)=100*8b/x$,解得带宽为3.2Mbps.在512字节的情况下带宽为16.384Mbps.</p>\n<h4 id=\"1-7\"><a href=\"#1-7\" class=\"headerlink\" title=\"1.7\"></a>1.7</h4><p>&emsp;&emsp;在同一市下属的各个区,邮政地址中前面的若干位是相同的,变动的是后几位的数字.可变的位数是根据下属区的个数而定.电话号码类似,不同的位数有特殊的含义,并且有可变的掩码.</p>\n<h4 id=\"1-8\"><a href=\"#1-8\" class=\"headerlink\" title=\"1.8\"></a>1.8</h4><p>&emsp;&emsp;存在地址不唯一的情况,在不同的内网,两个节点的地址是可能相等的,也就是说两个节点在各自的内网地址是同样的地址,如192.168.1.2</p>\n<h4 id=\"1-9\"><a href=\"#1-9\" class=\"headerlink\" title=\"1.9\"></a>1.9</h4><p>&emsp;&emsp;组播能够有效减少冗余广播,常见的应用场景包括电视电话会议,在线教育等.</p>\n<h4 id=\"1-10\"><a href=\"#1-10\" class=\"headerlink\" title=\"1.10\"></a>1.10</h4><p>&emsp;&emsp;FDM和STDM的问题是信道的利用率不够高,在传统的电视和电话网络中,没有足够的交互性,如电视的数据流是单向的,用户只接受数据流.电话类似,两个人不会同时都在说话.但是计算机网络中,很多突发性和长时间的数据流.使用FDM和STDM的话,信道效率不高.</p>\n<h4 id=\"1-11\"><a href=\"#1-11\" class=\"headerlink\" title=\"1.11\"></a>1.11</h4><p>每比特的传输时间为$1/10^9bps=1\\mu s$, 长度为$1<em>10^{-9} s</em>2.3*10^8m/s=0.23m$</p>\n<h4 id=\"1-12\"><a href=\"#1-12\" class=\"headerlink\" title=\"1.12\"></a>1.12</h4><p>共需要$(x<em>8</em>10^3)/(y<em>10^6)=8x/y</em>10^{-3}=8x/y\\text{ms}$</p>\n<h4 id=\"1-13-amp-1-14\"><a href=\"#1-13-amp-1-14\" class=\"headerlink\" title=\"1.13&amp;1.14\"></a>1.13&amp;1.14</h4><ol>\n<li>最小RTT为$2<em>55</em>10^9/3*10^{-8}=1100/3s=367s$</li>\n<li>延迟带宽积为$RTT/2<em>128</em>10^3=2.348*10^7bit=2.93MB$</li>\n<li>总时延为$5MB/128kbps+RTT/2=5<em>8</em>10^6/128*10^3+183.5=496s$</li>\n</ol>\n<h4 id=\"1-17\"><a href=\"#1-17\" class=\"headerlink\" title=\"1.17\"></a>1.17</h4><ol>\n<li>总时延为$2<em>(5</em>10^3b/1Gbps+10*10^{-6})=30\\mu s$</li>\n<li>有三个交换机,即是有4个链路总时延为上题的两倍,即$60\\mu s$</li>\n<li>对于直通式转发,交换机上只有$128b/1Gbps=128ns$的延迟,加上4个传播延迟和1个传输延迟,总延迟为$45.384\\mu s$.</li>\n</ol>\n<h4 id=\"1-27\"><a href=\"#1-27\" class=\"headerlink\" title=\"1.27\"></a>1.27</h4><ol>\n<li>带宽为$1920<em>1080</em>24*30=1.492992Gbps\\approx 1.5Gbps$</li>\n<li>带宽为$8000*8=64Kbps$</li>\n<li>带宽为$260*50=13kbps$</li>\n<li>带宽为$24*88200=2.1168Mbps\\approx 2.1Mbps$</li>\n</ol>\n<h4 id=\"1-37\"><a href=\"#1-37\" class=\"headerlink\" title=\"1.37\"></a>1.37</h4><p><img src=\"https://i.loli.net/2019/07/24/5d37afba402a934744.png\" alt><br>&emsp;&emsp;这里只设置了5跳,星号可能是防火墙封掉了ICMP的返回.第一跳是我的网关,一共发送4个数据包,后面显示的是数据包返回的时间.</p>\n<p>&emsp;&emsp;traceroute是利用ICMP包和IP头的TTL位.具体是traceroute先发送一个TTL为1的数据包,当到达路径上第一个路由时,TTL减1,路由器收到TTL为0的包便会丢弃,并返回ICMP time exceeded的信息,traceroute收到返回的包,便可知道路径上的路由器信息.接着发送第二个包,TTL为2.以此累加,一直到到达目的地址.因为traceroute发送的包中port number是一个一般应用程序不会用到的端口(30000以上),因此目标主机会返回ICMP port unreachable的信息.traceroute收到信息就知道已到达了目标地址.</p>\n<p>&emsp;&emsp;很显然可以看出对于一些路由器丢弃TTL为0的包时,不返回信息,那么traceroute就没法工作.</p>\n"},{"title":"Network-chapter3","date":"2019-08-24T05:43:33.000Z","visitors":null,"mathjax":true,"_content":"章节三的习题:\n\n#### 3.2\n\nExercise | Switch | Input | Output\n:---: | :---: | :---: | :---:\n| | |{Port, VCI} | {Port, VCI}\na) | 1 | 0&emsp;0 | 1&emsp;0\n|| 2 | 3&emsp;0 | 1&emsp;0\n|| 4 | 3&emsp;0 | 0&emsp;0\nb) | 3 | 3&emsp;0 | 0&emsp;0\n|| 2 | 0&emsp;0 | 1&emsp;1\n|| 4 | 3&emsp;1 | 1&emsp;0\nc) | 4 | 2&emsp;0 | 3&emsp;2\n|| 2 | 1&emsp;2 | 3&emsp;1\n|| 1 | 1&emsp;1 | 2&emsp;0\nd) | 4 | 0&emsp;1 | 3&emsp;3\n|| 2 | 1&emsp;3 | 3&emsp;2\n|| 1 | 1&emsp;2 | 3&emsp;0\ne) | 3 | 2&emsp;0 | 0&emsp;1\n|| 1 | 0&emsp;1 | 2&emsp;0\nf) | 4 | 0&emsp;2 | 3&emsp;4\n|| 2 | 1&emsp;4 | 0&emsp;2\n|| 3 | 0&emsp;2 | 1&emsp;0\n\n#### 3.3\n\nSwitch | Destination | Next hip\n:---: | :---: | :---:\nA | B | C\n|| C | C\n|| D | C\n|| E | C\n|| F | C\nB | A | E\n|| C | E\n|| D | E \n|| E | E\n|| F | E\nC | A | A\n|| B | E\n|| D | E\n|| E | E\n|| F | F\nD | A | E\n|| B | E\n|| C | E\n|| E | E\n|| F | E\nE | A | C\n|| B | B\n|| C | C\n||D|D\n||F|C\nF|A|C\n||B|C\n||C|C\n||D|C\n||E|C\n\n#### 3.5\n\n&emsp;&emsp;一共有三条VC\n\n1)从A到B,使用S1的1和2端口.\n\n2)从A到D,使用S1的1和3端口,S2的1和3端口,S3的1和2端口.\n\n3)从B到D,使用S1的2和3端口,S2的1和3端口,S3的1和2端口.\n\n#### 3.13\n\n&emsp;&emsp;网桥和端口之间的映射关系为\nBridge | Port\n:---: | :---:\nB1|C&emsp;E\nB2|A&emsp;B\nB3|D&emsp;E&emsp;F&emsp;G&emsp;H\nB4|G&emsp;I\nB5|Idle\nB6|J&emsp;H\nB7|A&emsp;C\n\n#### 3.15\nB1: A-interface: A; B2-interface: C\nB2: B1-interface: A; B3-interface: C; B4-interface: D\nB3: B2-interface: A,D; C-interface: C\nB4: B2-interface: D; D-interface: D\n\n#### 3.22\n&emsp;&emsp;集线器主要的问题是冲突问题,任何主机发送的包都会在集线器处冲突.\n\n#### 3.26\n&emsp;&emsp;因为总线速度低于内存带宽,所以总线速度是瓶颈,因为每个包要经过总线两次,所以传输效率应为一半$800Mbps/2=400Mbps$,一共能处理$400Mbps/100Mbps=4$个接口.\n\n#### 3.29\n&emsp;&emsp;a) 输入队列满的时候会丢失这样的分组\n\n&emsp;&emsp;b) 这叫做*head-of line blocking*(排头阻塞)\n\n&emsp;&emsp;c)在输出队列中配置缓存,这样只有在输出队列满的时候才会丢失分组.\n\n#### 3.33\n&emsp;&emsp;有一种方法叫做\"unnumbered point-to-point link\",用于发送的端口不和其他网络中其他主机通信,其他主机也无法访问到该端口,而这个端口只能发送数据.参考RFC1812,section2.2.7.\n\n#### 3.34\n&emsp;&emsp;ipv4的offset字段为13位,而ip包的长度为$2^{16}$,因此分组的偏移应为$2^{16}/2^{13}=8$\n\n#### 3.46\na)\n||A|B|C|D|E|F\n:---: | :---: | :---: | :---: | :---:| :---: | :---: | :---:\nA|0|$\\infty$|3|8|$\\infty$|$\\infty$\nB|$\\infty$|0|$\\infty$|$\\infty$|2|$\\infty$|\nC|3|$\\infty$|0|$\\infty$|1|6|\nD|8|$\\infty$|$\\infty$|0|2|$\\infty$|\nE|$\\infty$|2|1|2|0|$\\infty$|\nF|$\\infty$|$\\infty$|6|$\\infty$|$\\infty$|0\n\nb)\n||A|B|C|D|E|F\n:---: | :---: | :---: | :---: | :---:| :---: | :---: | :---:\nA|0|$\\infty$|3|8|4|9\nB|$\\infty$|0|3|4|2|$\\infty$\nC|3|3|0|3|1|6\nD|8|4|3|0|2|$\\infty$\nE|4|2|1|2|0|7\nF|9|$\\infty$|6|$\\infty$|7|0\n\nc)\n||A|B|C|D|E|F\n:---: | :---: | :---: | :---: | :---:| :---: | :---: | :---:\nA|0|6|3|8|4|9\nB|6|0|3|4|2|9\nC|3|3|0|3|1|6\nD|8|4|3|0|2|9\nE|4|2|1|2|0|7\nF|9|9|6|9|7|0\n\n#### 3.48\nConfirmed|Tentative\n:---: | :---: \n(D,0,-)|\n(D,0,-)|(A,8,A)\n||(E,2,E)\n(D,0,-), (E,2,E)|(B,4,E)\n||(C,3,E)\n(D,0,-), (E,2,E), (C,3,E)|(A,6,E)\n||(F,9,E)\n||(B,4,E)\n(D,0,-), (E,2,E), (C,3,E), (B,4,E)|(A,6,E)\n||(F,9,E)\n(D,0,-), (E,2,E), (C,3,E), (B,4,E), (A,6,E)| (F,9,E)\n(D,0,-), (E,2,E), (C,3,E), (B,4,E), (A,6,E),(F,9,E)|\n\n#### 3.55\na)128.96.39.10和子网号128.96.39.0匹配,数据包会转发到接口0.\n\nb)同理,128.96.40.12地址的数据包转发给R2.\n\nc)因为子网掩码是255.255.255.128,所以对于目标地址128.96.40.151的子网号是128.96.40.128,可见路由表中没有这个子网号,所以转发到默认的R4.\n\n#### 3.68\na)根据四个子网的主机数量,A需要128个地址,B需要64个地址,C和D需要32个地址.\n则可以设计:\n\nA的子网号为12.1.1.0/25\n\nB的子网号为12.1.1.128/26\n\nC的子网号为12.1.1.192/27\n\nD的子网号为12.1.1.224/27.\n\nb)以上的设计已经可以容纳32的主机数量,如果再增加主机可以通过网桥连接,或者是重新划分子网的方式.","source":"_posts/Network-chapter3.md","raw":"---\ntitle: Network-chapter3\ndate: 2019-08-24 13:43:33\ncategories: Computer Network A Systems Approach\nvisitors: \nmathjax: true\ntags: Network\n---\n章节三的习题:\n\n#### 3.2\n\nExercise | Switch | Input | Output\n:---: | :---: | :---: | :---:\n| | |{Port, VCI} | {Port, VCI}\na) | 1 | 0&emsp;0 | 1&emsp;0\n|| 2 | 3&emsp;0 | 1&emsp;0\n|| 4 | 3&emsp;0 | 0&emsp;0\nb) | 3 | 3&emsp;0 | 0&emsp;0\n|| 2 | 0&emsp;0 | 1&emsp;1\n|| 4 | 3&emsp;1 | 1&emsp;0\nc) | 4 | 2&emsp;0 | 3&emsp;2\n|| 2 | 1&emsp;2 | 3&emsp;1\n|| 1 | 1&emsp;1 | 2&emsp;0\nd) | 4 | 0&emsp;1 | 3&emsp;3\n|| 2 | 1&emsp;3 | 3&emsp;2\n|| 1 | 1&emsp;2 | 3&emsp;0\ne) | 3 | 2&emsp;0 | 0&emsp;1\n|| 1 | 0&emsp;1 | 2&emsp;0\nf) | 4 | 0&emsp;2 | 3&emsp;4\n|| 2 | 1&emsp;4 | 0&emsp;2\n|| 3 | 0&emsp;2 | 1&emsp;0\n\n#### 3.3\n\nSwitch | Destination | Next hip\n:---: | :---: | :---:\nA | B | C\n|| C | C\n|| D | C\n|| E | C\n|| F | C\nB | A | E\n|| C | E\n|| D | E \n|| E | E\n|| F | E\nC | A | A\n|| B | E\n|| D | E\n|| E | E\n|| F | F\nD | A | E\n|| B | E\n|| C | E\n|| E | E\n|| F | E\nE | A | C\n|| B | B\n|| C | C\n||D|D\n||F|C\nF|A|C\n||B|C\n||C|C\n||D|C\n||E|C\n\n#### 3.5\n\n&emsp;&emsp;一共有三条VC\n\n1)从A到B,使用S1的1和2端口.\n\n2)从A到D,使用S1的1和3端口,S2的1和3端口,S3的1和2端口.\n\n3)从B到D,使用S1的2和3端口,S2的1和3端口,S3的1和2端口.\n\n#### 3.13\n\n&emsp;&emsp;网桥和端口之间的映射关系为\nBridge | Port\n:---: | :---:\nB1|C&emsp;E\nB2|A&emsp;B\nB3|D&emsp;E&emsp;F&emsp;G&emsp;H\nB4|G&emsp;I\nB5|Idle\nB6|J&emsp;H\nB7|A&emsp;C\n\n#### 3.15\nB1: A-interface: A; B2-interface: C\nB2: B1-interface: A; B3-interface: C; B4-interface: D\nB3: B2-interface: A,D; C-interface: C\nB4: B2-interface: D; D-interface: D\n\n#### 3.22\n&emsp;&emsp;集线器主要的问题是冲突问题,任何主机发送的包都会在集线器处冲突.\n\n#### 3.26\n&emsp;&emsp;因为总线速度低于内存带宽,所以总线速度是瓶颈,因为每个包要经过总线两次,所以传输效率应为一半$800Mbps/2=400Mbps$,一共能处理$400Mbps/100Mbps=4$个接口.\n\n#### 3.29\n&emsp;&emsp;a) 输入队列满的时候会丢失这样的分组\n\n&emsp;&emsp;b) 这叫做*head-of line blocking*(排头阻塞)\n\n&emsp;&emsp;c)在输出队列中配置缓存,这样只有在输出队列满的时候才会丢失分组.\n\n#### 3.33\n&emsp;&emsp;有一种方法叫做\"unnumbered point-to-point link\",用于发送的端口不和其他网络中其他主机通信,其他主机也无法访问到该端口,而这个端口只能发送数据.参考RFC1812,section2.2.7.\n\n#### 3.34\n&emsp;&emsp;ipv4的offset字段为13位,而ip包的长度为$2^{16}$,因此分组的偏移应为$2^{16}/2^{13}=8$\n\n#### 3.46\na)\n||A|B|C|D|E|F\n:---: | :---: | :---: | :---: | :---:| :---: | :---: | :---:\nA|0|$\\infty$|3|8|$\\infty$|$\\infty$\nB|$\\infty$|0|$\\infty$|$\\infty$|2|$\\infty$|\nC|3|$\\infty$|0|$\\infty$|1|6|\nD|8|$\\infty$|$\\infty$|0|2|$\\infty$|\nE|$\\infty$|2|1|2|0|$\\infty$|\nF|$\\infty$|$\\infty$|6|$\\infty$|$\\infty$|0\n\nb)\n||A|B|C|D|E|F\n:---: | :---: | :---: | :---: | :---:| :---: | :---: | :---:\nA|0|$\\infty$|3|8|4|9\nB|$\\infty$|0|3|4|2|$\\infty$\nC|3|3|0|3|1|6\nD|8|4|3|0|2|$\\infty$\nE|4|2|1|2|0|7\nF|9|$\\infty$|6|$\\infty$|7|0\n\nc)\n||A|B|C|D|E|F\n:---: | :---: | :---: | :---: | :---:| :---: | :---: | :---:\nA|0|6|3|8|4|9\nB|6|0|3|4|2|9\nC|3|3|0|3|1|6\nD|8|4|3|0|2|9\nE|4|2|1|2|0|7\nF|9|9|6|9|7|0\n\n#### 3.48\nConfirmed|Tentative\n:---: | :---: \n(D,0,-)|\n(D,0,-)|(A,8,A)\n||(E,2,E)\n(D,0,-), (E,2,E)|(B,4,E)\n||(C,3,E)\n(D,0,-), (E,2,E), (C,3,E)|(A,6,E)\n||(F,9,E)\n||(B,4,E)\n(D,0,-), (E,2,E), (C,3,E), (B,4,E)|(A,6,E)\n||(F,9,E)\n(D,0,-), (E,2,E), (C,3,E), (B,4,E), (A,6,E)| (F,9,E)\n(D,0,-), (E,2,E), (C,3,E), (B,4,E), (A,6,E),(F,9,E)|\n\n#### 3.55\na)128.96.39.10和子网号128.96.39.0匹配,数据包会转发到接口0.\n\nb)同理,128.96.40.12地址的数据包转发给R2.\n\nc)因为子网掩码是255.255.255.128,所以对于目标地址128.96.40.151的子网号是128.96.40.128,可见路由表中没有这个子网号,所以转发到默认的R4.\n\n#### 3.68\na)根据四个子网的主机数量,A需要128个地址,B需要64个地址,C和D需要32个地址.\n则可以设计:\n\nA的子网号为12.1.1.0/25\n\nB的子网号为12.1.1.128/26\n\nC的子网号为12.1.1.192/27\n\nD的子网号为12.1.1.224/27.\n\nb)以上的设计已经可以容纳32的主机数量,如果再增加主机可以通过网桥连接,或者是重新划分子网的方式.","slug":"Network-chapter3","published":1,"updated":"2019-08-30T02:49:58.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nsz0019z4o0qw49t1wh","content":"<p>章节三的习题:</p>\n<h4 id=\"3-2\"><a href=\"#3-2\" class=\"headerlink\" title=\"3.2\"></a>3.2</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Exercise</th>\n<th style=\"text-align:center\">Switch</th>\n<th style=\"text-align:center\">Input</th>\n<th style=\"text-align:center\">Output</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">{Port, VCI}</td>\n<td>{Port, VCI}</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">a)</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0&emsp;0</td>\n<td style=\"text-align:center\">1&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">3&emsp;0</td>\n<td>1&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">3&emsp;0</td>\n<td>0&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">b)</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">3&emsp;0</td>\n<td style=\"text-align:center\">0&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0&emsp;0</td>\n<td>1&emsp;1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">3&emsp;1</td>\n<td>1&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">c)</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">2&emsp;0</td>\n<td style=\"text-align:center\">3&emsp;2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1&emsp;2</td>\n<td>3&emsp;1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1&emsp;1</td>\n<td>2&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">d)</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0&emsp;1</td>\n<td style=\"text-align:center\">3&emsp;3</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1&emsp;3</td>\n<td>3&emsp;2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1&emsp;2</td>\n<td>3&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">e)</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">2&emsp;0</td>\n<td style=\"text-align:center\">0&emsp;1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0&emsp;1</td>\n<td>2&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">f)</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0&emsp;2</td>\n<td style=\"text-align:center\">3&emsp;4</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1&emsp;4</td>\n<td>0&emsp;2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0&emsp;2</td>\n<td>1&emsp;0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"3-3\"><a href=\"#3-3\" class=\"headerlink\" title=\"3.3\"></a>3.3</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Switch</th>\n<th style=\"text-align:center\">Destination</th>\n<th style=\"text-align:center\">Next hip</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">B</td>\n<td style=\"text-align:center\">C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">B</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>E </td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">C</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">A</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">B</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>F</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">D</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">B</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">E</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">B</td>\n<td>B</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>D</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">F</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">B</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>C</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"3-5\"><a href=\"#3-5\" class=\"headerlink\" title=\"3.5\"></a>3.5</h4><p>&emsp;&emsp;一共有三条VC</p>\n<p>1)从A到B,使用S1的1和2端口.</p>\n<p>2)从A到D,使用S1的1和3端口,S2的1和3端口,S3的1和2端口.</p>\n<p>3)从B到D,使用S1的2和3端口,S2的1和3端口,S3的1和2端口.</p>\n<h4 id=\"3-13\"><a href=\"#3-13\" class=\"headerlink\" title=\"3.13\"></a>3.13</h4><p>&emsp;&emsp;网桥和端口之间的映射关系为<br>Bridge | Port<br>:—-: | :—-:<br>B1|C&emsp;E<br>B2|A&emsp;B<br>B3|D&emsp;E&emsp;F&emsp;G&emsp;H<br>B4|G&emsp;I<br>B5|Idle<br>B6|J&emsp;H<br>B7|A&emsp;C</p>\n<h4 id=\"3-15\"><a href=\"#3-15\" class=\"headerlink\" title=\"3.15\"></a>3.15</h4><p>B1: A-interface: A; B2-interface: C<br>B2: B1-interface: A; B3-interface: C; B4-interface: D<br>B3: B2-interface: A,D; C-interface: C<br>B4: B2-interface: D; D-interface: D</p>\n<h4 id=\"3-22\"><a href=\"#3-22\" class=\"headerlink\" title=\"3.22\"></a>3.22</h4><p>&emsp;&emsp;集线器主要的问题是冲突问题,任何主机发送的包都会在集线器处冲突.</p>\n<h4 id=\"3-26\"><a href=\"#3-26\" class=\"headerlink\" title=\"3.26\"></a>3.26</h4><p>&emsp;&emsp;因为总线速度低于内存带宽,所以总线速度是瓶颈,因为每个包要经过总线两次,所以传输效率应为一半$800Mbps/2=400Mbps$,一共能处理$400Mbps/100Mbps=4$个接口.</p>\n<h4 id=\"3-29\"><a href=\"#3-29\" class=\"headerlink\" title=\"3.29\"></a>3.29</h4><p>&emsp;&emsp;a) 输入队列满的时候会丢失这样的分组</p>\n<p>&emsp;&emsp;b) 这叫做<em>head-of line blocking</em>(排头阻塞)</p>\n<p>&emsp;&emsp;c)在输出队列中配置缓存,这样只有在输出队列满的时候才会丢失分组.</p>\n<h4 id=\"3-33\"><a href=\"#3-33\" class=\"headerlink\" title=\"3.33\"></a>3.33</h4><p>&emsp;&emsp;有一种方法叫做”unnumbered point-to-point link”,用于发送的端口不和其他网络中其他主机通信,其他主机也无法访问到该端口,而这个端口只能发送数据.参考RFC1812,section2.2.7.</p>\n<h4 id=\"3-34\"><a href=\"#3-34\" class=\"headerlink\" title=\"3.34\"></a>3.34</h4><p>&emsp;&emsp;ipv4的offset字段为13位,而ip包的长度为$2^{16}$,因此分组的偏移应为$2^{16}/2^{13}=8$</p>\n<h4 id=\"3-46\"><a href=\"#3-46\" class=\"headerlink\" title=\"3.46\"></a>3.46</h4><p>a)<br>||A|B|C|D|E|F<br>:—-: | :—-: | :—-: | :—-: | :—-:| :—-: | :—-: | :—-:<br>A|0|$\\infty$|3|8|$\\infty$|$\\infty$<br>B|$\\infty$|0|$\\infty$|$\\infty$|2|$\\infty$|<br>C|3|$\\infty$|0|$\\infty$|1|6|<br>D|8|$\\infty$|$\\infty$|0|2|$\\infty$|<br>E|$\\infty$|2|1|2|0|$\\infty$|<br>F|$\\infty$|$\\infty$|6|$\\infty$|$\\infty$|0</p>\n<p>b)<br>||A|B|C|D|E|F<br>:—-: | :—-: | :—-: | :—-: | :—-:| :—-: | :—-: | :—-:<br>A|0|$\\infty$|3|8|4|9<br>B|$\\infty$|0|3|4|2|$\\infty$<br>C|3|3|0|3|1|6<br>D|8|4|3|0|2|$\\infty$<br>E|4|2|1|2|0|7<br>F|9|$\\infty$|6|$\\infty$|7|0</p>\n<p>c)<br>||A|B|C|D|E|F<br>:—-: | :—-: | :—-: | :—-: | :—-:| :—-: | :—-: | :—-:<br>A|0|6|3|8|4|9<br>B|6|0|3|4|2|9<br>C|3|3|0|3|1|6<br>D|8|4|3|0|2|9<br>E|4|2|1|2|0|7<br>F|9|9|6|9|7|0</p>\n<h4 id=\"3-48\"><a href=\"#3-48\" class=\"headerlink\" title=\"3.48\"></a>3.48</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Confirmed</th>\n<th style=\"text-align:center\">Tentative</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">(D,0,-)</td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-)</td>\n<td style=\"text-align:center\">(A,8,A)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(E,2,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E)</td>\n<td style=\"text-align:center\">(B,4,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(C,3,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E), (C,3,E)</td>\n<td style=\"text-align:center\">(A,6,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(F,9,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(B,4,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E), (C,3,E), (B,4,E)</td>\n<td style=\"text-align:center\">(A,6,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(F,9,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E), (C,3,E), (B,4,E), (A,6,E)</td>\n<td style=\"text-align:center\">(F,9,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E), (C,3,E), (B,4,E), (A,6,E),(F,9,E)</td>\n<td style=\"text-align:center\"></td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"3-55\"><a href=\"#3-55\" class=\"headerlink\" title=\"3.55\"></a>3.55</h4><p>a)128.96.39.10和子网号128.96.39.0匹配,数据包会转发到接口0.</p>\n<p>b)同理,128.96.40.12地址的数据包转发给R2.</p>\n<p>c)因为子网掩码是255.255.255.128,所以对于目标地址128.96.40.151的子网号是128.96.40.128,可见路由表中没有这个子网号,所以转发到默认的R4.</p>\n<h4 id=\"3-68\"><a href=\"#3-68\" class=\"headerlink\" title=\"3.68\"></a>3.68</h4><p>a)根据四个子网的主机数量,A需要128个地址,B需要64个地址,C和D需要32个地址.<br>则可以设计:</p>\n<p>A的子网号为12.1.1.0/25</p>\n<p>B的子网号为12.1.1.128/26</p>\n<p>C的子网号为12.1.1.192/27</p>\n<p>D的子网号为12.1.1.224/27.</p>\n<p>b)以上的设计已经可以容纳32的主机数量,如果再增加主机可以通过网桥连接,或者是重新划分子网的方式.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>章节三的习题:</p>\n<h4 id=\"3-2\"><a href=\"#3-2\" class=\"headerlink\" title=\"3.2\"></a>3.2</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Exercise</th>\n<th style=\"text-align:center\">Switch</th>\n<th style=\"text-align:center\">Input</th>\n<th style=\"text-align:center\">Output</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">{Port, VCI}</td>\n<td>{Port, VCI}</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">a)</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0&emsp;0</td>\n<td style=\"text-align:center\">1&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">3&emsp;0</td>\n<td>1&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">3&emsp;0</td>\n<td>0&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">b)</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">3&emsp;0</td>\n<td style=\"text-align:center\">0&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0&emsp;0</td>\n<td>1&emsp;1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">3&emsp;1</td>\n<td>1&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">c)</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">2&emsp;0</td>\n<td style=\"text-align:center\">3&emsp;2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1&emsp;2</td>\n<td>3&emsp;1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1&emsp;1</td>\n<td>2&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">d)</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0&emsp;1</td>\n<td style=\"text-align:center\">3&emsp;3</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1&emsp;3</td>\n<td>3&emsp;2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1&emsp;2</td>\n<td>3&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">e)</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">2&emsp;0</td>\n<td style=\"text-align:center\">0&emsp;1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0&emsp;1</td>\n<td>2&emsp;0</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">f)</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0&emsp;2</td>\n<td style=\"text-align:center\">3&emsp;4</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1&emsp;4</td>\n<td>0&emsp;2</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0&emsp;2</td>\n<td>1&emsp;0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"3-3\"><a href=\"#3-3\" class=\"headerlink\" title=\"3.3\"></a>3.3</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Switch</th>\n<th style=\"text-align:center\">Destination</th>\n<th style=\"text-align:center\">Next hip</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">B</td>\n<td style=\"text-align:center\">C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">B</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>E </td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">C</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">A</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">B</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>F</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">D</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">B</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>E</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">E</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">B</td>\n<td>B</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>D</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">F</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">F</td>\n<td style=\"text-align:center\">A</td>\n<td style=\"text-align:center\">C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">B</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">C</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">D</td>\n<td>C</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">E</td>\n<td>C</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"3-5\"><a href=\"#3-5\" class=\"headerlink\" title=\"3.5\"></a>3.5</h4><p>&emsp;&emsp;一共有三条VC</p>\n<p>1)从A到B,使用S1的1和2端口.</p>\n<p>2)从A到D,使用S1的1和3端口,S2的1和3端口,S3的1和2端口.</p>\n<p>3)从B到D,使用S1的2和3端口,S2的1和3端口,S3的1和2端口.</p>\n<h4 id=\"3-13\"><a href=\"#3-13\" class=\"headerlink\" title=\"3.13\"></a>3.13</h4><p>&emsp;&emsp;网桥和端口之间的映射关系为<br>Bridge | Port<br>:—-: | :—-:<br>B1|C&emsp;E<br>B2|A&emsp;B<br>B3|D&emsp;E&emsp;F&emsp;G&emsp;H<br>B4|G&emsp;I<br>B5|Idle<br>B6|J&emsp;H<br>B7|A&emsp;C</p>\n<h4 id=\"3-15\"><a href=\"#3-15\" class=\"headerlink\" title=\"3.15\"></a>3.15</h4><p>B1: A-interface: A; B2-interface: C<br>B2: B1-interface: A; B3-interface: C; B4-interface: D<br>B3: B2-interface: A,D; C-interface: C<br>B4: B2-interface: D; D-interface: D</p>\n<h4 id=\"3-22\"><a href=\"#3-22\" class=\"headerlink\" title=\"3.22\"></a>3.22</h4><p>&emsp;&emsp;集线器主要的问题是冲突问题,任何主机发送的包都会在集线器处冲突.</p>\n<h4 id=\"3-26\"><a href=\"#3-26\" class=\"headerlink\" title=\"3.26\"></a>3.26</h4><p>&emsp;&emsp;因为总线速度低于内存带宽,所以总线速度是瓶颈,因为每个包要经过总线两次,所以传输效率应为一半$800Mbps/2=400Mbps$,一共能处理$400Mbps/100Mbps=4$个接口.</p>\n<h4 id=\"3-29\"><a href=\"#3-29\" class=\"headerlink\" title=\"3.29\"></a>3.29</h4><p>&emsp;&emsp;a) 输入队列满的时候会丢失这样的分组</p>\n<p>&emsp;&emsp;b) 这叫做<em>head-of line blocking</em>(排头阻塞)</p>\n<p>&emsp;&emsp;c)在输出队列中配置缓存,这样只有在输出队列满的时候才会丢失分组.</p>\n<h4 id=\"3-33\"><a href=\"#3-33\" class=\"headerlink\" title=\"3.33\"></a>3.33</h4><p>&emsp;&emsp;有一种方法叫做”unnumbered point-to-point link”,用于发送的端口不和其他网络中其他主机通信,其他主机也无法访问到该端口,而这个端口只能发送数据.参考RFC1812,section2.2.7.</p>\n<h4 id=\"3-34\"><a href=\"#3-34\" class=\"headerlink\" title=\"3.34\"></a>3.34</h4><p>&emsp;&emsp;ipv4的offset字段为13位,而ip包的长度为$2^{16}$,因此分组的偏移应为$2^{16}/2^{13}=8$</p>\n<h4 id=\"3-46\"><a href=\"#3-46\" class=\"headerlink\" title=\"3.46\"></a>3.46</h4><p>a)<br>||A|B|C|D|E|F<br>:—-: | :—-: | :—-: | :—-: | :—-:| :—-: | :—-: | :—-:<br>A|0|$\\infty$|3|8|$\\infty$|$\\infty$<br>B|$\\infty$|0|$\\infty$|$\\infty$|2|$\\infty$|<br>C|3|$\\infty$|0|$\\infty$|1|6|<br>D|8|$\\infty$|$\\infty$|0|2|$\\infty$|<br>E|$\\infty$|2|1|2|0|$\\infty$|<br>F|$\\infty$|$\\infty$|6|$\\infty$|$\\infty$|0</p>\n<p>b)<br>||A|B|C|D|E|F<br>:—-: | :—-: | :—-: | :—-: | :—-:| :—-: | :—-: | :—-:<br>A|0|$\\infty$|3|8|4|9<br>B|$\\infty$|0|3|4|2|$\\infty$<br>C|3|3|0|3|1|6<br>D|8|4|3|0|2|$\\infty$<br>E|4|2|1|2|0|7<br>F|9|$\\infty$|6|$\\infty$|7|0</p>\n<p>c)<br>||A|B|C|D|E|F<br>:—-: | :—-: | :—-: | :—-: | :—-:| :—-: | :—-: | :—-:<br>A|0|6|3|8|4|9<br>B|6|0|3|4|2|9<br>C|3|3|0|3|1|6<br>D|8|4|3|0|2|9<br>E|4|2|1|2|0|7<br>F|9|9|6|9|7|0</p>\n<h4 id=\"3-48\"><a href=\"#3-48\" class=\"headerlink\" title=\"3.48\"></a>3.48</h4><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Confirmed</th>\n<th style=\"text-align:center\">Tentative</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">(D,0,-)</td>\n<td style=\"text-align:center\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-)</td>\n<td style=\"text-align:center\">(A,8,A)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(E,2,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E)</td>\n<td style=\"text-align:center\">(B,4,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(C,3,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E), (C,3,E)</td>\n<td style=\"text-align:center\">(A,6,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(F,9,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(B,4,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E), (C,3,E), (B,4,E)</td>\n<td style=\"text-align:center\">(A,6,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td>(F,9,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E), (C,3,E), (B,4,E), (A,6,E)</td>\n<td style=\"text-align:center\">(F,9,E)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(D,0,-), (E,2,E), (C,3,E), (B,4,E), (A,6,E),(F,9,E)</td>\n<td style=\"text-align:center\"></td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"3-55\"><a href=\"#3-55\" class=\"headerlink\" title=\"3.55\"></a>3.55</h4><p>a)128.96.39.10和子网号128.96.39.0匹配,数据包会转发到接口0.</p>\n<p>b)同理,128.96.40.12地址的数据包转发给R2.</p>\n<p>c)因为子网掩码是255.255.255.128,所以对于目标地址128.96.40.151的子网号是128.96.40.128,可见路由表中没有这个子网号,所以转发到默认的R4.</p>\n<h4 id=\"3-68\"><a href=\"#3-68\" class=\"headerlink\" title=\"3.68\"></a>3.68</h4><p>a)根据四个子网的主机数量,A需要128个地址,B需要64个地址,C和D需要32个地址.<br>则可以设计:</p>\n<p>A的子网号为12.1.1.0/25</p>\n<p>B的子网号为12.1.1.128/26</p>\n<p>C的子网号为12.1.1.192/27</p>\n<p>D的子网号为12.1.1.224/27.</p>\n<p>b)以上的设计已经可以容纳32的主机数量,如果再增加主机可以通过网桥连接,或者是重新划分子网的方式.</p>\n"},{"title":"Network-chapter5","date":"2019-08-31T01:19:03.000Z","visitors":null,"mathjax":true,"_content":"章节五的习题:\n\n#### 5.2\na)可以考虑这样的情况:客户端想要请求文件a,但是发出了请求文件b的数据包.随即撤回,并发送新的数据包请求文件a.但是请求数据包丢失了.最后服务器端只收到了请求文件b的数据包,并给客户端返回了文件b.\n\nb)每次发送请求使用不同的端口号;每个数据包带上序号或者是时间戳,这样错序到达的数据包可以丢弃.\n\n#### 5.4\n&emsp;&emsp;在主机A发送给主机B数据包(Flags=FIN)后,会转移到状态FIN_WAIT_1,然后主机A会收到主机B回复的数据包(Flags=ACK+FIN).这种情况会发生在主机B在收到主机A发送的(Flags=FIN)后,立即关闭连接,并发送返回自己的FIN和从主机A收到的ACK.\n\n&emsp;&emsp;这种情况不常见,因为一般情况是主机B一般先返回ACK,之后再返回FIN.也就是说对于主机A,状态转移的顺序是:\n$ESTABLISHED\\rightarrow FIN\\_WAIT\\_1\\rightarrow FIN\\_WAIT\\_2\\rightarrow TIME\\_WAIT\\rightarrow CLOSED$\n对于主机B,状态转移的顺序是:\n$ESTABLISHED\\rightarrow CLOSE\\_WAIT\\rightarrow TIME\\_WAIT\\rightarrow CLOSED$\n\n#### 5.9\n&emsp;&emsp;已知延迟带宽积为$100*10^{-3}s*1*10^9bps=10^8b=12.5MB$,因此接收端的窗口大小为24(因为$2^{24}=1.67*10^7$,并且报文段最小为1字节).\n\n&emsp;&emsp;SequenceNum用来防止在最大段生存期内回绕.在段最大生存期能够发送$30s*1Gbps=3.75GB$的数据,因此需要32位($2^{32}=4.3G$).\n\n#### 5.12\na)TCP的序号字段有32位,也就是在达到回绕之前能够发送$2^{32}B$数据,因此需要$2^{32}B/(10^9bps/8)=34s$\n\nb)在回绕时间内增量1000次,就是每34ms增量一次.\n\n#### 5.14\na)如果是重传,那么前后两个包的序号应该是一致的,如果是主机B崩溃重启,那么后发送的包序号和之前的不同.因为初始建立TCP连接时初始序号是随机选择的.\n\n#### 5.16\na)伪造的分组发送到主机A后,A会返回分组给B,但是由于ACK+1,B识别这是个无效的分组后,发送RST给A,重置TCP连接.","source":"_posts/Network-chapter5.md","raw":"---\ntitle: Network-chapter5\ndate: 2019-08-31 09:19:03\ncategories: Computer Network A Systems Approach\nvisitors: \nmathjax: true\ntags: Network\n---\n章节五的习题:\n\n#### 5.2\na)可以考虑这样的情况:客户端想要请求文件a,但是发出了请求文件b的数据包.随即撤回,并发送新的数据包请求文件a.但是请求数据包丢失了.最后服务器端只收到了请求文件b的数据包,并给客户端返回了文件b.\n\nb)每次发送请求使用不同的端口号;每个数据包带上序号或者是时间戳,这样错序到达的数据包可以丢弃.\n\n#### 5.4\n&emsp;&emsp;在主机A发送给主机B数据包(Flags=FIN)后,会转移到状态FIN_WAIT_1,然后主机A会收到主机B回复的数据包(Flags=ACK+FIN).这种情况会发生在主机B在收到主机A发送的(Flags=FIN)后,立即关闭连接,并发送返回自己的FIN和从主机A收到的ACK.\n\n&emsp;&emsp;这种情况不常见,因为一般情况是主机B一般先返回ACK,之后再返回FIN.也就是说对于主机A,状态转移的顺序是:\n$ESTABLISHED\\rightarrow FIN\\_WAIT\\_1\\rightarrow FIN\\_WAIT\\_2\\rightarrow TIME\\_WAIT\\rightarrow CLOSED$\n对于主机B,状态转移的顺序是:\n$ESTABLISHED\\rightarrow CLOSE\\_WAIT\\rightarrow TIME\\_WAIT\\rightarrow CLOSED$\n\n#### 5.9\n&emsp;&emsp;已知延迟带宽积为$100*10^{-3}s*1*10^9bps=10^8b=12.5MB$,因此接收端的窗口大小为24(因为$2^{24}=1.67*10^7$,并且报文段最小为1字节).\n\n&emsp;&emsp;SequenceNum用来防止在最大段生存期内回绕.在段最大生存期能够发送$30s*1Gbps=3.75GB$的数据,因此需要32位($2^{32}=4.3G$).\n\n#### 5.12\na)TCP的序号字段有32位,也就是在达到回绕之前能够发送$2^{32}B$数据,因此需要$2^{32}B/(10^9bps/8)=34s$\n\nb)在回绕时间内增量1000次,就是每34ms增量一次.\n\n#### 5.14\na)如果是重传,那么前后两个包的序号应该是一致的,如果是主机B崩溃重启,那么后发送的包序号和之前的不同.因为初始建立TCP连接时初始序号是随机选择的.\n\n#### 5.16\na)伪造的分组发送到主机A后,A会返回分组给B,但是由于ACK+1,B识别这是个无效的分组后,发送RST给A,重置TCP连接.","slug":"Network-chapter5","published":1,"updated":"2019-09-04T06:43:49.191Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt0001cz4o0y1tmaqjk","content":"<p>章节五的习题:</p>\n<h4 id=\"5-2\"><a href=\"#5-2\" class=\"headerlink\" title=\"5.2\"></a>5.2</h4><p>a)可以考虑这样的情况:客户端想要请求文件a,但是发出了请求文件b的数据包.随即撤回,并发送新的数据包请求文件a.但是请求数据包丢失了.最后服务器端只收到了请求文件b的数据包,并给客户端返回了文件b.</p>\n<p>b)每次发送请求使用不同的端口号;每个数据包带上序号或者是时间戳,这样错序到达的数据包可以丢弃.</p>\n<h4 id=\"5-4\"><a href=\"#5-4\" class=\"headerlink\" title=\"5.4\"></a>5.4</h4><p>&emsp;&emsp;在主机A发送给主机B数据包(Flags=FIN)后,会转移到状态FIN_WAIT_1,然后主机A会收到主机B回复的数据包(Flags=ACK+FIN).这种情况会发生在主机B在收到主机A发送的(Flags=FIN)后,立即关闭连接,并发送返回自己的FIN和从主机A收到的ACK.</p>\n<p>&emsp;&emsp;这种情况不常见,因为一般情况是主机B一般先返回ACK,之后再返回FIN.也就是说对于主机A,状态转移的顺序是:<br>$ESTABLISHED\\rightarrow FIN_WAIT_1\\rightarrow FIN_WAIT_2\\rightarrow TIME_WAIT\\rightarrow CLOSED$<br>对于主机B,状态转移的顺序是:<br>$ESTABLISHED\\rightarrow CLOSE_WAIT\\rightarrow TIME_WAIT\\rightarrow CLOSED$</p>\n<h4 id=\"5-9\"><a href=\"#5-9\" class=\"headerlink\" title=\"5.9\"></a>5.9</h4><p>&emsp;&emsp;已知延迟带宽积为$100<em>10^{-3}s</em>1<em>10^9bps=10^8b=12.5MB$,因此接收端的窗口大小为24(因为$2^{24}=1.67</em>10^7$,并且报文段最小为1字节).</p>\n<p>&emsp;&emsp;SequenceNum用来防止在最大段生存期内回绕.在段最大生存期能够发送$30s*1Gbps=3.75GB$的数据,因此需要32位($2^{32}=4.3G$).</p>\n<h4 id=\"5-12\"><a href=\"#5-12\" class=\"headerlink\" title=\"5.12\"></a>5.12</h4><p>a)TCP的序号字段有32位,也就是在达到回绕之前能够发送$2^{32}B$数据,因此需要$2^{32}B/(10^9bps/8)=34s$</p>\n<p>b)在回绕时间内增量1000次,就是每34ms增量一次.</p>\n<h4 id=\"5-14\"><a href=\"#5-14\" class=\"headerlink\" title=\"5.14\"></a>5.14</h4><p>a)如果是重传,那么前后两个包的序号应该是一致的,如果是主机B崩溃重启,那么后发送的包序号和之前的不同.因为初始建立TCP连接时初始序号是随机选择的.</p>\n<h4 id=\"5-16\"><a href=\"#5-16\" class=\"headerlink\" title=\"5.16\"></a>5.16</h4><p>a)伪造的分组发送到主机A后,A会返回分组给B,但是由于ACK+1,B识别这是个无效的分组后,发送RST给A,重置TCP连接.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>章节五的习题:</p>\n<h4 id=\"5-2\"><a href=\"#5-2\" class=\"headerlink\" title=\"5.2\"></a>5.2</h4><p>a)可以考虑这样的情况:客户端想要请求文件a,但是发出了请求文件b的数据包.随即撤回,并发送新的数据包请求文件a.但是请求数据包丢失了.最后服务器端只收到了请求文件b的数据包,并给客户端返回了文件b.</p>\n<p>b)每次发送请求使用不同的端口号;每个数据包带上序号或者是时间戳,这样错序到达的数据包可以丢弃.</p>\n<h4 id=\"5-4\"><a href=\"#5-4\" class=\"headerlink\" title=\"5.4\"></a>5.4</h4><p>&emsp;&emsp;在主机A发送给主机B数据包(Flags=FIN)后,会转移到状态FIN_WAIT_1,然后主机A会收到主机B回复的数据包(Flags=ACK+FIN).这种情况会发生在主机B在收到主机A发送的(Flags=FIN)后,立即关闭连接,并发送返回自己的FIN和从主机A收到的ACK.</p>\n<p>&emsp;&emsp;这种情况不常见,因为一般情况是主机B一般先返回ACK,之后再返回FIN.也就是说对于主机A,状态转移的顺序是:<br>$ESTABLISHED\\rightarrow FIN_WAIT_1\\rightarrow FIN_WAIT_2\\rightarrow TIME_WAIT\\rightarrow CLOSED$<br>对于主机B,状态转移的顺序是:<br>$ESTABLISHED\\rightarrow CLOSE_WAIT\\rightarrow TIME_WAIT\\rightarrow CLOSED$</p>\n<h4 id=\"5-9\"><a href=\"#5-9\" class=\"headerlink\" title=\"5.9\"></a>5.9</h4><p>&emsp;&emsp;已知延迟带宽积为$100<em>10^{-3}s</em>1<em>10^9bps=10^8b=12.5MB$,因此接收端的窗口大小为24(因为$2^{24}=1.67</em>10^7$,并且报文段最小为1字节).</p>\n<p>&emsp;&emsp;SequenceNum用来防止在最大段生存期内回绕.在段最大生存期能够发送$30s*1Gbps=3.75GB$的数据,因此需要32位($2^{32}=4.3G$).</p>\n<h4 id=\"5-12\"><a href=\"#5-12\" class=\"headerlink\" title=\"5.12\"></a>5.12</h4><p>a)TCP的序号字段有32位,也就是在达到回绕之前能够发送$2^{32}B$数据,因此需要$2^{32}B/(10^9bps/8)=34s$</p>\n<p>b)在回绕时间内增量1000次,就是每34ms增量一次.</p>\n<h4 id=\"5-14\"><a href=\"#5-14\" class=\"headerlink\" title=\"5.14\"></a>5.14</h4><p>a)如果是重传,那么前后两个包的序号应该是一致的,如果是主机B崩溃重启,那么后发送的包序号和之前的不同.因为初始建立TCP连接时初始序号是随机选择的.</p>\n<h4 id=\"5-16\"><a href=\"#5-16\" class=\"headerlink\" title=\"5.16\"></a>5.16</h4><p>a)伪造的分组发送到主机A后,A会返回分组给B,但是由于ACK+1,B识别这是个无效的分组后,发送RST给A,重置TCP连接.</p>\n"},{"title":"Perceptron","date":"2019-05-29T02:22:37.000Z","mathjax":true,"_content":"#### 感知机算法\n  \n感知机算法是一种二分类的线性分类模型,仅适用线性可分的二分类问题.\n\n---\n##### 算法步骤:\n1. 选取初值$\\omega_0,b_0$\n2. 在训练集中选取数据$(x_i,y_i)$\n3. 如果$y_i(\\omega\\cdot x_i+b)\\leq0$\n$$\n\\begin{aligned}\n    \\omega&\\leftarrow\\omega +\\eta y_ix_i \\\\\n    b&\\leftarrow b+\\eta y_i\n\\end{aligned}\n$$\n4. 重复算法,直至没有误分类点.\n\n##### 感知机算法的对偶形式:\n1. $\\alpha\\leftarrow0,b\\leftarrow0$\n2. 在训练集中选取数据$(x_i,y_i)$\n3. 如果$y_i(\\sum^N_{j=1}\\alpha_jy_jx_j\\cdot x_i+b)\\leq0$\n\n$$\n    \\begin{aligned}\n        \\alpha_i&\\leftarrow \\alpha_i +\\eta \\\\\n        b&\\leftarrow b+\\eta y_i\n    \\end{aligned}\n$$\n4. 重复算法,直至没有误分类点.\n\n对偶形式中用到了Gram矩阵,也就是对样本集做内积:\n$$G=[x_i\\cdot x_j]_{N\\times N}$$\n\n---\n感知机算法的缺点让这个算法的应用范围很窄,基本不会被实际用到.但是通过不同方向的改进,能够得到一些新的算法:比如因为感知机算法得到的解,依赖于初值的选择和迭代过程中选择误分类点的顺序,为了得到唯一超平面,需要引入约束条件,这就是线性SVM的思想；针对线性不可分的问题,通过多层感知机算法解决.","source":"_posts/Perceptron.md","raw":"---\ntitle: Perceptron\ndate: 2019-05-29 10:22:37\ncategories: 统计学习方法\ntags: machine learning\nmathjax: true\n---\n#### 感知机算法\n  \n感知机算法是一种二分类的线性分类模型,仅适用线性可分的二分类问题.\n\n---\n##### 算法步骤:\n1. 选取初值$\\omega_0,b_0$\n2. 在训练集中选取数据$(x_i,y_i)$\n3. 如果$y_i(\\omega\\cdot x_i+b)\\leq0$\n$$\n\\begin{aligned}\n    \\omega&\\leftarrow\\omega +\\eta y_ix_i \\\\\n    b&\\leftarrow b+\\eta y_i\n\\end{aligned}\n$$\n4. 重复算法,直至没有误分类点.\n\n##### 感知机算法的对偶形式:\n1. $\\alpha\\leftarrow0,b\\leftarrow0$\n2. 在训练集中选取数据$(x_i,y_i)$\n3. 如果$y_i(\\sum^N_{j=1}\\alpha_jy_jx_j\\cdot x_i+b)\\leq0$\n\n$$\n    \\begin{aligned}\n        \\alpha_i&\\leftarrow \\alpha_i +\\eta \\\\\n        b&\\leftarrow b+\\eta y_i\n    \\end{aligned}\n$$\n4. 重复算法,直至没有误分类点.\n\n对偶形式中用到了Gram矩阵,也就是对样本集做内积:\n$$G=[x_i\\cdot x_j]_{N\\times N}$$\n\n---\n感知机算法的缺点让这个算法的应用范围很窄,基本不会被实际用到.但是通过不同方向的改进,能够得到一些新的算法:比如因为感知机算法得到的解,依赖于初值的选择和迭代过程中选择误分类点的顺序,为了得到唯一超平面,需要引入约束条件,这就是线性SVM的思想；针对线性不可分的问题,通过多层感知机算法解决.","slug":"Perceptron","published":1,"updated":"2019-06-10T07:42:56.313Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt0001ez4o0d6vh16sp","content":"<h4 id=\"感知机算法\"><a href=\"#感知机算法\" class=\"headerlink\" title=\"感知机算法\"></a>感知机算法</h4><p>感知机算法是一种二分类的线性分类模型,仅适用线性可分的二分类问题.</p>\n<hr>\n<h5 id=\"算法步骤\"><a href=\"#算法步骤\" class=\"headerlink\" title=\"算法步骤:\"></a>算法步骤:</h5><ol>\n<li>选取初值$\\omega_0,b_0$</li>\n<li>在训练集中选取数据$(x_i,y_i)$</li>\n<li>如果$y_i(\\omega\\cdot x_i+b)\\leq0$<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n \\omega&\\leftarrow\\omega +\\eta y_ix_i \\\\\n b&\\leftarrow b+\\eta y_i\n\\end{aligned}</script></li>\n<li>重复算法,直至没有误分类点.</li>\n</ol>\n<h5 id=\"感知机算法的对偶形式\"><a href=\"#感知机算法的对偶形式\" class=\"headerlink\" title=\"感知机算法的对偶形式:\"></a>感知机算法的对偶形式:</h5><ol>\n<li>$\\alpha\\leftarrow0,b\\leftarrow0$</li>\n<li>在训练集中选取数据$(x_i,y_i)$</li>\n<li>如果$y_i(\\sum^N_{j=1}\\alpha_jy_jx_j\\cdot x_i+b)\\leq0$</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        \\alpha_i&\\leftarrow \\alpha_i +\\eta \\\\\n        b&\\leftarrow b+\\eta y_i\n    \\end{aligned}</script><ol>\n<li>重复算法,直至没有误分类点.</li>\n</ol>\n<p>对偶形式中用到了Gram矩阵,也就是对样本集做内积:</p>\n<script type=\"math/tex; mode=display\">G=[x_i\\cdot x_j]_{N\\times N}</script><hr>\n<p>感知机算法的缺点让这个算法的应用范围很窄,基本不会被实际用到.但是通过不同方向的改进,能够得到一些新的算法:比如因为感知机算法得到的解,依赖于初值的选择和迭代过程中选择误分类点的顺序,为了得到唯一超平面,需要引入约束条件,这就是线性SVM的思想；针对线性不可分的问题,通过多层感知机算法解决.</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"感知机算法\"><a href=\"#感知机算法\" class=\"headerlink\" title=\"感知机算法\"></a>感知机算法</h4><p>感知机算法是一种二分类的线性分类模型,仅适用线性可分的二分类问题.</p>\n<hr>\n<h5 id=\"算法步骤\"><a href=\"#算法步骤\" class=\"headerlink\" title=\"算法步骤:\"></a>算法步骤:</h5><ol>\n<li>选取初值$\\omega_0,b_0$</li>\n<li>在训练集中选取数据$(x_i,y_i)$</li>\n<li>如果$y_i(\\omega\\cdot x_i+b)\\leq0$<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n \\omega&\\leftarrow\\omega +\\eta y_ix_i \\\\\n b&\\leftarrow b+\\eta y_i\n\\end{aligned}</script></li>\n<li>重复算法,直至没有误分类点.</li>\n</ol>\n<h5 id=\"感知机算法的对偶形式\"><a href=\"#感知机算法的对偶形式\" class=\"headerlink\" title=\"感知机算法的对偶形式:\"></a>感知机算法的对偶形式:</h5><ol>\n<li>$\\alpha\\leftarrow0,b\\leftarrow0$</li>\n<li>在训练集中选取数据$(x_i,y_i)$</li>\n<li>如果$y_i(\\sum^N_{j=1}\\alpha_jy_jx_j\\cdot x_i+b)\\leq0$</li>\n</ol>\n<script type=\"math/tex; mode=display\">\n    \\begin{aligned}\n        \\alpha_i&\\leftarrow \\alpha_i +\\eta \\\\\n        b&\\leftarrow b+\\eta y_i\n    \\end{aligned}</script><ol>\n<li>重复算法,直至没有误分类点.</li>\n</ol>\n<p>对偶形式中用到了Gram矩阵,也就是对样本集做内积:</p>\n<script type=\"math/tex; mode=display\">G=[x_i\\cdot x_j]_{N\\times N}</script><hr>\n<p>感知机算法的缺点让这个算法的应用范围很窄,基本不会被实际用到.但是通过不同方向的改进,能够得到一些新的算法:比如因为感知机算法得到的解,依赖于初值的选择和迭代过程中选择误分类点的顺序,为了得到唯一超平面,需要引入约束条件,这就是线性SVM的思想；针对线性不可分的问题,通过多层感知机算法解决.</p>\n"},{"title":"Network-chapter2","date":"2019-07-24T01:45:43.000Z","visitors":null,"mathjax":true,"_content":"章节二的习题:\n\n#### 2.1\n\n&emsp;&emsp;几种编码方法的区分:\n\n&emsp;&emsp;**NRZ**:&emsp;低电平时为0,高电平时为1.对于连续的1或0,基线漂移和时钟同步会带来问题.接收方一般是用信号的平均值作为基线判断高低电平,如果连续出现1或0,基线会变化,接收方检测信号就会出现问题;另外接收方必须与发送方的时钟完全同步才能解析NRZ信号,这样布线成本会很高,之后的编码方法通过在信号内的跳变让接收方能够恢复时钟.\n\n&emsp;&emsp;**NRZI**:&emsp;信号为1时,进行跳变,信号为0时,不变.这种编码方式解决了连续的1的情况,但是没有解决连续的0的情况.\n\n&emsp;&emsp;**曼彻斯特编码**:&emsp;用NRZ和时钟进行异或操作.信号为0时由低向高跳变,信号为1时由高向低跳变.因此接收方也能够很好的恢复时钟.但是因为跳变频率(波特率)加倍,曼彻斯特编码的效率只有NRZ和NRZI的一半.\n\n&emsp;&emsp;**4B/5B编码**:&emsp;用5比特来编码4比特信号,然后使用NRZI编码传输.通过限制连续的0的数量(最多一个前导0,最多两个末尾0)来解决NRZI的问题,同时效率比曼彻斯特编码要高(80%).\n\n#### 2.2\n\n&emsp;&emsp;4B/5B编码为11100 01011 11110 10101\n\n#### 2.4\n\n&emsp;&emsp;HDLC中**比特填充**过程如下.发送方在5个连续的1后填充0(除了表示帧开始和结束的01111110),接收方在接收到5个1后分情况处理,如果后面跟着0,那么去掉0;如果后面是10,那么为帧特殊比特序列;如果后面是11,那么为无效帧,丢弃整个帧.\n\n&emsp;&emsp;因此比特序列为1101011111<font color=\"red\">0</font>01011111<font color=\"red\">0</font>101011111<font color=\"red\">0</font>110\n\n#### 2.7\n\n&emsp;&emsp;比特序列为01101011111<font color=\"red\">0</font>1010011111<font color=\"red\">1</font>10110<font color=\"red\">01111110</font>\n\n&emsp;&emsp;前一个0为应该移除的比特填充位,下一个1为传输错误造成的位,最后是表示帧结束的特殊比特序列.\n\n#### 2.11\n\n&emsp;&emsp;差错检测.最简单的二维奇偶校验.对每个7比特附加1比特的平衡1的个数,使1的个数为偶数.最后的检验字节对每个比特位进行奇偶校验.\n\n&emsp;&emsp;出现3比特错.可能的情况有三行各出现1个比特错;一行出现2比特错,另一行出现1比特错;一行出现3比特错.第一种情况在每个校验位和检验字节都能检测到.第二种情况在检验字节能够检测和检验位能够检测到.第三种情况同样可以在检验字节和出现差错的检验位检测到.\n\n&emsp;&emsp;二维奇偶检验能够检测所有的1,3比特错及大部分2,4比特错,例外的4比特错情况为,两行各出现2比特错,并且比特位一致,二维奇偶检验无法检测这种情况.例外的2比特错情况为,同一行出现2比特错,则在行校验中无法检查出,即使通过列校验知道出错的比特位在哪一列.\n\n#### 2.14\n\n&emsp;&emsp;对于两个非零数,在没有溢出的情况下,肯定不会为F0000,在溢出的情况下,需要加进位,则一定$>$F0001.\n\n#### 2.19\n\n&emsp;&emsp;CRC校验码的步骤,已知用于传输的多项式$C(x)$长为$k$.\n\n1)在消息的后面扩展$k$个0\n\n2)用得到的扩展后的消息除$C(x)$,得到余数.\n\n3)从扩展后的消息中减去余数(等于直接和k个0异或,即把k个0替换成余数)\n\n&emsp;&emsp;对消息1011 0010 0100 1011后面加8个0,然后除1000 00111,得余数1001 0011,最后应传输的信息为1011 0010 0100 1011 1001 0011\n\n#### 2.23\n\n&emsp;&emsp;a) 时延$=40km/(2*10^8m/s)=0.2ms$\n\n&emsp;&emsp;b) 超时值$=2*EstinateRTT=0.8ms$\n\n&emsp;&emsp;c) 因为网络可能会有拥塞等情况,仍然可能出现超时.\n\n#### 2.25\n\n&emsp;&emsp;$RTT=2*3*10^4km/(3*10^8m/s)=0.2s$\n\n&emsp;&emsp;时延带宽积为$10^6bps/(10^3*8b)*0.2s=25$帧.\n\n&emsp;&emsp;也就是在RWS=1的情况下,需要5比特作为序号.因为$MaxSeqNum\\geq SWS+1$. \n\n&emsp;&emsp;在RWS=SWS的情况下,需要6比特,因为$SWS<(MaxSeqNum+1)/2$.","source":"_posts/Network-chapter2.md","raw":"---\ntitle: Network-chapter2\ndate: 2019-07-24 09:45:43\ncategories: Computer Network A Systems Approach\nvisitors: \nmathjax: true\ntags: Network\n---\n章节二的习题:\n\n#### 2.1\n\n&emsp;&emsp;几种编码方法的区分:\n\n&emsp;&emsp;**NRZ**:&emsp;低电平时为0,高电平时为1.对于连续的1或0,基线漂移和时钟同步会带来问题.接收方一般是用信号的平均值作为基线判断高低电平,如果连续出现1或0,基线会变化,接收方检测信号就会出现问题;另外接收方必须与发送方的时钟完全同步才能解析NRZ信号,这样布线成本会很高,之后的编码方法通过在信号内的跳变让接收方能够恢复时钟.\n\n&emsp;&emsp;**NRZI**:&emsp;信号为1时,进行跳变,信号为0时,不变.这种编码方式解决了连续的1的情况,但是没有解决连续的0的情况.\n\n&emsp;&emsp;**曼彻斯特编码**:&emsp;用NRZ和时钟进行异或操作.信号为0时由低向高跳变,信号为1时由高向低跳变.因此接收方也能够很好的恢复时钟.但是因为跳变频率(波特率)加倍,曼彻斯特编码的效率只有NRZ和NRZI的一半.\n\n&emsp;&emsp;**4B/5B编码**:&emsp;用5比特来编码4比特信号,然后使用NRZI编码传输.通过限制连续的0的数量(最多一个前导0,最多两个末尾0)来解决NRZI的问题,同时效率比曼彻斯特编码要高(80%).\n\n#### 2.2\n\n&emsp;&emsp;4B/5B编码为11100 01011 11110 10101\n\n#### 2.4\n\n&emsp;&emsp;HDLC中**比特填充**过程如下.发送方在5个连续的1后填充0(除了表示帧开始和结束的01111110),接收方在接收到5个1后分情况处理,如果后面跟着0,那么去掉0;如果后面是10,那么为帧特殊比特序列;如果后面是11,那么为无效帧,丢弃整个帧.\n\n&emsp;&emsp;因此比特序列为1101011111<font color=\"red\">0</font>01011111<font color=\"red\">0</font>101011111<font color=\"red\">0</font>110\n\n#### 2.7\n\n&emsp;&emsp;比特序列为01101011111<font color=\"red\">0</font>1010011111<font color=\"red\">1</font>10110<font color=\"red\">01111110</font>\n\n&emsp;&emsp;前一个0为应该移除的比特填充位,下一个1为传输错误造成的位,最后是表示帧结束的特殊比特序列.\n\n#### 2.11\n\n&emsp;&emsp;差错检测.最简单的二维奇偶校验.对每个7比特附加1比特的平衡1的个数,使1的个数为偶数.最后的检验字节对每个比特位进行奇偶校验.\n\n&emsp;&emsp;出现3比特错.可能的情况有三行各出现1个比特错;一行出现2比特错,另一行出现1比特错;一行出现3比特错.第一种情况在每个校验位和检验字节都能检测到.第二种情况在检验字节能够检测和检验位能够检测到.第三种情况同样可以在检验字节和出现差错的检验位检测到.\n\n&emsp;&emsp;二维奇偶检验能够检测所有的1,3比特错及大部分2,4比特错,例外的4比特错情况为,两行各出现2比特错,并且比特位一致,二维奇偶检验无法检测这种情况.例外的2比特错情况为,同一行出现2比特错,则在行校验中无法检查出,即使通过列校验知道出错的比特位在哪一列.\n\n#### 2.14\n\n&emsp;&emsp;对于两个非零数,在没有溢出的情况下,肯定不会为F0000,在溢出的情况下,需要加进位,则一定$>$F0001.\n\n#### 2.19\n\n&emsp;&emsp;CRC校验码的步骤,已知用于传输的多项式$C(x)$长为$k$.\n\n1)在消息的后面扩展$k$个0\n\n2)用得到的扩展后的消息除$C(x)$,得到余数.\n\n3)从扩展后的消息中减去余数(等于直接和k个0异或,即把k个0替换成余数)\n\n&emsp;&emsp;对消息1011 0010 0100 1011后面加8个0,然后除1000 00111,得余数1001 0011,最后应传输的信息为1011 0010 0100 1011 1001 0011\n\n#### 2.23\n\n&emsp;&emsp;a) 时延$=40km/(2*10^8m/s)=0.2ms$\n\n&emsp;&emsp;b) 超时值$=2*EstinateRTT=0.8ms$\n\n&emsp;&emsp;c) 因为网络可能会有拥塞等情况,仍然可能出现超时.\n\n#### 2.25\n\n&emsp;&emsp;$RTT=2*3*10^4km/(3*10^8m/s)=0.2s$\n\n&emsp;&emsp;时延带宽积为$10^6bps/(10^3*8b)*0.2s=25$帧.\n\n&emsp;&emsp;也就是在RWS=1的情况下,需要5比特作为序号.因为$MaxSeqNum\\geq SWS+1$. \n\n&emsp;&emsp;在RWS=SWS的情况下,需要6比特,因为$SWS<(MaxSeqNum+1)/2$.","slug":"Network-chapter2","published":1,"updated":"2019-08-24T04:21:06.433Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt2001jz4o07m3atagd","content":"<p>章节二的习题:</p>\n<h4 id=\"2-1\"><a href=\"#2-1\" class=\"headerlink\" title=\"2.1\"></a>2.1</h4><p>&emsp;&emsp;几种编码方法的区分:</p>\n<p>&emsp;&emsp;<strong>NRZ</strong>:&emsp;低电平时为0,高电平时为1.对于连续的1或0,基线漂移和时钟同步会带来问题.接收方一般是用信号的平均值作为基线判断高低电平,如果连续出现1或0,基线会变化,接收方检测信号就会出现问题;另外接收方必须与发送方的时钟完全同步才能解析NRZ信号,这样布线成本会很高,之后的编码方法通过在信号内的跳变让接收方能够恢复时钟.</p>\n<p>&emsp;&emsp;<strong>NRZI</strong>:&emsp;信号为1时,进行跳变,信号为0时,不变.这种编码方式解决了连续的1的情况,但是没有解决连续的0的情况.</p>\n<p>&emsp;&emsp;<strong>曼彻斯特编码</strong>:&emsp;用NRZ和时钟进行异或操作.信号为0时由低向高跳变,信号为1时由高向低跳变.因此接收方也能够很好的恢复时钟.但是因为跳变频率(波特率)加倍,曼彻斯特编码的效率只有NRZ和NRZI的一半.</p>\n<p>&emsp;&emsp;<strong>4B/5B编码</strong>:&emsp;用5比特来编码4比特信号,然后使用NRZI编码传输.通过限制连续的0的数量(最多一个前导0,最多两个末尾0)来解决NRZI的问题,同时效率比曼彻斯特编码要高(80%).</p>\n<h4 id=\"2-2\"><a href=\"#2-2\" class=\"headerlink\" title=\"2.2\"></a>2.2</h4><p>&emsp;&emsp;4B/5B编码为11100 01011 11110 10101</p>\n<h4 id=\"2-4\"><a href=\"#2-4\" class=\"headerlink\" title=\"2.4\"></a>2.4</h4><p>&emsp;&emsp;HDLC中<strong>比特填充</strong>过程如下.发送方在5个连续的1后填充0(除了表示帧开始和结束的01111110),接收方在接收到5个1后分情况处理,如果后面跟着0,那么去掉0;如果后面是10,那么为帧特殊比特序列;如果后面是11,那么为无效帧,丢弃整个帧.</p>\n<p>&emsp;&emsp;因此比特序列为1101011111<font color=\"red\">0</font>01011111<font color=\"red\">0</font>101011111<font color=\"red\">0</font>110</p>\n<h4 id=\"2-7\"><a href=\"#2-7\" class=\"headerlink\" title=\"2.7\"></a>2.7</h4><p>&emsp;&emsp;比特序列为01101011111<font color=\"red\">0</font>1010011111<font color=\"red\">1</font>10110<font color=\"red\">01111110</font></p>\n<p>&emsp;&emsp;前一个0为应该移除的比特填充位,下一个1为传输错误造成的位,最后是表示帧结束的特殊比特序列.</p>\n<h4 id=\"2-11\"><a href=\"#2-11\" class=\"headerlink\" title=\"2.11\"></a>2.11</h4><p>&emsp;&emsp;差错检测.最简单的二维奇偶校验.对每个7比特附加1比特的平衡1的个数,使1的个数为偶数.最后的检验字节对每个比特位进行奇偶校验.</p>\n<p>&emsp;&emsp;出现3比特错.可能的情况有三行各出现1个比特错;一行出现2比特错,另一行出现1比特错;一行出现3比特错.第一种情况在每个校验位和检验字节都能检测到.第二种情况在检验字节能够检测和检验位能够检测到.第三种情况同样可以在检验字节和出现差错的检验位检测到.</p>\n<p>&emsp;&emsp;二维奇偶检验能够检测所有的1,3比特错及大部分2,4比特错,例外的4比特错情况为,两行各出现2比特错,并且比特位一致,二维奇偶检验无法检测这种情况.例外的2比特错情况为,同一行出现2比特错,则在行校验中无法检查出,即使通过列校验知道出错的比特位在哪一列.</p>\n<h4 id=\"2-14\"><a href=\"#2-14\" class=\"headerlink\" title=\"2.14\"></a>2.14</h4><p>&emsp;&emsp;对于两个非零数,在没有溢出的情况下,肯定不会为F0000,在溢出的情况下,需要加进位,则一定$&gt;$F0001.</p>\n<h4 id=\"2-19\"><a href=\"#2-19\" class=\"headerlink\" title=\"2.19\"></a>2.19</h4><p>&emsp;&emsp;CRC校验码的步骤,已知用于传输的多项式$C(x)$长为$k$.</p>\n<p>1)在消息的后面扩展$k$个0</p>\n<p>2)用得到的扩展后的消息除$C(x)$,得到余数.</p>\n<p>3)从扩展后的消息中减去余数(等于直接和k个0异或,即把k个0替换成余数)</p>\n<p>&emsp;&emsp;对消息1011 0010 0100 1011后面加8个0,然后除1000 00111,得余数1001 0011,最后应传输的信息为1011 0010 0100 1011 1001 0011</p>\n<h4 id=\"2-23\"><a href=\"#2-23\" class=\"headerlink\" title=\"2.23\"></a>2.23</h4><p>&emsp;&emsp;a) 时延$=40km/(2*10^8m/s)=0.2ms$</p>\n<p>&emsp;&emsp;b) 超时值$=2*EstinateRTT=0.8ms$</p>\n<p>&emsp;&emsp;c) 因为网络可能会有拥塞等情况,仍然可能出现超时.</p>\n<h4 id=\"2-25\"><a href=\"#2-25\" class=\"headerlink\" title=\"2.25\"></a>2.25</h4><p>&emsp;&emsp;$RTT=2<em>3</em>10^4km/(3*10^8m/s)=0.2s$</p>\n<p>&emsp;&emsp;时延带宽积为$10^6bps/(10^3<em>8b)</em>0.2s=25$帧.</p>\n<p>&emsp;&emsp;也就是在RWS=1的情况下,需要5比特作为序号.因为$MaxSeqNum\\geq SWS+1$. </p>\n<p>&emsp;&emsp;在RWS=SWS的情况下,需要6比特,因为$SWS&lt;(MaxSeqNum+1)/2$.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>章节二的习题:</p>\n<h4 id=\"2-1\"><a href=\"#2-1\" class=\"headerlink\" title=\"2.1\"></a>2.1</h4><p>&emsp;&emsp;几种编码方法的区分:</p>\n<p>&emsp;&emsp;<strong>NRZ</strong>:&emsp;低电平时为0,高电平时为1.对于连续的1或0,基线漂移和时钟同步会带来问题.接收方一般是用信号的平均值作为基线判断高低电平,如果连续出现1或0,基线会变化,接收方检测信号就会出现问题;另外接收方必须与发送方的时钟完全同步才能解析NRZ信号,这样布线成本会很高,之后的编码方法通过在信号内的跳变让接收方能够恢复时钟.</p>\n<p>&emsp;&emsp;<strong>NRZI</strong>:&emsp;信号为1时,进行跳变,信号为0时,不变.这种编码方式解决了连续的1的情况,但是没有解决连续的0的情况.</p>\n<p>&emsp;&emsp;<strong>曼彻斯特编码</strong>:&emsp;用NRZ和时钟进行异或操作.信号为0时由低向高跳变,信号为1时由高向低跳变.因此接收方也能够很好的恢复时钟.但是因为跳变频率(波特率)加倍,曼彻斯特编码的效率只有NRZ和NRZI的一半.</p>\n<p>&emsp;&emsp;<strong>4B/5B编码</strong>:&emsp;用5比特来编码4比特信号,然后使用NRZI编码传输.通过限制连续的0的数量(最多一个前导0,最多两个末尾0)来解决NRZI的问题,同时效率比曼彻斯特编码要高(80%).</p>\n<h4 id=\"2-2\"><a href=\"#2-2\" class=\"headerlink\" title=\"2.2\"></a>2.2</h4><p>&emsp;&emsp;4B/5B编码为11100 01011 11110 10101</p>\n<h4 id=\"2-4\"><a href=\"#2-4\" class=\"headerlink\" title=\"2.4\"></a>2.4</h4><p>&emsp;&emsp;HDLC中<strong>比特填充</strong>过程如下.发送方在5个连续的1后填充0(除了表示帧开始和结束的01111110),接收方在接收到5个1后分情况处理,如果后面跟着0,那么去掉0;如果后面是10,那么为帧特殊比特序列;如果后面是11,那么为无效帧,丢弃整个帧.</p>\n<p>&emsp;&emsp;因此比特序列为1101011111<font color=\"red\">0</font>01011111<font color=\"red\">0</font>101011111<font color=\"red\">0</font>110</p>\n<h4 id=\"2-7\"><a href=\"#2-7\" class=\"headerlink\" title=\"2.7\"></a>2.7</h4><p>&emsp;&emsp;比特序列为01101011111<font color=\"red\">0</font>1010011111<font color=\"red\">1</font>10110<font color=\"red\">01111110</font></p>\n<p>&emsp;&emsp;前一个0为应该移除的比特填充位,下一个1为传输错误造成的位,最后是表示帧结束的特殊比特序列.</p>\n<h4 id=\"2-11\"><a href=\"#2-11\" class=\"headerlink\" title=\"2.11\"></a>2.11</h4><p>&emsp;&emsp;差错检测.最简单的二维奇偶校验.对每个7比特附加1比特的平衡1的个数,使1的个数为偶数.最后的检验字节对每个比特位进行奇偶校验.</p>\n<p>&emsp;&emsp;出现3比特错.可能的情况有三行各出现1个比特错;一行出现2比特错,另一行出现1比特错;一行出现3比特错.第一种情况在每个校验位和检验字节都能检测到.第二种情况在检验字节能够检测和检验位能够检测到.第三种情况同样可以在检验字节和出现差错的检验位检测到.</p>\n<p>&emsp;&emsp;二维奇偶检验能够检测所有的1,3比特错及大部分2,4比特错,例外的4比特错情况为,两行各出现2比特错,并且比特位一致,二维奇偶检验无法检测这种情况.例外的2比特错情况为,同一行出现2比特错,则在行校验中无法检查出,即使通过列校验知道出错的比特位在哪一列.</p>\n<h4 id=\"2-14\"><a href=\"#2-14\" class=\"headerlink\" title=\"2.14\"></a>2.14</h4><p>&emsp;&emsp;对于两个非零数,在没有溢出的情况下,肯定不会为F0000,在溢出的情况下,需要加进位,则一定$&gt;$F0001.</p>\n<h4 id=\"2-19\"><a href=\"#2-19\" class=\"headerlink\" title=\"2.19\"></a>2.19</h4><p>&emsp;&emsp;CRC校验码的步骤,已知用于传输的多项式$C(x)$长为$k$.</p>\n<p>1)在消息的后面扩展$k$个0</p>\n<p>2)用得到的扩展后的消息除$C(x)$,得到余数.</p>\n<p>3)从扩展后的消息中减去余数(等于直接和k个0异或,即把k个0替换成余数)</p>\n<p>&emsp;&emsp;对消息1011 0010 0100 1011后面加8个0,然后除1000 00111,得余数1001 0011,最后应传输的信息为1011 0010 0100 1011 1001 0011</p>\n<h4 id=\"2-23\"><a href=\"#2-23\" class=\"headerlink\" title=\"2.23\"></a>2.23</h4><p>&emsp;&emsp;a) 时延$=40km/(2*10^8m/s)=0.2ms$</p>\n<p>&emsp;&emsp;b) 超时值$=2*EstinateRTT=0.8ms$</p>\n<p>&emsp;&emsp;c) 因为网络可能会有拥塞等情况,仍然可能出现超时.</p>\n<h4 id=\"2-25\"><a href=\"#2-25\" class=\"headerlink\" title=\"2.25\"></a>2.25</h4><p>&emsp;&emsp;$RTT=2<em>3</em>10^4km/(3*10^8m/s)=0.2s$</p>\n<p>&emsp;&emsp;时延带宽积为$10^6bps/(10^3<em>8b)</em>0.2s=25$帧.</p>\n<p>&emsp;&emsp;也就是在RWS=1的情况下,需要5比特作为序号.因为$MaxSeqNum\\geq SWS+1$. </p>\n<p>&emsp;&emsp;在RWS=SWS的情况下,需要6比特,因为$SWS&lt;(MaxSeqNum+1)/2$.</p>\n"},{"title":"pacman update problem","date":"2019-09-01T03:43:09.000Z","visitors":null,"mathjax":true,"_content":"在使用pacman更新npm时出现问题.\n\n    npm: /usr/lib/node_modules/npm/node_modules/socks-proxy-agent/yarn.lock exists in filesystem\n    Errors occurred, no packages were upgraded.\n\n需要用以下命令强制更新\n\n    pacman -S package-name --overwrite '*'\n\n用man查看pacman,可以得到--overwrite是更新(-S或-U)的参数","source":"_posts/pacman-update-problem.md","raw":"---\ntitle: pacman update problem\ndate: 2019-09-01 11:43:09\ncategories: solved problem\nvisitors: \nmathjax: true\ntags: Manjaro\n---\n在使用pacman更新npm时出现问题.\n\n    npm: /usr/lib/node_modules/npm/node_modules/socks-proxy-agent/yarn.lock exists in filesystem\n    Errors occurred, no packages were upgraded.\n\n需要用以下命令强制更新\n\n    pacman -S package-name --overwrite '*'\n\n用man查看pacman,可以得到--overwrite是更新(-S或-U)的参数","slug":"pacman-update-problem","published":1,"updated":"2019-09-01T03:49:43.339Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt3001mz4o0un1j3nsg","content":"<p>在使用pacman更新npm时出现问题.</p>\n<pre><code>npm: /usr/lib/node_modules/npm/node_modules/socks-proxy-agent/yarn.lock exists in filesystem\nErrors occurred, no packages were upgraded.\n</code></pre><p>需要用以下命令强制更新</p>\n<pre><code>pacman -S package-name --overwrite &#39;*&#39;\n</code></pre><p>用man查看pacman,可以得到—overwrite是更新(-S或-U)的参数</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在使用pacman更新npm时出现问题.</p>\n<pre><code>npm: /usr/lib/node_modules/npm/node_modules/socks-proxy-agent/yarn.lock exists in filesystem\nErrors occurred, no packages were upgraded.\n</code></pre><p>需要用以下命令强制更新</p>\n<pre><code>pacman -S package-name --overwrite &#39;*&#39;\n</code></pre><p>用man查看pacman,可以得到—overwrite是更新(-S或-U)的参数</p>\n"},{"title":"亚述","date":"2019-06-19T05:46:47.000Z","visitors":null,"mathjax":true,"_content":"&emsp;&emsp;补一点语言学的知识.顺便再复习一下历史.按照<远征记>的写作方法,先了解一下那些在古代有名的民族.语言学分类主要参考Glottolog,一个由马克思$\\cdot$普朗克人类历史科学研究所维护的目录学数据库.辅助参考Ethnologue,由基督教语言学研究机构SIL国际出版.\n\n&emsp;&emsp;亚述人,以创建亚述帝国(公元前935-前612年)而闻名于世.主要使用阿卡德语亚述方言,阿卡德语(Akkadian)是已知最早的闪族语言,属亚非语系闪米特语族东闪米特语支.文字使用源于古苏美尔语的楔形文字.阿卡德语这一分支现已灭绝.当今的亚述人主要使用新阿拉姆语(Aramaic, 属于亚非语系闪米特语族中闪米特语支,古阿拉姆语是圣经所使用的语言),他们称为新亚述语.亚述人主要信奉基督教.\n\n&emsp;&emsp;在公元前2600年左右,胡里特人和闪米特人逐渐融合成亚述人.有记录的亚述国王名单可以追溯到大约公元前2000年,在两河流域延续了几千年.在亚述帝国时空前强大,主要在现在的伊拉克境内,最强盛时版图包含了巴勒斯坦,叙利亚,一部分埃及.这个横跨亚非大陆的军事奴隶制帝国是当时中东最大的势力.当代亚述人仍然以伊拉克为最集中的区域.\n\n&emsp;&emsp;亚述帝国的国都定在尼尼微,位于现在伊拉克的摩苏尔附近.摩苏尔在阿拉伯语中是\"连接点\"的意思,位于两河流域,土耳其,伊朗的连接位置.也让这里受尽了无数的劫难.这里在圣经中被提及了多次,相传是诺亚的曾孙Nimrod所建.因为亚述帝国在征服战争中极为残暴,而且亚述人喜欢猎狮,犹太人将尼尼微成为\"血腥的狮穴\".\n\n&emsp;&emsp;在中世纪阿拉伯人侵入两河流域时,亚述人由于没有改信伊斯兰教而受到压迫,原因就是因为基督教的礼拜可以使用亚述人自己的阿拉姆语,而伊斯兰教只能使用阿拉伯语.近代,伊拉克战争以及之后的ISIS对亚述人造成了很深远的影响,再次迫使他们离散到各地.\n\n&emsp;&emsp;经过千年的磨难,亚述人从过去的阿卡德语改为了阿拉米语,从过去的多神崇拜变成现在的基督教(当然也有部分改信伊斯兰教,称为穆哈拉米人Mhallami),但仍然认同自己是亚述人.过去的亚述帝国Assyria现在已经演变成Syria,也就是叙利亚.","source":"_posts/亚述.md","raw":"---\ntitle: 亚述\ndate: 2019-06-19 13:46:47\ncategories: 语言学-亚非语系\nvisitors: \nmathjax: true\ntags: 杂谈\n---\n&emsp;&emsp;补一点语言学的知识.顺便再复习一下历史.按照<远征记>的写作方法,先了解一下那些在古代有名的民族.语言学分类主要参考Glottolog,一个由马克思$\\cdot$普朗克人类历史科学研究所维护的目录学数据库.辅助参考Ethnologue,由基督教语言学研究机构SIL国际出版.\n\n&emsp;&emsp;亚述人,以创建亚述帝国(公元前935-前612年)而闻名于世.主要使用阿卡德语亚述方言,阿卡德语(Akkadian)是已知最早的闪族语言,属亚非语系闪米特语族东闪米特语支.文字使用源于古苏美尔语的楔形文字.阿卡德语这一分支现已灭绝.当今的亚述人主要使用新阿拉姆语(Aramaic, 属于亚非语系闪米特语族中闪米特语支,古阿拉姆语是圣经所使用的语言),他们称为新亚述语.亚述人主要信奉基督教.\n\n&emsp;&emsp;在公元前2600年左右,胡里特人和闪米特人逐渐融合成亚述人.有记录的亚述国王名单可以追溯到大约公元前2000年,在两河流域延续了几千年.在亚述帝国时空前强大,主要在现在的伊拉克境内,最强盛时版图包含了巴勒斯坦,叙利亚,一部分埃及.这个横跨亚非大陆的军事奴隶制帝国是当时中东最大的势力.当代亚述人仍然以伊拉克为最集中的区域.\n\n&emsp;&emsp;亚述帝国的国都定在尼尼微,位于现在伊拉克的摩苏尔附近.摩苏尔在阿拉伯语中是\"连接点\"的意思,位于两河流域,土耳其,伊朗的连接位置.也让这里受尽了无数的劫难.这里在圣经中被提及了多次,相传是诺亚的曾孙Nimrod所建.因为亚述帝国在征服战争中极为残暴,而且亚述人喜欢猎狮,犹太人将尼尼微成为\"血腥的狮穴\".\n\n&emsp;&emsp;在中世纪阿拉伯人侵入两河流域时,亚述人由于没有改信伊斯兰教而受到压迫,原因就是因为基督教的礼拜可以使用亚述人自己的阿拉姆语,而伊斯兰教只能使用阿拉伯语.近代,伊拉克战争以及之后的ISIS对亚述人造成了很深远的影响,再次迫使他们离散到各地.\n\n&emsp;&emsp;经过千年的磨难,亚述人从过去的阿卡德语改为了阿拉米语,从过去的多神崇拜变成现在的基督教(当然也有部分改信伊斯兰教,称为穆哈拉米人Mhallami),但仍然认同自己是亚述人.过去的亚述帝国Assyria现在已经演变成Syria,也就是叙利亚.","slug":"亚述","published":1,"updated":"2019-06-19T08:14:17.089Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt4001rz4o0oi8gcu7t","content":"<p>&emsp;&emsp;补一点语言学的知识.顺便再复习一下历史.按照&lt;远征记&gt;的写作方法,先了解一下那些在古代有名的民族.语言学分类主要参考Glottolog,一个由马克思$\\cdot$普朗克人类历史科学研究所维护的目录学数据库.辅助参考Ethnologue,由基督教语言学研究机构SIL国际出版.</p>\n<p>&emsp;&emsp;亚述人,以创建亚述帝国(公元前935-前612年)而闻名于世.主要使用阿卡德语亚述方言,阿卡德语(Akkadian)是已知最早的闪族语言,属亚非语系闪米特语族东闪米特语支.文字使用源于古苏美尔语的楔形文字.阿卡德语这一分支现已灭绝.当今的亚述人主要使用新阿拉姆语(Aramaic, 属于亚非语系闪米特语族中闪米特语支,古阿拉姆语是圣经所使用的语言),他们称为新亚述语.亚述人主要信奉基督教.</p>\n<p>&emsp;&emsp;在公元前2600年左右,胡里特人和闪米特人逐渐融合成亚述人.有记录的亚述国王名单可以追溯到大约公元前2000年,在两河流域延续了几千年.在亚述帝国时空前强大,主要在现在的伊拉克境内,最强盛时版图包含了巴勒斯坦,叙利亚,一部分埃及.这个横跨亚非大陆的军事奴隶制帝国是当时中东最大的势力.当代亚述人仍然以伊拉克为最集中的区域.</p>\n<p>&emsp;&emsp;亚述帝国的国都定在尼尼微,位于现在伊拉克的摩苏尔附近.摩苏尔在阿拉伯语中是”连接点”的意思,位于两河流域,土耳其,伊朗的连接位置.也让这里受尽了无数的劫难.这里在圣经中被提及了多次,相传是诺亚的曾孙Nimrod所建.因为亚述帝国在征服战争中极为残暴,而且亚述人喜欢猎狮,犹太人将尼尼微成为”血腥的狮穴”.</p>\n<p>&emsp;&emsp;在中世纪阿拉伯人侵入两河流域时,亚述人由于没有改信伊斯兰教而受到压迫,原因就是因为基督教的礼拜可以使用亚述人自己的阿拉姆语,而伊斯兰教只能使用阿拉伯语.近代,伊拉克战争以及之后的ISIS对亚述人造成了很深远的影响,再次迫使他们离散到各地.</p>\n<p>&emsp;&emsp;经过千年的磨难,亚述人从过去的阿卡德语改为了阿拉米语,从过去的多神崇拜变成现在的基督教(当然也有部分改信伊斯兰教,称为穆哈拉米人Mhallami),但仍然认同自己是亚述人.过去的亚述帝国Assyria现在已经演变成Syria,也就是叙利亚.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>&emsp;&emsp;补一点语言学的知识.顺便再复习一下历史.按照&lt;远征记&gt;的写作方法,先了解一下那些在古代有名的民族.语言学分类主要参考Glottolog,一个由马克思$\\cdot$普朗克人类历史科学研究所维护的目录学数据库.辅助参考Ethnologue,由基督教语言学研究机构SIL国际出版.</p>\n<p>&emsp;&emsp;亚述人,以创建亚述帝国(公元前935-前612年)而闻名于世.主要使用阿卡德语亚述方言,阿卡德语(Akkadian)是已知最早的闪族语言,属亚非语系闪米特语族东闪米特语支.文字使用源于古苏美尔语的楔形文字.阿卡德语这一分支现已灭绝.当今的亚述人主要使用新阿拉姆语(Aramaic, 属于亚非语系闪米特语族中闪米特语支,古阿拉姆语是圣经所使用的语言),他们称为新亚述语.亚述人主要信奉基督教.</p>\n<p>&emsp;&emsp;在公元前2600年左右,胡里特人和闪米特人逐渐融合成亚述人.有记录的亚述国王名单可以追溯到大约公元前2000年,在两河流域延续了几千年.在亚述帝国时空前强大,主要在现在的伊拉克境内,最强盛时版图包含了巴勒斯坦,叙利亚,一部分埃及.这个横跨亚非大陆的军事奴隶制帝国是当时中东最大的势力.当代亚述人仍然以伊拉克为最集中的区域.</p>\n<p>&emsp;&emsp;亚述帝国的国都定在尼尼微,位于现在伊拉克的摩苏尔附近.摩苏尔在阿拉伯语中是”连接点”的意思,位于两河流域,土耳其,伊朗的连接位置.也让这里受尽了无数的劫难.这里在圣经中被提及了多次,相传是诺亚的曾孙Nimrod所建.因为亚述帝国在征服战争中极为残暴,而且亚述人喜欢猎狮,犹太人将尼尼微成为”血腥的狮穴”.</p>\n<p>&emsp;&emsp;在中世纪阿拉伯人侵入两河流域时,亚述人由于没有改信伊斯兰教而受到压迫,原因就是因为基督教的礼拜可以使用亚述人自己的阿拉姆语,而伊斯兰教只能使用阿拉伯语.近代,伊拉克战争以及之后的ISIS对亚述人造成了很深远的影响,再次迫使他们离散到各地.</p>\n<p>&emsp;&emsp;经过千年的磨难,亚述人从过去的阿卡德语改为了阿拉米语,从过去的多神崇拜变成现在的基督教(当然也有部分改信伊斯兰教,称为穆哈拉米人Mhallami),但仍然认同自己是亚述人.过去的亚述帝国Assyria现在已经演变成Syria,也就是叙利亚.</p>\n"},{"title":"腓尼基","date":"2019-06-21T02:36:31.000Z","visitors":null,"mathjax":true,"_content":"&emsp;&emsp;腓尼基人是闪米特人的一支,以擅长航海与经商著称.使用腓尼基语和布匿语,使用腓尼基文字.属于亚非语系闪米特语族中闪米特语支.比较有意思的是,腓尼基字母是由楔形字母改造的,而希腊字母是以腓尼基字母改造而来的.腓尼基语现已消亡.\n\n&emsp;&emsp;腓尼基文明十分古老,早期臣服于古埃及.后来古埃及衰落后,迅速崛起.最强盛时是地中海当之无愧的霸主.之后和古希腊在地中海形成分庭抗礼之势.由腓尼基人统治的迦太基占据了西地中海,然而在经过三次布匿战争之后走向了衰落.迦太基城便是现在的突尼斯,在古罗马的文献中称为\"布匿\".\n\n&emsp;&emsp;腓尼基人原本是在地中海东岸,范围是现在的黎巴嫩和叙利亚.在公元前8世纪,狄多公主被排挤,为免遭迫害,逃到了突尼斯,并在柏柏人的领地建成了迦太基城.\n\n&emsp;&emsp;在布匿战争中最为亮眼的是迦太基将军汉尼拔,在坎尼战争中围歼了七万罗马士兵,战损比达到7:1.直至今日仍是军事史上最伟大的战役之一.可惜在罗马反攻时,汉尼拔军队因为驰援导致兵力下降,最终导致战争失败.在第三次布匿战争中,迦太基惨遭亡国.在十字军东征时,迦太基城几乎全部损坏殆尽.","source":"_posts/腓尼基.md","raw":"---\ntitle: 腓尼基\ndate: 2019-06-21 10:36:31\ncategories: 语言学-亚非语系\nvisitors: \nmathjax: true\ntags: 杂谈\n---\n&emsp;&emsp;腓尼基人是闪米特人的一支,以擅长航海与经商著称.使用腓尼基语和布匿语,使用腓尼基文字.属于亚非语系闪米特语族中闪米特语支.比较有意思的是,腓尼基字母是由楔形字母改造的,而希腊字母是以腓尼基字母改造而来的.腓尼基语现已消亡.\n\n&emsp;&emsp;腓尼基文明十分古老,早期臣服于古埃及.后来古埃及衰落后,迅速崛起.最强盛时是地中海当之无愧的霸主.之后和古希腊在地中海形成分庭抗礼之势.由腓尼基人统治的迦太基占据了西地中海,然而在经过三次布匿战争之后走向了衰落.迦太基城便是现在的突尼斯,在古罗马的文献中称为\"布匿\".\n\n&emsp;&emsp;腓尼基人原本是在地中海东岸,范围是现在的黎巴嫩和叙利亚.在公元前8世纪,狄多公主被排挤,为免遭迫害,逃到了突尼斯,并在柏柏人的领地建成了迦太基城.\n\n&emsp;&emsp;在布匿战争中最为亮眼的是迦太基将军汉尼拔,在坎尼战争中围歼了七万罗马士兵,战损比达到7:1.直至今日仍是军事史上最伟大的战役之一.可惜在罗马反攻时,汉尼拔军队因为驰援导致兵力下降,最终导致战争失败.在第三次布匿战争中,迦太基惨遭亡国.在十字军东征时,迦太基城几乎全部损坏殆尽.","slug":"腓尼基","published":1,"updated":"2019-06-21T03:11:05.735Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt5001uz4o0sn3mf2xf","content":"<p>&emsp;&emsp;腓尼基人是闪米特人的一支,以擅长航海与经商著称.使用腓尼基语和布匿语,使用腓尼基文字.属于亚非语系闪米特语族中闪米特语支.比较有意思的是,腓尼基字母是由楔形字母改造的,而希腊字母是以腓尼基字母改造而来的.腓尼基语现已消亡.</p>\n<p>&emsp;&emsp;腓尼基文明十分古老,早期臣服于古埃及.后来古埃及衰落后,迅速崛起.最强盛时是地中海当之无愧的霸主.之后和古希腊在地中海形成分庭抗礼之势.由腓尼基人统治的迦太基占据了西地中海,然而在经过三次布匿战争之后走向了衰落.迦太基城便是现在的突尼斯,在古罗马的文献中称为”布匿”.</p>\n<p>&emsp;&emsp;腓尼基人原本是在地中海东岸,范围是现在的黎巴嫩和叙利亚.在公元前8世纪,狄多公主被排挤,为免遭迫害,逃到了突尼斯,并在柏柏人的领地建成了迦太基城.</p>\n<p>&emsp;&emsp;在布匿战争中最为亮眼的是迦太基将军汉尼拔,在坎尼战争中围歼了七万罗马士兵,战损比达到7:1.直至今日仍是军事史上最伟大的战役之一.可惜在罗马反攻时,汉尼拔军队因为驰援导致兵力下降,最终导致战争失败.在第三次布匿战争中,迦太基惨遭亡国.在十字军东征时,迦太基城几乎全部损坏殆尽.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>&emsp;&emsp;腓尼基人是闪米特人的一支,以擅长航海与经商著称.使用腓尼基语和布匿语,使用腓尼基文字.属于亚非语系闪米特语族中闪米特语支.比较有意思的是,腓尼基字母是由楔形字母改造的,而希腊字母是以腓尼基字母改造而来的.腓尼基语现已消亡.</p>\n<p>&emsp;&emsp;腓尼基文明十分古老,早期臣服于古埃及.后来古埃及衰落后,迅速崛起.最强盛时是地中海当之无愧的霸主.之后和古希腊在地中海形成分庭抗礼之势.由腓尼基人统治的迦太基占据了西地中海,然而在经过三次布匿战争之后走向了衰落.迦太基城便是现在的突尼斯,在古罗马的文献中称为”布匿”.</p>\n<p>&emsp;&emsp;腓尼基人原本是在地中海东岸,范围是现在的黎巴嫩和叙利亚.在公元前8世纪,狄多公主被排挤,为免遭迫害,逃到了突尼斯,并在柏柏人的领地建成了迦太基城.</p>\n<p>&emsp;&emsp;在布匿战争中最为亮眼的是迦太基将军汉尼拔,在坎尼战争中围歼了七万罗马士兵,战损比达到7:1.直至今日仍是军事史上最伟大的战役之一.可惜在罗马反攻时,汉尼拔军队因为驰援导致兵力下降,最终导致战争失败.在第三次布匿战争中,迦太基惨遭亡国.在十字军东征时,迦太基城几乎全部损坏殆尽.</p>\n"},{"title":"亚非语系","date":"2019-06-19T07:50:20.000Z","visitors":null,"mathjax":true,"_content":"&emsp;&emsp;亚非语系(Afro-Asiatic)原来称为闪含语系,主要分布在阿拉伯半岛和非洲北部.因为根据<圣经>,诺亚有三个儿子,一个叫闪,一个叫含,还有一个叫雅弗.闪是希伯来人和亚述人的祖先,含是非洲人的先祖,雅弗是雅利安人的祖先.甚至在19世纪前西方都认为亚洲人都是含的子孙.\n\n&emsp;&emsp;亚非语系包括六个语族:柏柏尔语族,乍得语族,闪米特语族,库希特语族,奥摩语族和已经消亡的埃及语族.现在使用最多的是现代阿拉伯语.\n\n&emsp;&emsp;亚非语系主要的特征是:\n- 是屈折语.\n- 有三辅音:清辅音,浊辅音和重辅音.\n- 喉音丰富.\n- 词根基本由辅音组成,往中间插元音来变化.\n- 有格和性的区别.","source":"_posts/亚非语系.md","raw":"---\ntitle: 亚非语系\ndate: 2019-06-19 15:50:20\ncategories: 语言学-亚非语系\nvisitors: \nmathjax: true\ntags: 杂谈\n---\n&emsp;&emsp;亚非语系(Afro-Asiatic)原来称为闪含语系,主要分布在阿拉伯半岛和非洲北部.因为根据<圣经>,诺亚有三个儿子,一个叫闪,一个叫含,还有一个叫雅弗.闪是希伯来人和亚述人的祖先,含是非洲人的先祖,雅弗是雅利安人的祖先.甚至在19世纪前西方都认为亚洲人都是含的子孙.\n\n&emsp;&emsp;亚非语系包括六个语族:柏柏尔语族,乍得语族,闪米特语族,库希特语族,奥摩语族和已经消亡的埃及语族.现在使用最多的是现代阿拉伯语.\n\n&emsp;&emsp;亚非语系主要的特征是:\n- 是屈折语.\n- 有三辅音:清辅音,浊辅音和重辅音.\n- 喉音丰富.\n- 词根基本由辅音组成,往中间插元音来变化.\n- 有格和性的区别.","slug":"亚非语系","published":1,"updated":"2019-06-19T08:26:03.170Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt6001zz4o0z7hue8yg","content":"<p>&emsp;&emsp;亚非语系(Afro-Asiatic)原来称为闪含语系,主要分布在阿拉伯半岛和非洲北部.因为根据&lt;圣经&gt;,诺亚有三个儿子,一个叫闪,一个叫含,还有一个叫雅弗.闪是希伯来人和亚述人的祖先,含是非洲人的先祖,雅弗是雅利安人的祖先.甚至在19世纪前西方都认为亚洲人都是含的子孙.</p>\n<p>&emsp;&emsp;亚非语系包括六个语族:柏柏尔语族,乍得语族,闪米特语族,库希特语族,奥摩语族和已经消亡的埃及语族.现在使用最多的是现代阿拉伯语.</p>\n<p>&emsp;&emsp;亚非语系主要的特征是:</p>\n<ul>\n<li>是屈折语.</li>\n<li>有三辅音:清辅音,浊辅音和重辅音.</li>\n<li>喉音丰富.</li>\n<li>词根基本由辅音组成,往中间插元音来变化.</li>\n<li>有格和性的区别.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>&emsp;&emsp;亚非语系(Afro-Asiatic)原来称为闪含语系,主要分布在阿拉伯半岛和非洲北部.因为根据&lt;圣经&gt;,诺亚有三个儿子,一个叫闪,一个叫含,还有一个叫雅弗.闪是希伯来人和亚述人的祖先,含是非洲人的先祖,雅弗是雅利安人的祖先.甚至在19世纪前西方都认为亚洲人都是含的子孙.</p>\n<p>&emsp;&emsp;亚非语系包括六个语族:柏柏尔语族,乍得语族,闪米特语族,库希特语族,奥摩语族和已经消亡的埃及语族.现在使用最多的是现代阿拉伯语.</p>\n<p>&emsp;&emsp;亚非语系主要的特征是:</p>\n<ul>\n<li>是屈折语.</li>\n<li>有三辅音:清辅音,浊辅音和重辅音.</li>\n<li>喉音丰富.</li>\n<li>词根基本由辅音组成,往中间插元音来变化.</li>\n<li>有格和性的区别.</li>\n</ul>\n"},{"title":"苏美尔","date":"2019-06-21T03:12:34.000Z","visitors":null,"mathjax":true,"_content":"&emsp;&emsp;苏美尔是全世界最早的文明之一,可以追溯到公元前4500年.使用苏美尔语和楔形文字.苏美尔语在语言学上是一个孤立语言,不属于任何语族.\n\n&emsp;&emsp;据考证,黑头发的苏美尔人不属于闪米特人,也不属于印欧人,其起源仍然是个迷.\n","source":"_posts/苏美尔.md","raw":"---\ntitle: 苏美尔\ndate: 2019-06-21 11:12:34\ncategories: 语言学\nvisitors: \nmathjax: true\ntags: 杂谈\n---\n&emsp;&emsp;苏美尔是全世界最早的文明之一,可以追溯到公元前4500年.使用苏美尔语和楔形文字.苏美尔语在语言学上是一个孤立语言,不属于任何语族.\n\n&emsp;&emsp;据考证,黑头发的苏美尔人不属于闪米特人,也不属于印欧人,其起源仍然是个迷.\n","slug":"苏美尔","published":1,"updated":"2019-06-25T05:48:03.255Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt70022z4o0epsu0ypa","content":"<p>&emsp;&emsp;苏美尔是全世界最早的文明之一,可以追溯到公元前4500年.使用苏美尔语和楔形文字.苏美尔语在语言学上是一个孤立语言,不属于任何语族.</p>\n<p>&emsp;&emsp;据考证,黑头发的苏美尔人不属于闪米特人,也不属于印欧人,其起源仍然是个迷.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>&emsp;&emsp;苏美尔是全世界最早的文明之一,可以追溯到公元前4500年.使用苏美尔语和楔形文字.苏美尔语在语言学上是一个孤立语言,不属于任何语族.</p>\n<p>&emsp;&emsp;据考证,黑头发的苏美尔人不属于闪米特人,也不属于印欧人,其起源仍然是个迷.</p>\n"},{"title":"闪米特","date":"2019-06-19T06:58:01.000Z","visitors":null,"mathjax":true,"_content":"&emsp;&emsp;闪米特人是个极为古老的游牧民族.相传诺亚三个儿子之一闪(Shem)是其祖先.按照<创世纪>的说法,闪的儿子分别是亚述人,迦勒底人,塞巴人,希伯来人和阿拉姆人的祖先.\n\n&emsp;&emsp;闪米特语族是亚非语系下的一个语族.主要分为东闪米特语支(已消亡),南闪米特语支和中闪米特语支.\n\n&emsp;&emsp;现代使用的分支语言人数最多的是阿拉伯语,另外还包括希伯来语和阿拉米语.","source":"_posts/闪米特.md","raw":"---\ntitle: 闪米特\ndate: 2019-06-19 14:58:01\ncategories: 语言学-亚非语系\nvisitors: \nmathjax: true\ntags: 杂谈\n---\n&emsp;&emsp;闪米特人是个极为古老的游牧民族.相传诺亚三个儿子之一闪(Shem)是其祖先.按照<创世纪>的说法,闪的儿子分别是亚述人,迦勒底人,塞巴人,希伯来人和阿拉姆人的祖先.\n\n&emsp;&emsp;闪米特语族是亚非语系下的一个语族.主要分为东闪米特语支(已消亡),南闪米特语支和中闪米特语支.\n\n&emsp;&emsp;现代使用的分支语言人数最多的是阿拉伯语,另外还包括希伯来语和阿拉米语.","slug":"闪米特","published":1,"updated":"2019-06-19T07:48:35.220Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt80026z4o0pazvia89","content":"<p>&emsp;&emsp;闪米特人是个极为古老的游牧民族.相传诺亚三个儿子之一闪(Shem)是其祖先.按照&lt;创世纪&gt;的说法,闪的儿子分别是亚述人,迦勒底人,塞巴人,希伯来人和阿拉姆人的祖先.</p>\n<p>&emsp;&emsp;闪米特语族是亚非语系下的一个语族.主要分为东闪米特语支(已消亡),南闪米特语支和中闪米特语支.</p>\n<p>&emsp;&emsp;现代使用的分支语言人数最多的是阿拉伯语,另外还包括希伯来语和阿拉米语.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>&emsp;&emsp;闪米特人是个极为古老的游牧民族.相传诺亚三个儿子之一闪(Shem)是其祖先.按照&lt;创世纪&gt;的说法,闪的儿子分别是亚述人,迦勒底人,塞巴人,希伯来人和阿拉姆人的祖先.</p>\n<p>&emsp;&emsp;闪米特语族是亚非语系下的一个语族.主要分为东闪米特语支(已消亡),南闪米特语支和中闪米特语支.</p>\n<p>&emsp;&emsp;现代使用的分支语言人数最多的是阿拉伯语,另外还包括希伯来语和阿拉米语.</p>\n"},{"title":"Transformer","date":"2019-06-27T06:07:58.000Z","visitors":null,"mathjax":true,"_content":"&emsp;&emsp;这是2017年Google Brain的一篇论文,使用self-attention机制构建模型.和之前完全不同的思路,\n- RNN因为是递归式的结构,无法做到并行,而且要获得全局信息必须要做双向RNN.\n- CNN则是因为感受域限制,只能获取局部信息.\n- Transformer能够一步获取全局信息,在做NLP任务时表现很优异,如共指消解.\n\n&emsp;&emsp;Transformer使用传统的Encoder-Decoder结构,给定一个序列$(x_1,...,x_n)$经过处理得到$(y_1,...,y_m)$.模型的每一步都是自回归的,每次生成都使用之前的生成作为附加信息输入.\n\n<img src=\"http://ww1.sinaimg.cn/large/006tNc79gy1g4fpkw99egg30m80jo4ni.gif\" width=\"80%\" height=\"80%\">\n\n<img src=\"http://ww4.sinaimg.cn/large/006tNc79gy1g4fpbsaicwj30ng0ystfy.jpg\" width=\"80%\" height=\"80%\" aligns=center>\n\n{% codeblock lang:python %}\nclass EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n\n    def encode(self, src, src_mask):\n        return self.encode(self.src_embed(src), src_mask)\n\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n{% endcodeblock %}\n\n这是构建EncoderDecoder框架,注意到encode(src, src_mask)就是memory,也就是用encode的输出作为了Decode的输入,图上两个部分相连的那条线.\n\n{% codeblock lang:python %}\nclass Generator(nn.Module):\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n\n    def forward(self, x):\n        return F.log_softmax(self.proj(x), dim=-1)\n{% endcodeblock %}\n\n&emsp;&emsp;这里是定义一个标准的线性然后softmax的标准生成步骤.因为之前没有用过pytorch,所以还是记录一下用到的内置函数. \n\n- $nn.Linear()$函数是用来做线性变化的,输入一个维度为$m\\times n$的矩阵,经过线性变化$nn.Linear(n,t,bias=None)$得到的输入维度为$m\\times t$.具体的实现是输入矩阵乘$t\\times n$矩阵再加上一个维度为$m$的$\\text{bias}$.\n$$y=xA^T+\\text{bias}$$\n- $\\text{F.log\\_softmax()}$函数就是在softmax函数外面再加一个$\\text{log}$函数.需要注意的是$\\text{dim}$的指定,以二维函数为例,取$\\text{dim}=1$时,是对每一行进行softmax,而取0时是每一列进行计算.可以理解为沿着某条轴,将数据集中到轴上.\n$$\\log(\\text{Softmax}(x_{i})) = \\log(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)})$$\n\n---\n接下来是Encoder部分\n```python\ndef clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n```\n&emsp;&emsp;首先定义一个clone方法,因为Encoder是由多个相同的层构成的.这里设$N=6$\n```python\nclass Encoder(nn.Module):\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n```\n&emsp;&emsp;然后定义Encoder和LN模块.LayerNorm本来torch里也有内置函数.这里是自己实现的.\n$$y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}\\times \\gamma +\\beta$$\n&emsp;&emsp;其中$\\gamma$和$\\beta$是参数,$\\epsilon$是一个很小数,torch中取$e^{-5}$,为了保持数值的稳定性,防止分母趋近0.\n\n&emsp;&emsp;对应到图上就是$\\text{Add\\&Norm}$这一块,对于每个sublayer有$LayerNorm(x+Sublayer(x))$.具体是$dropout\\rightarrow sublayer的input相加\\rightarrow Normalized$.但是在实际编码中,将Norm放到了dropout前面.\n```python\nclass SublayerConnection(nn.Module):\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))\n```\n&emsp;&emsp;定义sublayer的残差连接.残差连接实际上就是$f(x)+x$,目的是为了反向传播的时候,梯度连乘,不会造成梯度消失.因为会出现常数项.可以看到是先normalizion再dropout.\n```python\nclass EncoderLayer(nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n\n    def forward(self, x, mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n```\n&emsp;&emsp;Encoder每层有两个sublayer,第一个sublayer[0]用多头注意力机制,第二个sublayer[1]用位置全连接前馈网络.\n\n---\n然后是Decoder层\n```python\nclass Decoder(nn.Module):\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)\n```\n和Encoder一样,设计为6个相同的层.\n```python\nclass DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n```\n&emsp;&emsp;Decoder的sublayer有三个,其中sublayer[0]和Encoder一样用自注意力机制,sublayer[2]同样是前馈网络,sublayer[1]是对Encoder的输出用多头注意力机制.残差网络和Norm和Encoder一样.\n```python\ndef subsequent_mask(size):\n    attn_shape = (1, size, size)\n    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n    return torch.from_numpy(subsequent_mask) == 0\n```\n&emsp;&emsp;定义subsequent_mask方法,为了在Decoder时遮盖当前位置后的内容. $np.triu$方法是取上三角矩阵,$k$为偏移量,正数时向上偏移,比如有矩阵$\\begin{vmatrix} 1&2 \\\\ 2&3 \\\\ 3&4 \\end{vmatrix}$, 其上三角矩阵为$\\begin{vmatrix} 1&2 \\\\ 0&3 \\\\ 0&0 \\end{vmatrix}$, $k=1$时为$\\begin{vmatrix} 0&2 \\\\ 0&0 \\\\ 0&0 \\end{vmatrix}$.最后返回的是取反后的矩阵.也就是上三角为0.\n\n---\n接下来是attent模块.","source":"_posts/Transformer.md","raw":"---\ntitle: Transformer\ndate: 2019-06-27 14:07:58\ncategories: NLP\nvisitors: \nmathjax: true\ntags: machine learning\n---\n&emsp;&emsp;这是2017年Google Brain的一篇论文,使用self-attention机制构建模型.和之前完全不同的思路,\n- RNN因为是递归式的结构,无法做到并行,而且要获得全局信息必须要做双向RNN.\n- CNN则是因为感受域限制,只能获取局部信息.\n- Transformer能够一步获取全局信息,在做NLP任务时表现很优异,如共指消解.\n\n&emsp;&emsp;Transformer使用传统的Encoder-Decoder结构,给定一个序列$(x_1,...,x_n)$经过处理得到$(y_1,...,y_m)$.模型的每一步都是自回归的,每次生成都使用之前的生成作为附加信息输入.\n\n<img src=\"http://ww1.sinaimg.cn/large/006tNc79gy1g4fpkw99egg30m80jo4ni.gif\" width=\"80%\" height=\"80%\">\n\n<img src=\"http://ww4.sinaimg.cn/large/006tNc79gy1g4fpbsaicwj30ng0ystfy.jpg\" width=\"80%\" height=\"80%\" aligns=center>\n\n{% codeblock lang:python %}\nclass EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n\n    def encode(self, src, src_mask):\n        return self.encode(self.src_embed(src), src_mask)\n\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n{% endcodeblock %}\n\n这是构建EncoderDecoder框架,注意到encode(src, src_mask)就是memory,也就是用encode的输出作为了Decode的输入,图上两个部分相连的那条线.\n\n{% codeblock lang:python %}\nclass Generator(nn.Module):\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n\n    def forward(self, x):\n        return F.log_softmax(self.proj(x), dim=-1)\n{% endcodeblock %}\n\n&emsp;&emsp;这里是定义一个标准的线性然后softmax的标准生成步骤.因为之前没有用过pytorch,所以还是记录一下用到的内置函数. \n\n- $nn.Linear()$函数是用来做线性变化的,输入一个维度为$m\\times n$的矩阵,经过线性变化$nn.Linear(n,t,bias=None)$得到的输入维度为$m\\times t$.具体的实现是输入矩阵乘$t\\times n$矩阵再加上一个维度为$m$的$\\text{bias}$.\n$$y=xA^T+\\text{bias}$$\n- $\\text{F.log\\_softmax()}$函数就是在softmax函数外面再加一个$\\text{log}$函数.需要注意的是$\\text{dim}$的指定,以二维函数为例,取$\\text{dim}=1$时,是对每一行进行softmax,而取0时是每一列进行计算.可以理解为沿着某条轴,将数据集中到轴上.\n$$\\log(\\text{Softmax}(x_{i})) = \\log(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)})$$\n\n---\n接下来是Encoder部分\n```python\ndef clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n```\n&emsp;&emsp;首先定义一个clone方法,因为Encoder是由多个相同的层构成的.这里设$N=6$\n```python\nclass Encoder(nn.Module):\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n```\n&emsp;&emsp;然后定义Encoder和LN模块.LayerNorm本来torch里也有内置函数.这里是自己实现的.\n$$y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}\\times \\gamma +\\beta$$\n&emsp;&emsp;其中$\\gamma$和$\\beta$是参数,$\\epsilon$是一个很小数,torch中取$e^{-5}$,为了保持数值的稳定性,防止分母趋近0.\n\n&emsp;&emsp;对应到图上就是$\\text{Add\\&Norm}$这一块,对于每个sublayer有$LayerNorm(x+Sublayer(x))$.具体是$dropout\\rightarrow sublayer的input相加\\rightarrow Normalized$.但是在实际编码中,将Norm放到了dropout前面.\n```python\nclass SublayerConnection(nn.Module):\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))\n```\n&emsp;&emsp;定义sublayer的残差连接.残差连接实际上就是$f(x)+x$,目的是为了反向传播的时候,梯度连乘,不会造成梯度消失.因为会出现常数项.可以看到是先normalizion再dropout.\n```python\nclass EncoderLayer(nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n\n    def forward(self, x, mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n```\n&emsp;&emsp;Encoder每层有两个sublayer,第一个sublayer[0]用多头注意力机制,第二个sublayer[1]用位置全连接前馈网络.\n\n---\n然后是Decoder层\n```python\nclass Decoder(nn.Module):\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)\n```\n和Encoder一样,设计为6个相同的层.\n```python\nclass DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n```\n&emsp;&emsp;Decoder的sublayer有三个,其中sublayer[0]和Encoder一样用自注意力机制,sublayer[2]同样是前馈网络,sublayer[1]是对Encoder的输出用多头注意力机制.残差网络和Norm和Encoder一样.\n```python\ndef subsequent_mask(size):\n    attn_shape = (1, size, size)\n    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n    return torch.from_numpy(subsequent_mask) == 0\n```\n&emsp;&emsp;定义subsequent_mask方法,为了在Decoder时遮盖当前位置后的内容. $np.triu$方法是取上三角矩阵,$k$为偏移量,正数时向上偏移,比如有矩阵$\\begin{vmatrix} 1&2 \\\\ 2&3 \\\\ 3&4 \\end{vmatrix}$, 其上三角矩阵为$\\begin{vmatrix} 1&2 \\\\ 0&3 \\\\ 0&0 \\end{vmatrix}$, $k=1$时为$\\begin{vmatrix} 0&2 \\\\ 0&0 \\\\ 0&0 \\end{vmatrix}$.最后返回的是取反后的矩阵.也就是上三角为0.\n\n---\n接下来是attent模块.","slug":"Transformer","published":1,"updated":"2019-06-27T12:55:03.274Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck04z9nt90029z4o0rx8o59h0","content":"<p>&emsp;&emsp;这是2017年Google Brain的一篇论文,使用self-attention机制构建模型.和之前完全不同的思路,</p>\n<ul>\n<li>RNN因为是递归式的结构,无法做到并行,而且要获得全局信息必须要做双向RNN.</li>\n<li>CNN则是因为感受域限制,只能获取局部信息.</li>\n<li>Transformer能够一步获取全局信息,在做NLP任务时表现很优异,如共指消解.</li>\n</ul>\n<p>&emsp;&emsp;Transformer使用传统的Encoder-Decoder结构,给定一个序列$(x_1,…,x_n)$经过处理得到$(y_1,…,y_m)$.模型的每一步都是自回归的,每次生成都使用之前的生成作为附加信息输入.</p>\n<p><img src=\"http://ww1.sinaimg.cn/large/006tNc79gy1g4fpkw99egg30m80jo4ni.gif\" width=\"80%\" height=\"80%\"></p>\n<p><img src=\"http://ww4.sinaimg.cn/large/006tNc79gy1g4fpbsaicwj30ng0ystfy.jpg\" width=\"80%\" height=\"80%\" aligns=\"center\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">EncoderDecoder</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class=\"line\">        super(EncoderDecoder, self).__init__()</span><br><span class=\"line\">        self.encoder = encoder</span><br><span class=\"line\">        self.decoder = decoder</span><br><span class=\"line\">        self.src_embed = src_embed</span><br><span class=\"line\">        self.tgt_embed = tgt_embed</span><br><span class=\"line\">        self.generator = generator</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encode</span><span class=\"params\">(self, src, src_mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.encode(self.src_embed(src), src_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decode</span><span class=\"params\">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>\n<p>这是构建EncoderDecoder框架,注意到encode(src, src_mask)就是memory,也就是用encode的输出作为了Decode的输入,图上两个部分相连的那条线.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Generator</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, d_model, vocab)</span>:</span></span><br><span class=\"line\">        super(Generator, self).__init__()</span><br><span class=\"line\">        self.proj = nn.Linear(d_model, vocab)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(self.proj(x), dim=<span class=\"number\">-1</span>)</span><br></pre></td></tr></table></figure>\n<p>&emsp;&emsp;这里是定义一个标准的线性然后softmax的标准生成步骤.因为之前没有用过pytorch,所以还是记录一下用到的内置函数. </p>\n<ul>\n<li>$nn.Linear()$函数是用来做线性变化的,输入一个维度为$m\\times n$的矩阵,经过线性变化$nn.Linear(n,t,bias=None)$得到的输入维度为$m\\times t$.具体的实现是输入矩阵乘$t\\times n$矩阵再加上一个维度为$m$的$\\text{bias}$.<script type=\"math/tex; mode=display\">y=xA^T+\\text{bias}</script></li>\n<li>$\\text{F.log_softmax()}$函数就是在softmax函数外面再加一个$\\text{log}$函数.需要注意的是$\\text{dim}$的指定,以二维函数为例,取$\\text{dim}=1$时,是对每一行进行softmax,而取0时是每一列进行计算.可以理解为沿着某条轴,将数据集中到轴上.<script type=\"math/tex; mode=display\">\\log(\\text{Softmax}(x_{i})) = \\log(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)})</script></li>\n</ul>\n<hr>\n<p>接下来是Encoder部分<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clones</span><span class=\"params\">(module, N)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.ModuleList([copy.deepcopy(module) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(N)])</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;首先定义一个clone方法,因为Encoder是由多个相同的层构成的.这里设$N=6$<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Encoder</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, layer, N)</span>:</span></span><br><span class=\"line\">        super(Encoder, self).__init__()</span><br><span class=\"line\">        self.layers = clones(layer, N)</span><br><span class=\"line\">        self.norm = LayerNorm(layer.size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> self.layers:</span><br><span class=\"line\">            x = layer(x, mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.norm(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LayerNorm</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, features, eps=<span class=\"number\">1e-6</span>)</span>:</span></span><br><span class=\"line\">        super(LayerNorm, self).__init__()</span><br><span class=\"line\">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class=\"line\">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class=\"line\">        self.eps = eps</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        mean = x.mean(<span class=\"number\">-1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        std = x.std(<span class=\"number\">-1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;然后定义Encoder和LN模块.LayerNorm本来torch里也有内置函数.这里是自己实现的.</p>\n<script type=\"math/tex; mode=display\">y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}\\times \\gamma +\\beta</script><p>&emsp;&emsp;其中$\\gamma$和$\\beta$是参数,$\\epsilon$是一个很小数,torch中取$e^{-5}$,为了保持数值的稳定性,防止分母趋近0.</p>\n<p>&emsp;&emsp;对应到图上就是$\\text{Add\\&amp;Norm}$这一块,对于每个sublayer有$LayerNorm(x+Sublayer(x))$.具体是$dropout\\rightarrow sublayer的input相加\\rightarrow Normalized$.但是在实际编码中,将Norm放到了dropout前面.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SublayerConnection</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, size, dropout)</span>:</span></span><br><span class=\"line\">        super(SublayerConnection, self).__init__()</span><br><span class=\"line\">        self.norm = LayerNorm(size)</span><br><span class=\"line\">        self.dropout = nn.Dropout(dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, sublayer)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;定义sublayer的残差连接.残差连接实际上就是$f(x)+x$,目的是为了反向传播的时候,梯度连乘,不会造成梯度消失.因为会出现常数项.可以看到是先normalizion再dropout.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">EncoderLayer</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class=\"line\">        super(EncoderLayer, self).__init__()</span><br><span class=\"line\">        self.self_attn = self_attn</span><br><span class=\"line\">        self.feed_forward = feed_forward</span><br><span class=\"line\">        self.sublayer = clones(SublayerConnection(size, dropout), <span class=\"number\">2</span>)</span><br><span class=\"line\">        self.size = size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, mask)</span>:</span></span><br><span class=\"line\">        x = self.sublayer[<span class=\"number\">0</span>](x, <span class=\"keyword\">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.sublayer[<span class=\"number\">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;Encoder每层有两个sublayer,第一个sublayer[0]用多头注意力机制,第二个sublayer[1]用位置全连接前馈网络.</p>\n<hr>\n<p>然后是Decoder层<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Decoder</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, layer, N)</span>:</span></span><br><span class=\"line\">        super(Decoder, self).__init__()</span><br><span class=\"line\">        self.layers = clones(layer, N)</span><br><span class=\"line\">        self.norm = LayerNorm(layer.size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> self.layers:</span><br><span class=\"line\">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.norm(x)</span><br></pre></td></tr></table></figure></p>\n<p>和Encoder一样,设计为6个相同的层.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DecoderLayer</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class=\"line\">        super(DecoderLayer, self).__init__()</span><br><span class=\"line\">        self.size = size</span><br><span class=\"line\">        self.self_attn = self_attn</span><br><span class=\"line\">        self.src_attn = src_attn</span><br><span class=\"line\">        self.feed_forward = feed_forward</span><br><span class=\"line\">        self.sublayer = clones(SublayerConnection(size, dropout), <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class=\"line\">        m = memory</span><br><span class=\"line\">        x = self.sublayer[<span class=\"number\">0</span>](x, <span class=\"keyword\">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class=\"line\">        x = self.sublayer[<span class=\"number\">1</span>](x, <span class=\"keyword\">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.sublayer[<span class=\"number\">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;Decoder的sublayer有三个,其中sublayer[0]和Encoder一样用自注意力机制,sublayer[2]同样是前馈网络,sublayer[1]是对Encoder的输出用多头注意力机制.残差网络和Norm和Encoder一样.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">subsequent_mask</span><span class=\"params\">(size)</span>:</span></span><br><span class=\"line\">    attn_shape = (<span class=\"number\">1</span>, size, size)</span><br><span class=\"line\">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class=\"number\">1</span>).astype(<span class=\"string\">'uint8'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.from_numpy(subsequent_mask) == <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;定义subsequent_mask方法,为了在Decoder时遮盖当前位置后的内容. $np.triu$方法是取上三角矩阵,$k$为偏移量,正数时向上偏移,比如有矩阵$\\begin{vmatrix} 1&amp;2 \\\\ 2&amp;3 \\\\ 3&amp;4 \\end{vmatrix}$, 其上三角矩阵为$\\begin{vmatrix} 1&amp;2 \\\\ 0&amp;3 \\\\ 0&amp;0 \\end{vmatrix}$, $k=1$时为$\\begin{vmatrix} 0&amp;2 \\\\ 0&amp;0 \\\\ 0&amp;0 \\end{vmatrix}$.最后返回的是取反后的矩阵.也就是上三角为0.</p>\n<hr>\n<p>接下来是attent模块.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>&emsp;&emsp;这是2017年Google Brain的一篇论文,使用self-attention机制构建模型.和之前完全不同的思路,</p>\n<ul>\n<li>RNN因为是递归式的结构,无法做到并行,而且要获得全局信息必须要做双向RNN.</li>\n<li>CNN则是因为感受域限制,只能获取局部信息.</li>\n<li>Transformer能够一步获取全局信息,在做NLP任务时表现很优异,如共指消解.</li>\n</ul>\n<p>&emsp;&emsp;Transformer使用传统的Encoder-Decoder结构,给定一个序列$(x_1,…,x_n)$经过处理得到$(y_1,…,y_m)$.模型的每一步都是自回归的,每次生成都使用之前的生成作为附加信息输入.</p>\n<p><img src=\"http://ww1.sinaimg.cn/large/006tNc79gy1g4fpkw99egg30m80jo4ni.gif\" width=\"80%\" height=\"80%\"></p>\n<p><img src=\"http://ww4.sinaimg.cn/large/006tNc79gy1g4fpbsaicwj30ng0ystfy.jpg\" width=\"80%\" height=\"80%\" aligns=\"center\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">EncoderDecoder</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class=\"line\">        super(EncoderDecoder, self).__init__()</span><br><span class=\"line\">        self.encoder = encoder</span><br><span class=\"line\">        self.decoder = decoder</span><br><span class=\"line\">        self.src_embed = src_embed</span><br><span class=\"line\">        self.tgt_embed = tgt_embed</span><br><span class=\"line\">        self.generator = generator</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encode</span><span class=\"params\">(self, src, src_mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.encode(self.src_embed(src), src_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decode</span><span class=\"params\">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>\n<p>这是构建EncoderDecoder框架,注意到encode(src, src_mask)就是memory,也就是用encode的输出作为了Decode的输入,图上两个部分相连的那条线.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Generator</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, d_model, vocab)</span>:</span></span><br><span class=\"line\">        super(Generator, self).__init__()</span><br><span class=\"line\">        self.proj = nn.Linear(d_model, vocab)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(self.proj(x), dim=<span class=\"number\">-1</span>)</span><br></pre></td></tr></table></figure>\n<p>&emsp;&emsp;这里是定义一个标准的线性然后softmax的标准生成步骤.因为之前没有用过pytorch,所以还是记录一下用到的内置函数. </p>\n<ul>\n<li>$nn.Linear()$函数是用来做线性变化的,输入一个维度为$m\\times n$的矩阵,经过线性变化$nn.Linear(n,t,bias=None)$得到的输入维度为$m\\times t$.具体的实现是输入矩阵乘$t\\times n$矩阵再加上一个维度为$m$的$\\text{bias}$.<script type=\"math/tex; mode=display\">y=xA^T+\\text{bias}</script></li>\n<li>$\\text{F.log_softmax()}$函数就是在softmax函数外面再加一个$\\text{log}$函数.需要注意的是$\\text{dim}$的指定,以二维函数为例,取$\\text{dim}=1$时,是对每一行进行softmax,而取0时是每一列进行计算.可以理解为沿着某条轴,将数据集中到轴上.<script type=\"math/tex; mode=display\">\\log(\\text{Softmax}(x_{i})) = \\log(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)})</script></li>\n</ul>\n<hr>\n<p>接下来是Encoder部分<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clones</span><span class=\"params\">(module, N)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.ModuleList([copy.deepcopy(module) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(N)])</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;首先定义一个clone方法,因为Encoder是由多个相同的层构成的.这里设$N=6$<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Encoder</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, layer, N)</span>:</span></span><br><span class=\"line\">        super(Encoder, self).__init__()</span><br><span class=\"line\">        self.layers = clones(layer, N)</span><br><span class=\"line\">        self.norm = LayerNorm(layer.size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> self.layers:</span><br><span class=\"line\">            x = layer(x, mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.norm(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LayerNorm</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, features, eps=<span class=\"number\">1e-6</span>)</span>:</span></span><br><span class=\"line\">        super(LayerNorm, self).__init__()</span><br><span class=\"line\">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class=\"line\">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class=\"line\">        self.eps = eps</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        mean = x.mean(<span class=\"number\">-1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        std = x.std(<span class=\"number\">-1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;然后定义Encoder和LN模块.LayerNorm本来torch里也有内置函数.这里是自己实现的.</p>\n<script type=\"math/tex; mode=display\">y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}\\times \\gamma +\\beta</script><p>&emsp;&emsp;其中$\\gamma$和$\\beta$是参数,$\\epsilon$是一个很小数,torch中取$e^{-5}$,为了保持数值的稳定性,防止分母趋近0.</p>\n<p>&emsp;&emsp;对应到图上就是$\\text{Add\\&amp;Norm}$这一块,对于每个sublayer有$LayerNorm(x+Sublayer(x))$.具体是$dropout\\rightarrow sublayer的input相加\\rightarrow Normalized$.但是在实际编码中,将Norm放到了dropout前面.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SublayerConnection</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, size, dropout)</span>:</span></span><br><span class=\"line\">        super(SublayerConnection, self).__init__()</span><br><span class=\"line\">        self.norm = LayerNorm(size)</span><br><span class=\"line\">        self.dropout = nn.Dropout(dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, sublayer)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;定义sublayer的残差连接.残差连接实际上就是$f(x)+x$,目的是为了反向传播的时候,梯度连乘,不会造成梯度消失.因为会出现常数项.可以看到是先normalizion再dropout.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">EncoderLayer</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class=\"line\">        super(EncoderLayer, self).__init__()</span><br><span class=\"line\">        self.self_attn = self_attn</span><br><span class=\"line\">        self.feed_forward = feed_forward</span><br><span class=\"line\">        self.sublayer = clones(SublayerConnection(size, dropout), <span class=\"number\">2</span>)</span><br><span class=\"line\">        self.size = size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, mask)</span>:</span></span><br><span class=\"line\">        x = self.sublayer[<span class=\"number\">0</span>](x, <span class=\"keyword\">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.sublayer[<span class=\"number\">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;Encoder每层有两个sublayer,第一个sublayer[0]用多头注意力机制,第二个sublayer[1]用位置全连接前馈网络.</p>\n<hr>\n<p>然后是Decoder层<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Decoder</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, layer, N)</span>:</span></span><br><span class=\"line\">        super(Decoder, self).__init__()</span><br><span class=\"line\">        self.layers = clones(layer, N)</span><br><span class=\"line\">        self.norm = LayerNorm(layer.size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> self.layers:</span><br><span class=\"line\">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.norm(x)</span><br></pre></td></tr></table></figure></p>\n<p>和Encoder一样,设计为6个相同的层.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DecoderLayer</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class=\"line\">        super(DecoderLayer, self).__init__()</span><br><span class=\"line\">        self.size = size</span><br><span class=\"line\">        self.self_attn = self_attn</span><br><span class=\"line\">        self.src_attn = src_attn</span><br><span class=\"line\">        self.feed_forward = feed_forward</span><br><span class=\"line\">        self.sublayer = clones(SublayerConnection(size, dropout), <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class=\"line\">        m = memory</span><br><span class=\"line\">        x = self.sublayer[<span class=\"number\">0</span>](x, <span class=\"keyword\">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class=\"line\">        x = self.sublayer[<span class=\"number\">1</span>](x, <span class=\"keyword\">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.sublayer[<span class=\"number\">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;Decoder的sublayer有三个,其中sublayer[0]和Encoder一样用自注意力机制,sublayer[2]同样是前馈网络,sublayer[1]是对Encoder的输出用多头注意力机制.残差网络和Norm和Encoder一样.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">subsequent_mask</span><span class=\"params\">(size)</span>:</span></span><br><span class=\"line\">    attn_shape = (<span class=\"number\">1</span>, size, size)</span><br><span class=\"line\">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class=\"number\">1</span>).astype(<span class=\"string\">'uint8'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.from_numpy(subsequent_mask) == <span class=\"number\">0</span></span><br></pre></td></tr></table></figure></p>\n<p>&emsp;&emsp;定义subsequent_mask方法,为了在Decoder时遮盖当前位置后的内容. $np.triu$方法是取上三角矩阵,$k$为偏移量,正数时向上偏移,比如有矩阵$\\begin{vmatrix} 1&amp;2 \\\\ 2&amp;3 \\\\ 3&amp;4 \\end{vmatrix}$, 其上三角矩阵为$\\begin{vmatrix} 1&amp;2 \\\\ 0&amp;3 \\\\ 0&amp;0 \\end{vmatrix}$, $k=1$时为$\\begin{vmatrix} 0&amp;2 \\\\ 0&amp;0 \\\\ 0&amp;0 \\end{vmatrix}$.最后返回的是取反后的矩阵.也就是上三角为0.</p>\n<hr>\n<p>接下来是attent模块.</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ck04z9ns80001z4o03x3k0nbd","category_id":"ck04z9nsf0004z4o0lh69hyjp","_id":"ck04z9nso000fz4o0ahw7eda7"},{"post_id":"ck04z9nsj0008z4o0k46tn3pq","category_id":"ck04z9nsf0004z4o0lh69hyjp","_id":"ck04z9nsr000kz4o06c0eadz4"},{"post_id":"ck04z9nsl000cz4o0rnjq0i1v","category_id":"ck04z9nsf0004z4o0lh69hyjp","_id":"ck04z9nss000nz4o0yfc9xxgb"},{"post_id":"ck04z9nsc0003z4o09tb4rjbz","category_id":"ck04z9nsk0009z4o0tbxjjdiy","_id":"ck04z9nsu000rz4o0f5d566tv"},{"post_id":"ck04z9nsm000ez4o0atu6wzkm","category_id":"ck04z9nsf0004z4o0lh69hyjp","_id":"ck04z9nsv000vz4o0n5azeh2v"},{"post_id":"ck04z9nsq000jz4o09gpzbpxj","category_id":"ck04z9nsk0009z4o0tbxjjdiy","_id":"ck04z9nsw000zz4o0cuiq2v3n"},{"post_id":"ck04z9nsh0006z4o00kkj22dk","category_id":"ck04z9nsf0004z4o0lh69hyjp","_id":"ck04z9nsx0012z4o0p61updw0"},{"post_id":"ck04z9nst000qz4o0cgmy3r50","category_id":"ck04z9nsf0004z4o0lh69hyjp","_id":"ck04z9nsy0015z4o0ugy7i4cz"},{"post_id":"ck04z9nsi0007z4o0meljdopx","category_id":"ck04z9nss000oz4o0jt8egsk9","_id":"ck04z9nsz001az4o0p1ct04d6"},{"post_id":"ck04z9nsu000uz4o0jvmu3k3j","category_id":"ck04z9nsf0004z4o0lh69hyjp","_id":"ck04z9nt0001dz4o0l4dfba1k"},{"post_id":"ck04z9nsv000yz4o0iyemwxnc","category_id":"ck04z9nsf0004z4o0lh69hyjp","_id":"ck04z9nt1001hz4o0cbs1l6b3"},{"post_id":"ck04z9nsr000mz4o0kmcg9mlu","category_id":"ck04z9nsv000wz4o05ykeajw1","_id":"ck04z9nt2001kz4o0473ar0cl"},{"post_id":"ck04z9nt0001cz4o0y1tmaqjk","category_id":"ck04z9nsy0016z4o0ef59plmh","_id":"ck04z9nt3001pz4o0ir0npaw0"},{"post_id":"ck04z9nsw0011z4o081xi7d4h","category_id":"ck04z9nsy0016z4o0ef59plmh","_id":"ck04z9nt5001sz4o089gmtvqx"},{"post_id":"ck04z9nt0001ez4o0d6vh16sp","category_id":"ck04z9nsf0004z4o0lh69hyjp","_id":"ck04z9nt6001xz4o0yyj05jf6"},{"post_id":"ck04z9nt2001jz4o07m3atagd","category_id":"ck04z9nsy0016z4o0ef59plmh","_id":"ck04z9nt70020z4o0px7nzdyp"},{"post_id":"ck04z9nsx0014z4o0ecjmshch","category_id":"ck04z9nsy0016z4o0ef59plmh","_id":"ck04z9nt80023z4o06abqzxph"},{"post_id":"ck04z9nsz0019z4o0qw49t1wh","category_id":"ck04z9nsy0016z4o0ef59plmh","_id":"ck04z9nt90028z4o0yx916m21"},{"post_id":"ck04z9nt3001mz4o0un1j3nsg","category_id":"ck04z9nt6001vz4o0orpp30e1","_id":"ck04z9nta002az4o0rlgm4arl"},{"post_id":"ck04z9nt80026z4o0pazvia89","category_id":"ck04z9nt80024z4o0dexrexzy","_id":"ck04z9ntb002fz4o0rz4sj7c9"},{"post_id":"ck04z9nt4001rz4o0oi8gcu7t","category_id":"ck04z9nt80024z4o0dexrexzy","_id":"ck04z9ntc002jz4o0yxw0r63e"},{"post_id":"ck04z9nt5001uz4o0sn3mf2xf","category_id":"ck04z9nt80024z4o0dexrexzy","_id":"ck04z9ntc002mz4o0jav6vool"},{"post_id":"ck04z9nt6001zz4o0z7hue8yg","category_id":"ck04z9nt80024z4o0dexrexzy","_id":"ck04z9ntd002pz4o0zshi5q4w"},{"post_id":"ck04z9nt70022z4o0epsu0ypa","category_id":"ck04z9ntc002lz4o0fl12n48c","_id":"ck04z9ntf002sz4o0ubuvbr2t"},{"post_id":"ck04z9nt90029z4o0rx8o59h0","category_id":"ck04z9ntd002qz4o0uj697nd7","_id":"ck04z9ntf002tz4o03d3fb76t"}],"PostTag":[{"post_id":"ck04z9ns80001z4o03x3k0nbd","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9nsl000bz4o0pn66vvwe"},{"post_id":"ck04z9nsj0008z4o0k46tn3pq","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9nsm000dz4o0dnc0ah6v"},{"post_id":"ck04z9nsl000cz4o0rnjq0i1v","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9nsq000iz4o0rn71f6b1"},{"post_id":"ck04z9nsc0003z4o09tb4rjbz","tag_id":"ck04z9nsk000az4o00yr3vkvj","_id":"ck04z9nsr000lz4o0jvsz9yjm"},{"post_id":"ck04z9nsm000ez4o0atu6wzkm","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9nst000pz4o0sk0f5o5b"},{"post_id":"ck04z9nsq000jz4o09gpzbpxj","tag_id":"ck04z9nsk000az4o00yr3vkvj","_id":"ck04z9nsu000tz4o06oeq4ypb"},{"post_id":"ck04z9nsh0006z4o00kkj22dk","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9nsv000xz4o01omiknmw"},{"post_id":"ck04z9nst000qz4o0cgmy3r50","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9nsw0010z4o0yx5ynm5s"},{"post_id":"ck04z9nsu000uz4o0jvmu3k3j","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9nsx0013z4o0obykf1cn"},{"post_id":"ck04z9nsv000yz4o0iyemwxnc","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9nsy0018z4o0gk4jaea3"},{"post_id":"ck04z9nsr000mz4o0kmcg9mlu","tag_id":"ck04z9nsu000sz4o0lkgiaga6","_id":"ck04z9nsz001bz4o08lqh4rvc"},{"post_id":"ck04z9nt0001cz4o0y1tmaqjk","tag_id":"ck04z9nsy0017z4o0jkluxq58","_id":"ck04z9nt2001iz4o0fck2a9fm"},{"post_id":"ck04z9nsw0011z4o081xi7d4h","tag_id":"ck04z9nsy0017z4o0jkluxq58","_id":"ck04z9nt3001lz4o0zdx86ig0"},{"post_id":"ck04z9nt0001ez4o0d6vh16sp","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9nt4001qz4o0zku2jpxh"},{"post_id":"ck04z9nt2001jz4o07m3atagd","tag_id":"ck04z9nsy0017z4o0jkluxq58","_id":"ck04z9nt5001tz4o0gbew19hf"},{"post_id":"ck04z9nsx0014z4o0ecjmshch","tag_id":"ck04z9nsy0017z4o0jkluxq58","_id":"ck04z9nt6001yz4o0zubwxul5"},{"post_id":"ck04z9nsz0019z4o0qw49t1wh","tag_id":"ck04z9nsy0017z4o0jkluxq58","_id":"ck04z9nt70021z4o03mqjs9c2"},{"post_id":"ck04z9nt3001mz4o0un1j3nsg","tag_id":"ck04z9nt6001wz4o02aybh0g2","_id":"ck04z9nt90027z4o01ugqjuop"},{"post_id":"ck04z9nt80026z4o0pazvia89","tag_id":"ck04z9nt80025z4o08rx1kde1","_id":"ck04z9nta002dz4o0ptbc2e02"},{"post_id":"ck04z9nt4001rz4o0oi8gcu7t","tag_id":"ck04z9nt80025z4o08rx1kde1","_id":"ck04z9ntb002ez4o0fs2ng6sr"},{"post_id":"ck04z9nt90029z4o0rx8o59h0","tag_id":"ck04z9nsg0005z4o072fvrf9r","_id":"ck04z9ntc002iz4o0omrutwb3"},{"post_id":"ck04z9nt5001uz4o0sn3mf2xf","tag_id":"ck04z9nt80025z4o08rx1kde1","_id":"ck04z9ntc002kz4o0cozu99z4"},{"post_id":"ck04z9nt6001zz4o0z7hue8yg","tag_id":"ck04z9nt80025z4o08rx1kde1","_id":"ck04z9ntd002oz4o07urzmk6b"},{"post_id":"ck04z9nt70022z4o0epsu0ypa","tag_id":"ck04z9nt80025z4o08rx1kde1","_id":"ck04z9nte002rz4o0x38fo4ib"}],"Tag":[{"name":"machine learning","_id":"ck04z9nsg0005z4o072fvrf9r"},{"name":"AI","_id":"ck04z9nsk000az4o00yr3vkvj"},{"name":"big data","_id":"ck04z9nsu000sz4o0lkgiaga6"},{"name":"Network","_id":"ck04z9nsy0017z4o0jkluxq58"},{"name":"Manjaro","_id":"ck04z9nt6001wz4o02aybh0g2"},{"name":"杂谈","_id":"ck04z9nt80025z4o08rx1kde1"}]}}